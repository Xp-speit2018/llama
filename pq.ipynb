{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv, LlamaDecoderLayer, LlamaMLP, LlamaRMSNorm, LlamaModel, LlamaSdpaAttention, LlamaPreTrainedModel\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "\n",
    "class LlamaForCausalLMResearch(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "        self.model = LlamaModelResearch(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        self.middleware = {}\n",
    "        self.fwcall = 0\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        self.fwcall += 1\n",
    "        print(f\"fwcall: {self.fwcall}, key cache size(one layer): {past_key_values[0][0].shape if past_key_values is not None else None}\")\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "class LlamaModelResearch(LlamaModel):\n",
    "    def __init__(self, config):\n",
    "        super(LlamaModel, self).__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayerResearch(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        self.middleware = {}\n",
    "\n",
    "class LlamaDecoderLayerResearch(LlamaDecoderLayer):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super(LlamaDecoderLayer, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LlamaSdpaAttentionResearch(config, layer_idx)\n",
    "        # self.self_attn = LlamaSdpaAttention(config, layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "class LlamaSdpaAttentionResearch(LlamaSdpaAttention):\n",
    "    \"\"\"\n",
    "    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n",
    "    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n",
    "    SDPA API.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.middleware = {}\n",
    "\n",
    "    # Adapted from LlamaAttention.forward\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        if output_attentions:\n",
    "            return super().forward(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "            )\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        self.middleware.update({\"query_states\": query_states, \"key_states\": key_states, \"value_states\": value_states})\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # In case static cache is used, it is an instance attribute.\n",
    "        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        causal_mask = attention_mask\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n",
    "\n",
    "        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n",
    "        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
    "        if query_states.device.type == \"cuda\" and causal_mask is not None:\n",
    "            query_states = query_states.contiguous()\n",
    "            key_states = key_states.contiguous()\n",
    "            value_states = value_states.contiguous()\n",
    "\n",
    "        # In case we are not compiling, we may set `causal_mask` to None, which is required to dispatch to SDPA's Flash Attention 2 backend, rather\n",
    "        # relying on the `is_causal` argument.\n",
    "        attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attn_mask=causal_mask,\n",
    "            dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "            is_causal=causal_mask is None and q_len > 1,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, None, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLMResearch.from_pretrained(\"llama-2-7b-hf\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x LlamaDecoderLayerResearch(\n",
       "    (self_attn): LlamaSdpaAttentionResearch(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "      (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm()\n",
       "    (post_attention_layernorm): LlamaRMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先用4096个token生成KV Cache，用来训练PQ。32层、每层32个head，一共需要1024个PQ索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaForCausalLMResearch' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwcall: 1, key cache size(one layer): None\n",
      "fwcall: 2, key cache size(one layer): torch.Size([1, 32, 8, 128])\n",
      "fwcall: 3, key cache size(one layer): torch.Size([1, 32, 9, 128])\n",
      "fwcall: 4, key cache size(one layer): torch.Size([1, 32, 10, 128])\n",
      "fwcall: 5, key cache size(one layer): torch.Size([1, 32, 11, 128])\n",
      "fwcall: 6, key cache size(one layer): torch.Size([1, 32, 12, 128])\n",
      "fwcall: 7, key cache size(one layer): torch.Size([1, 32, 13, 128])\n",
      "fwcall: 8, key cache size(one layer): torch.Size([1, 32, 14, 128])\n",
      "fwcall: 9, key cache size(one layer): torch.Size([1, 32, 15, 128])\n",
      "fwcall: 10, key cache size(one layer): torch.Size([1, 32, 16, 128])\n",
      "fwcall: 11, key cache size(one layer): torch.Size([1, 32, 17, 128])\n",
      "fwcall: 12, key cache size(one layer): torch.Size([1, 32, 18, 128])\n",
      "fwcall: 13, key cache size(one layer): torch.Size([1, 32, 19, 128])\n",
      "fwcall: 14, key cache size(one layer): torch.Size([1, 32, 20, 128])\n",
      "fwcall: 15, key cache size(one layer): torch.Size([1, 32, 21, 128])\n",
      "fwcall: 16, key cache size(one layer): torch.Size([1, 32, 22, 128])\n",
      "fwcall: 17, key cache size(one layer): torch.Size([1, 32, 23, 128])\n",
      "fwcall: 18, key cache size(one layer): torch.Size([1, 32, 24, 128])\n",
      "fwcall: 19, key cache size(one layer): torch.Size([1, 32, 25, 128])\n",
      "fwcall: 20, key cache size(one layer): torch.Size([1, 32, 26, 128])\n",
      "fwcall: 21, key cache size(one layer): torch.Size([1, 32, 27, 128])\n",
      "fwcall: 22, key cache size(one layer): torch.Size([1, 32, 28, 128])\n",
      "fwcall: 23, key cache size(one layer): torch.Size([1, 32, 29, 128])\n",
      "fwcall: 24, key cache size(one layer): torch.Size([1, 32, 30, 128])\n",
      "fwcall: 25, key cache size(one layer): torch.Size([1, 32, 31, 128])\n",
      "fwcall: 26, key cache size(one layer): torch.Size([1, 32, 32, 128])\n",
      "fwcall: 27, key cache size(one layer): torch.Size([1, 32, 33, 128])\n",
      "fwcall: 28, key cache size(one layer): torch.Size([1, 32, 34, 128])\n",
      "fwcall: 29, key cache size(one layer): torch.Size([1, 32, 35, 128])\n",
      "fwcall: 30, key cache size(one layer): torch.Size([1, 32, 36, 128])\n",
      "fwcall: 31, key cache size(one layer): torch.Size([1, 32, 37, 128])\n",
      "fwcall: 32, key cache size(one layer): torch.Size([1, 32, 38, 128])\n",
      "fwcall: 33, key cache size(one layer): torch.Size([1, 32, 39, 128])\n",
      "fwcall: 34, key cache size(one layer): torch.Size([1, 32, 40, 128])\n",
      "fwcall: 35, key cache size(one layer): torch.Size([1, 32, 41, 128])\n",
      "fwcall: 36, key cache size(one layer): torch.Size([1, 32, 42, 128])\n",
      "fwcall: 37, key cache size(one layer): torch.Size([1, 32, 43, 128])\n",
      "fwcall: 38, key cache size(one layer): torch.Size([1, 32, 44, 128])\n",
      "fwcall: 39, key cache size(one layer): torch.Size([1, 32, 45, 128])\n",
      "fwcall: 40, key cache size(one layer): torch.Size([1, 32, 46, 128])\n",
      "fwcall: 41, key cache size(one layer): torch.Size([1, 32, 47, 128])\n",
      "fwcall: 42, key cache size(one layer): torch.Size([1, 32, 48, 128])\n",
      "fwcall: 43, key cache size(one layer): torch.Size([1, 32, 49, 128])\n",
      "fwcall: 44, key cache size(one layer): torch.Size([1, 32, 50, 128])\n",
      "fwcall: 45, key cache size(one layer): torch.Size([1, 32, 51, 128])\n",
      "fwcall: 46, key cache size(one layer): torch.Size([1, 32, 52, 128])\n",
      "fwcall: 47, key cache size(one layer): torch.Size([1, 32, 53, 128])\n",
      "fwcall: 48, key cache size(one layer): torch.Size([1, 32, 54, 128])\n",
      "fwcall: 49, key cache size(one layer): torch.Size([1, 32, 55, 128])\n",
      "fwcall: 50, key cache size(one layer): torch.Size([1, 32, 56, 128])\n",
      "fwcall: 51, key cache size(one layer): torch.Size([1, 32, 57, 128])\n",
      "fwcall: 52, key cache size(one layer): torch.Size([1, 32, 58, 128])\n",
      "fwcall: 53, key cache size(one layer): torch.Size([1, 32, 59, 128])\n",
      "fwcall: 54, key cache size(one layer): torch.Size([1, 32, 60, 128])\n",
      "fwcall: 55, key cache size(one layer): torch.Size([1, 32, 61, 128])\n",
      "fwcall: 56, key cache size(one layer): torch.Size([1, 32, 62, 128])\n",
      "fwcall: 57, key cache size(one layer): torch.Size([1, 32, 63, 128])\n",
      "fwcall: 58, key cache size(one layer): torch.Size([1, 32, 64, 128])\n",
      "fwcall: 59, key cache size(one layer): torch.Size([1, 32, 65, 128])\n",
      "fwcall: 60, key cache size(one layer): torch.Size([1, 32, 66, 128])\n",
      "fwcall: 61, key cache size(one layer): torch.Size([1, 32, 67, 128])\n",
      "fwcall: 62, key cache size(one layer): torch.Size([1, 32, 68, 128])\n",
      "fwcall: 63, key cache size(one layer): torch.Size([1, 32, 69, 128])\n",
      "fwcall: 64, key cache size(one layer): torch.Size([1, 32, 70, 128])\n",
      "fwcall: 65, key cache size(one layer): torch.Size([1, 32, 71, 128])\n",
      "fwcall: 66, key cache size(one layer): torch.Size([1, 32, 72, 128])\n",
      "fwcall: 67, key cache size(one layer): torch.Size([1, 32, 73, 128])\n",
      "fwcall: 68, key cache size(one layer): torch.Size([1, 32, 74, 128])\n",
      "fwcall: 69, key cache size(one layer): torch.Size([1, 32, 75, 128])\n",
      "fwcall: 70, key cache size(one layer): torch.Size([1, 32, 76, 128])\n",
      "fwcall: 71, key cache size(one layer): torch.Size([1, 32, 77, 128])\n",
      "fwcall: 72, key cache size(one layer): torch.Size([1, 32, 78, 128])\n",
      "fwcall: 73, key cache size(one layer): torch.Size([1, 32, 79, 128])\n",
      "fwcall: 74, key cache size(one layer): torch.Size([1, 32, 80, 128])\n",
      "fwcall: 75, key cache size(one layer): torch.Size([1, 32, 81, 128])\n",
      "fwcall: 76, key cache size(one layer): torch.Size([1, 32, 82, 128])\n",
      "fwcall: 77, key cache size(one layer): torch.Size([1, 32, 83, 128])\n",
      "fwcall: 78, key cache size(one layer): torch.Size([1, 32, 84, 128])\n",
      "fwcall: 79, key cache size(one layer): torch.Size([1, 32, 85, 128])\n",
      "fwcall: 80, key cache size(one layer): torch.Size([1, 32, 86, 128])\n",
      "fwcall: 81, key cache size(one layer): torch.Size([1, 32, 87, 128])\n",
      "fwcall: 82, key cache size(one layer): torch.Size([1, 32, 88, 128])\n",
      "fwcall: 83, key cache size(one layer): torch.Size([1, 32, 89, 128])\n",
      "fwcall: 84, key cache size(one layer): torch.Size([1, 32, 90, 128])\n",
      "fwcall: 85, key cache size(one layer): torch.Size([1, 32, 91, 128])\n",
      "fwcall: 86, key cache size(one layer): torch.Size([1, 32, 92, 128])\n",
      "fwcall: 87, key cache size(one layer): torch.Size([1, 32, 93, 128])\n",
      "fwcall: 88, key cache size(one layer): torch.Size([1, 32, 94, 128])\n",
      "fwcall: 89, key cache size(one layer): torch.Size([1, 32, 95, 128])\n",
      "fwcall: 90, key cache size(one layer): torch.Size([1, 32, 96, 128])\n",
      "fwcall: 91, key cache size(one layer): torch.Size([1, 32, 97, 128])\n",
      "fwcall: 92, key cache size(one layer): torch.Size([1, 32, 98, 128])\n",
      "fwcall: 93, key cache size(one layer): torch.Size([1, 32, 99, 128])\n",
      "fwcall: 94, key cache size(one layer): torch.Size([1, 32, 100, 128])\n",
      "fwcall: 95, key cache size(one layer): torch.Size([1, 32, 101, 128])\n",
      "fwcall: 96, key cache size(one layer): torch.Size([1, 32, 102, 128])\n",
      "fwcall: 97, key cache size(one layer): torch.Size([1, 32, 103, 128])\n",
      "fwcall: 98, key cache size(one layer): torch.Size([1, 32, 104, 128])\n",
      "fwcall: 99, key cache size(one layer): torch.Size([1, 32, 105, 128])\n",
      "fwcall: 100, key cache size(one layer): torch.Size([1, 32, 106, 128])\n",
      "fwcall: 101, key cache size(one layer): torch.Size([1, 32, 107, 128])\n",
      "fwcall: 102, key cache size(one layer): torch.Size([1, 32, 108, 128])\n",
      "fwcall: 103, key cache size(one layer): torch.Size([1, 32, 109, 128])\n",
      "fwcall: 104, key cache size(one layer): torch.Size([1, 32, 110, 128])\n",
      "fwcall: 105, key cache size(one layer): torch.Size([1, 32, 111, 128])\n",
      "fwcall: 106, key cache size(one layer): torch.Size([1, 32, 112, 128])\n",
      "fwcall: 107, key cache size(one layer): torch.Size([1, 32, 113, 128])\n",
      "fwcall: 108, key cache size(one layer): torch.Size([1, 32, 114, 128])\n",
      "fwcall: 109, key cache size(one layer): torch.Size([1, 32, 115, 128])\n",
      "fwcall: 110, key cache size(one layer): torch.Size([1, 32, 116, 128])\n",
      "fwcall: 111, key cache size(one layer): torch.Size([1, 32, 117, 128])\n",
      "fwcall: 112, key cache size(one layer): torch.Size([1, 32, 118, 128])\n",
      "fwcall: 113, key cache size(one layer): torch.Size([1, 32, 119, 128])\n",
      "fwcall: 114, key cache size(one layer): torch.Size([1, 32, 120, 128])\n",
      "fwcall: 115, key cache size(one layer): torch.Size([1, 32, 121, 128])\n",
      "fwcall: 116, key cache size(one layer): torch.Size([1, 32, 122, 128])\n",
      "fwcall: 117, key cache size(one layer): torch.Size([1, 32, 123, 128])\n",
      "fwcall: 118, key cache size(one layer): torch.Size([1, 32, 124, 128])\n",
      "fwcall: 119, key cache size(one layer): torch.Size([1, 32, 125, 128])\n",
      "fwcall: 120, key cache size(one layer): torch.Size([1, 32, 126, 128])\n",
      "fwcall: 121, key cache size(one layer): torch.Size([1, 32, 127, 128])\n",
      "fwcall: 122, key cache size(one layer): torch.Size([1, 32, 128, 128])\n",
      "fwcall: 123, key cache size(one layer): torch.Size([1, 32, 129, 128])\n",
      "fwcall: 124, key cache size(one layer): torch.Size([1, 32, 130, 128])\n",
      "fwcall: 125, key cache size(one layer): torch.Size([1, 32, 131, 128])\n",
      "fwcall: 126, key cache size(one layer): torch.Size([1, 32, 132, 128])\n",
      "fwcall: 127, key cache size(one layer): torch.Size([1, 32, 133, 128])\n",
      "fwcall: 128, key cache size(one layer): torch.Size([1, 32, 134, 128])\n",
      "fwcall: 129, key cache size(one layer): torch.Size([1, 32, 135, 128])\n",
      "fwcall: 130, key cache size(one layer): torch.Size([1, 32, 136, 128])\n",
      "fwcall: 131, key cache size(one layer): torch.Size([1, 32, 137, 128])\n",
      "fwcall: 132, key cache size(one layer): torch.Size([1, 32, 138, 128])\n",
      "fwcall: 133, key cache size(one layer): torch.Size([1, 32, 139, 128])\n",
      "fwcall: 134, key cache size(one layer): torch.Size([1, 32, 140, 128])\n",
      "fwcall: 135, key cache size(one layer): torch.Size([1, 32, 141, 128])\n",
      "fwcall: 136, key cache size(one layer): torch.Size([1, 32, 142, 128])\n",
      "fwcall: 137, key cache size(one layer): torch.Size([1, 32, 143, 128])\n",
      "fwcall: 138, key cache size(one layer): torch.Size([1, 32, 144, 128])\n",
      "fwcall: 139, key cache size(one layer): torch.Size([1, 32, 145, 128])\n",
      "fwcall: 140, key cache size(one layer): torch.Size([1, 32, 146, 128])\n",
      "fwcall: 141, key cache size(one layer): torch.Size([1, 32, 147, 128])\n",
      "fwcall: 142, key cache size(one layer): torch.Size([1, 32, 148, 128])\n",
      "fwcall: 143, key cache size(one layer): torch.Size([1, 32, 149, 128])\n",
      "fwcall: 144, key cache size(one layer): torch.Size([1, 32, 150, 128])\n",
      "fwcall: 145, key cache size(one layer): torch.Size([1, 32, 151, 128])\n",
      "fwcall: 146, key cache size(one layer): torch.Size([1, 32, 152, 128])\n",
      "fwcall: 147, key cache size(one layer): torch.Size([1, 32, 153, 128])\n",
      "fwcall: 148, key cache size(one layer): torch.Size([1, 32, 154, 128])\n",
      "fwcall: 149, key cache size(one layer): torch.Size([1, 32, 155, 128])\n",
      "fwcall: 150, key cache size(one layer): torch.Size([1, 32, 156, 128])\n",
      "fwcall: 151, key cache size(one layer): torch.Size([1, 32, 157, 128])\n",
      "fwcall: 152, key cache size(one layer): torch.Size([1, 32, 158, 128])\n",
      "fwcall: 153, key cache size(one layer): torch.Size([1, 32, 159, 128])\n",
      "fwcall: 154, key cache size(one layer): torch.Size([1, 32, 160, 128])\n",
      "fwcall: 155, key cache size(one layer): torch.Size([1, 32, 161, 128])\n",
      "fwcall: 156, key cache size(one layer): torch.Size([1, 32, 162, 128])\n",
      "fwcall: 157, key cache size(one layer): torch.Size([1, 32, 163, 128])\n",
      "fwcall: 158, key cache size(one layer): torch.Size([1, 32, 164, 128])\n",
      "fwcall: 159, key cache size(one layer): torch.Size([1, 32, 165, 128])\n",
      "fwcall: 160, key cache size(one layer): torch.Size([1, 32, 166, 128])\n",
      "fwcall: 161, key cache size(one layer): torch.Size([1, 32, 167, 128])\n",
      "fwcall: 162, key cache size(one layer): torch.Size([1, 32, 168, 128])\n",
      "fwcall: 163, key cache size(one layer): torch.Size([1, 32, 169, 128])\n",
      "fwcall: 164, key cache size(one layer): torch.Size([1, 32, 170, 128])\n",
      "fwcall: 165, key cache size(one layer): torch.Size([1, 32, 171, 128])\n",
      "fwcall: 166, key cache size(one layer): torch.Size([1, 32, 172, 128])\n",
      "fwcall: 167, key cache size(one layer): torch.Size([1, 32, 173, 128])\n",
      "fwcall: 168, key cache size(one layer): torch.Size([1, 32, 174, 128])\n",
      "fwcall: 169, key cache size(one layer): torch.Size([1, 32, 175, 128])\n",
      "fwcall: 170, key cache size(one layer): torch.Size([1, 32, 176, 128])\n",
      "fwcall: 171, key cache size(one layer): torch.Size([1, 32, 177, 128])\n",
      "fwcall: 172, key cache size(one layer): torch.Size([1, 32, 178, 128])\n",
      "fwcall: 173, key cache size(one layer): torch.Size([1, 32, 179, 128])\n",
      "fwcall: 174, key cache size(one layer): torch.Size([1, 32, 180, 128])\n",
      "fwcall: 175, key cache size(one layer): torch.Size([1, 32, 181, 128])\n",
      "fwcall: 176, key cache size(one layer): torch.Size([1, 32, 182, 128])\n",
      "fwcall: 177, key cache size(one layer): torch.Size([1, 32, 183, 128])\n",
      "fwcall: 178, key cache size(one layer): torch.Size([1, 32, 184, 128])\n",
      "fwcall: 179, key cache size(one layer): torch.Size([1, 32, 185, 128])\n",
      "fwcall: 180, key cache size(one layer): torch.Size([1, 32, 186, 128])\n",
      "fwcall: 181, key cache size(one layer): torch.Size([1, 32, 187, 128])\n",
      "fwcall: 182, key cache size(one layer): torch.Size([1, 32, 188, 128])\n",
      "fwcall: 183, key cache size(one layer): torch.Size([1, 32, 189, 128])\n",
      "fwcall: 184, key cache size(one layer): torch.Size([1, 32, 190, 128])\n",
      "fwcall: 185, key cache size(one layer): torch.Size([1, 32, 191, 128])\n",
      "fwcall: 186, key cache size(one layer): torch.Size([1, 32, 192, 128])\n",
      "fwcall: 187, key cache size(one layer): torch.Size([1, 32, 193, 128])\n",
      "fwcall: 188, key cache size(one layer): torch.Size([1, 32, 194, 128])\n",
      "fwcall: 189, key cache size(one layer): torch.Size([1, 32, 195, 128])\n",
      "fwcall: 190, key cache size(one layer): torch.Size([1, 32, 196, 128])\n",
      "fwcall: 191, key cache size(one layer): torch.Size([1, 32, 197, 128])\n",
      "fwcall: 192, key cache size(one layer): torch.Size([1, 32, 198, 128])\n",
      "fwcall: 193, key cache size(one layer): torch.Size([1, 32, 199, 128])\n",
      "fwcall: 194, key cache size(one layer): torch.Size([1, 32, 200, 128])\n",
      "fwcall: 195, key cache size(one layer): torch.Size([1, 32, 201, 128])\n",
      "fwcall: 196, key cache size(one layer): torch.Size([1, 32, 202, 128])\n",
      "fwcall: 197, key cache size(one layer): torch.Size([1, 32, 203, 128])\n",
      "fwcall: 198, key cache size(one layer): torch.Size([1, 32, 204, 128])\n",
      "fwcall: 199, key cache size(one layer): torch.Size([1, 32, 205, 128])\n",
      "fwcall: 200, key cache size(one layer): torch.Size([1, 32, 206, 128])\n",
      "fwcall: 201, key cache size(one layer): torch.Size([1, 32, 207, 128])\n",
      "fwcall: 202, key cache size(one layer): torch.Size([1, 32, 208, 128])\n",
      "fwcall: 203, key cache size(one layer): torch.Size([1, 32, 209, 128])\n",
      "fwcall: 204, key cache size(one layer): torch.Size([1, 32, 210, 128])\n",
      "fwcall: 205, key cache size(one layer): torch.Size([1, 32, 211, 128])\n",
      "fwcall: 206, key cache size(one layer): torch.Size([1, 32, 212, 128])\n",
      "fwcall: 207, key cache size(one layer): torch.Size([1, 32, 213, 128])\n",
      "fwcall: 208, key cache size(one layer): torch.Size([1, 32, 214, 128])\n",
      "fwcall: 209, key cache size(one layer): torch.Size([1, 32, 215, 128])\n",
      "fwcall: 210, key cache size(one layer): torch.Size([1, 32, 216, 128])\n",
      "fwcall: 211, key cache size(one layer): torch.Size([1, 32, 217, 128])\n",
      "fwcall: 212, key cache size(one layer): torch.Size([1, 32, 218, 128])\n",
      "fwcall: 213, key cache size(one layer): torch.Size([1, 32, 219, 128])\n",
      "fwcall: 214, key cache size(one layer): torch.Size([1, 32, 220, 128])\n",
      "fwcall: 215, key cache size(one layer): torch.Size([1, 32, 221, 128])\n",
      "fwcall: 216, key cache size(one layer): torch.Size([1, 32, 222, 128])\n",
      "fwcall: 217, key cache size(one layer): torch.Size([1, 32, 223, 128])\n",
      "fwcall: 218, key cache size(one layer): torch.Size([1, 32, 224, 128])\n",
      "fwcall: 219, key cache size(one layer): torch.Size([1, 32, 225, 128])\n",
      "fwcall: 220, key cache size(one layer): torch.Size([1, 32, 226, 128])\n",
      "fwcall: 221, key cache size(one layer): torch.Size([1, 32, 227, 128])\n",
      "fwcall: 222, key cache size(one layer): torch.Size([1, 32, 228, 128])\n",
      "fwcall: 223, key cache size(one layer): torch.Size([1, 32, 229, 128])\n",
      "fwcall: 224, key cache size(one layer): torch.Size([1, 32, 230, 128])\n",
      "fwcall: 225, key cache size(one layer): torch.Size([1, 32, 231, 128])\n",
      "fwcall: 226, key cache size(one layer): torch.Size([1, 32, 232, 128])\n",
      "fwcall: 227, key cache size(one layer): torch.Size([1, 32, 233, 128])\n",
      "fwcall: 228, key cache size(one layer): torch.Size([1, 32, 234, 128])\n",
      "fwcall: 229, key cache size(one layer): torch.Size([1, 32, 235, 128])\n",
      "fwcall: 230, key cache size(one layer): torch.Size([1, 32, 236, 128])\n",
      "fwcall: 231, key cache size(one layer): torch.Size([1, 32, 237, 128])\n",
      "fwcall: 232, key cache size(one layer): torch.Size([1, 32, 238, 128])\n",
      "fwcall: 233, key cache size(one layer): torch.Size([1, 32, 239, 128])\n",
      "fwcall: 234, key cache size(one layer): torch.Size([1, 32, 240, 128])\n",
      "fwcall: 235, key cache size(one layer): torch.Size([1, 32, 241, 128])\n",
      "fwcall: 236, key cache size(one layer): torch.Size([1, 32, 242, 128])\n",
      "fwcall: 237, key cache size(one layer): torch.Size([1, 32, 243, 128])\n",
      "fwcall: 238, key cache size(one layer): torch.Size([1, 32, 244, 128])\n",
      "fwcall: 239, key cache size(one layer): torch.Size([1, 32, 245, 128])\n",
      "fwcall: 240, key cache size(one layer): torch.Size([1, 32, 246, 128])\n",
      "fwcall: 241, key cache size(one layer): torch.Size([1, 32, 247, 128])\n",
      "fwcall: 242, key cache size(one layer): torch.Size([1, 32, 248, 128])\n",
      "fwcall: 243, key cache size(one layer): torch.Size([1, 32, 249, 128])\n",
      "fwcall: 244, key cache size(one layer): torch.Size([1, 32, 250, 128])\n",
      "fwcall: 245, key cache size(one layer): torch.Size([1, 32, 251, 128])\n",
      "fwcall: 246, key cache size(one layer): torch.Size([1, 32, 252, 128])\n",
      "fwcall: 247, key cache size(one layer): torch.Size([1, 32, 253, 128])\n",
      "fwcall: 248, key cache size(one layer): torch.Size([1, 32, 254, 128])\n",
      "[{'generated_text': 'SJTU SEIEE is a research-oriented institution with top researchers of different fields, who pay close attention to high-quality research training and cultivating innovative talents. We have been engaged in undergraduate program innovation in recent years, and the university has been selected as “The Best Innovative Undergraduate School of the Ministry of Education” with a series of awards. We have been approved for a national engineering research center in the industry of micro-nano power system. We have been supported by the Ministry of Science and Technology for the development of national key laboratories and engineering research centers and have won the “Most Innovative” and “Most Beautiful” among all the universities supported by the Ministry of Education. We have 114 state-level scientific research projects, 3 national level key laboratories, 14 engineering technology centers, and 29 provincial, ministerial, and municipal-level scientific research institutes. We have undertaken more than 6000 projects in scientific research, cooperated well with more than 1000 national and international enterprises, and transferred a number of successful technologies and achievements. We have'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# device = \"cpu\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = device,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "sequence = \"SJTU SEIEE is\"\n",
    "# sequence =  \"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\"\n",
    "# sequence += \"Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.\"\n",
    "# sequence += \"The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.\"\n",
    "# sequence += \"In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.\"\n",
    "# sequence += \"We are releasing the following models to the general public for research and commercial use‡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§ 2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well.\"\n",
    "# sequence += \"We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\"\n",
    "# sequence += \"To create the new family of Llama 2 models, we began with the pretraining approach described in Touvron et al. (2023), using an optimized auto-regressive transformer, but made several changes to improve performance. Specifically, we performed more robust data cleaning, updated our data mixes, trained on 40% more total tokens, doubled the context length, and used grouped-query attention (GQA) to improve inference scalability for our larger models. Table 1 compares the attributes of the new Llama 2 models with the Llama 1 models.\"\n",
    "# sequence += \"Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations. We performed a variety of pretraining data investigations so that users can better understand the potential capabilities and limitations of our models; results can be found in Section 4.1.\"\n",
    "# sequence += \"We adopt most of the pretraining setting and model architecture from Llama 1. We use the standard transformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and Sennrich, 2019), use the SwiGLU activation function (Shazeer, 2020), and rotary positional embeddings (RoPE, Su et al. 2022). The primary architectural differences from Llama 1 include increased context length and grouped-query attention (GQA). We detail in Appendix Section A.2.1 each of these differences with ablation experiments to demonstrate their importance.\"\n",
    "# sequence += \"We trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with β1 = 0.9, β2 = 0.95, eps = 10−5. We use a cosine learning rate schedule, with warmup of 2000 steps, and decay final learning rate down to 10% of the peak learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. Figure 5 (a) shows the training loss for Llama 2 with these hyperparameters.\"\n",
    "# sequence += \"Training Hardware. We pretrained our models on Meta’s Research Super Cluster (RSC) (Lee and Sengupta, 2022) as well as internal production clusters. Both clusters use NVIDIA A100s. There are two key differences between the two clusters, with the first being the type of interconnect available: RSC uses NVIDIA Quantum InfiniBand while our production cluster is equipped with a RoCE (RDMA over converged Ethernet) solution based on commodity ethernet Switches. Both of these solutions interconnect 200 Gbps end-points. The second difference is the per-GPU power consumption cap — RSC uses 400W while our production cluster uses 350W. With this two-cluster setup, we were able to compare the suitability of these different types of interconnect for large scale training. RoCE (which is a more affordable, commercial interconnect network)\"\n",
    "# sequence += \"Table 2: CO2 emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta’s sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others. can scale almost as well as expensive Infiniband up to 2000 GPUs, which makes pretraining even more democratizable.\"\n",
    "# sequence += \"Carbon Footprint of Pretraining. Following preceding research (Bender et al., 2021a; Patterson et al., 2021; Wu et al., 2022; Dodge et al., 2022) and using power consumption estimates of GPU devices and carbon efficiency, we aim to calculate the carbon emissions resulting from the pretraining of Llama 2 models. The actual power usage of a GPU is dependent on its utilization and is likely to vary from the Thermal Design Power (TDP) that we employ as an estimation for GPU power. It is important to note that our calculations do not account for further power demands, such as those from interconnect or non-GPU server power consumption, nor from datacenter cooling systems. Additionally, the carbon output related to the production of AI hardware, like GPUs, could add to the overall carbon footprint as suggested by Gupta et al. (2022b,a). Table 2 summarizes the carbon emission for pretraining the Llama 2 family of models. A cumulative of 3.3M GPU hours of computation was performed on hardware of type A100-80GB (TDP of 400W or 350W). We estimate the total emissions for training to be 539 tCO2eq, of which 100% were directly offset by Meta’s sustainability program.∗∗ Our open release strategy also means that these pretraining costs will not need to be incurred by other companies, saving more global resources.\"\n",
    "# sequence += \"In this section, we report the results for the Llama 1 and Llama 2 base models, MosaicML Pretrained Transformer (MPT)†† models, and Falcon (Almazrouei et al., 2023) models on standard academic benchmarks. For all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon models internally. For these models, we always pick the best score between our evaluation framework and any publicly reported results. In Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety benchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The results for all the individual benchmarks are available in Section A.2.2.\"\n",
    "# sequence += \"• Code. We report the average pass@1 scores of our models on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). • Commonsense Reasoning. We report the average of PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019a), WinoGrande (Sakaguchi et al., 2021), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and CommonsenseQA (Talmor et al., 2018). We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. • World Knowledge. We evaluate the 5-shot performance on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) and report the average. • Reading Comprehension. For reading comprehension, we report the 0-shot average on SQuAD (Rajpurkar et al., 2018), QuAC (Choi et al., 2018), and BoolQ (Clark et al., 2019). • MATH. We report the average of the GSM8K (8 shot) (Cobbe et al., 2021) and MATH (4 shot) (Hendrycks et al., 2021) benchmarks at top 1.\"\n",
    "# sequence += \"As shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the results on MMLU and BBH by ≈5 and ≈8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 2 70B model outperforms all open-source models. In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L. We also analysed the potential data contamination and share the details in Section A.6.\"\n",
    "# sequence += \"Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources. In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns (Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\"\n",
    "# sequence += \"Getting Started. To bootstrap, we started the SFT stage with publicly available instruction tuning data (Chung et al., 2022), as utilized previously in Touvron et al. (2023). Quality Is All You Need. Third-party SFT data is available from many different sources, but we found that many of these have insufficient diversity and quality — in particular for aligning LLMs towards dialogue-style instructions. As a result, we focused first on collecting several thousand examples of high-quality SFT data, as illustrated in Table 5. By setting aside millions of examples from third-party datasets and using fewer but higher-quality examples from our own vendor-based annotation efforts, our results notably improved. These findings are similar in spirit to Zhou et al. (2023), which also finds that a limited set of clean instruction-tuning data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of 27,540 annotations. Note that we do not include any Meta user data. We also observed that different annotation platforms and vendors can result in markedly different downstream model performance, highlighting the importance of data checks even when using vendors to source annotations. To validate our data quality, we carefully examined a set of 180 examples, comparing the annotations provided by humans with the samples generated by the model through manual scrutiny. Surprisingly, we found that the outputs sampled from the resulting SFT model were often competitive with SFT data handwritten by human annotators, suggesting that we could reprioritize and devote more annotation effort to preference-based annotation for RLHF. Fine-Tuning Details. For supervised fine-tuning, we use a cosine learning rate schedule with an initial learning rate of 2 × 10−5, a weight decay of 0.1, a batch size of 64, and a sequence length of 4096 tokens. For the fine-tuning process, each sample consists of a prompt and an answer. To ensure the model sequence length is properly filled, we concatenate all the prompts and answers from the training set. A special token is utilized to separate the prompt and answer segments. We utilize an autoregressive objective and zero-out the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we fine-tune the model for 2 epochs.\"\n",
    "\n",
    "output = pipe(sequence, max_length=256, do_sample=True, temperature=0.9)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextGenerationPipeline中使用了自回归。首次运行时使用给定token作为输入，后面每次输入一个token。key cache size的解释为`(num_batch, num_heads, seq_len, head_dim)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力层中的`q_proj`, `k_proj`, `v_proj`分别是生成$Q$、$K$、$V$的线性层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           Linear\n",
      "\u001b[0;31mString form:\u001b[0m    Linear(in_features=4096, out_features=4096, bias=False)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/faiss/lib/python3.11/site-packages/torch/nn/modules/linear.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
      "\n",
      "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "Args:\n",
      "    in_features: size of each input sample\n",
      "    out_features: size of each output sample\n",
      "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "        Default: ``True``\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "    - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "Attributes:\n",
      "    weight: the learnable weights of the module of shape\n",
      "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "            If :attr:`bias` is ``True``, the values are initialized from\n",
      "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> m = nn.Linear(20, 30)\n",
      "    >>> input = torch.randn(128, 20)\n",
      "    >>> output = m(input)\n",
      "    >>> print(output.size())\n",
      "    torch.Size([128, 30])\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule."
     ]
    }
   ],
   "source": [
    "attn.q_proj?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Llama的transformers实现中, kv cache由注意力层的中间变量`past_key_value`表示。对于生成式模型，这是一个`DynamicCache`类，定义如下：\n",
    "```python\n",
    "class DynamicCache(Cache):\n",
    "    \"\"\"\n",
    "    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n",
    "\n",
    "    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n",
    "    `[batch_size, num_heads, seq_len, head_dim]`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n",
    "```\n",
    "Cache在LlamaModel前向传播之前被创建，并作为forward的参数流经所有层。生命周期为一次前向传播，在整个模型一次推理之后会被返回。  \n",
    "Pipeline使用了某种自回归手段（TODO: 查清pipeline的源码）使得一次推理的输出（包括缓存）传给了下一次输入。这使得Cache的生命周期扩展到了一次pipeline。\n",
    "```python\n",
    "class LlamaForCausalLM(LlamaPreTrainedModel):\n",
    "    ...\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        ...\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sadly past_key_value is not present as a property\n",
    "getattr(attn, \"past_key_value\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicCache()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = attn.middleware\n",
    "m['past_key_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-0.5439,  0.2430, -0.2144,  ...,  0.3013, -0.2847,  0.3466],\n",
       "           [-0.2758,  0.0278, -0.0080,  ...,  0.4342, -0.1943,  0.4467],\n",
       "           [ 0.7921, -0.3864, -0.3918,  ..., -0.3684,  0.3204, -0.3939],\n",
       "           ...,\n",
       "           [-0.0603, -0.0925,  0.0043,  ...,  0.4392, -0.2186,  0.4145],\n",
       "           [ 0.2962, -0.1703, -0.2377,  ..., -0.0684,  0.1394, -0.0229],\n",
       "           [-0.1052,  0.2297,  0.1839,  ...,  0.4951, -0.2811,  0.4878]],\n",
       " \n",
       "          [[ 0.6405,  0.8949,  0.4003,  ..., -0.8481,  0.2465, -0.6571],\n",
       "           [ 0.7500,  0.6643, -0.3054,  ..., -0.0711,  0.3391, -0.0927],\n",
       "           [-1.1822, -0.9251,  0.2281,  ...,  0.5307, -0.5518,  0.5414],\n",
       "           ...,\n",
       "           [ 0.7189, -0.5203, -0.2361,  ...,  0.5054, -0.0025,  0.4217],\n",
       "           [-0.6590,  0.7698,  0.0271,  ..., -0.5157,  0.0932, -0.3848],\n",
       "           [-0.6171,  0.5436,  0.5091,  ...,  0.2413,  0.1192,  0.1818]],\n",
       " \n",
       "          [[-0.1716,  0.1786,  0.3422,  ...,  1.6794,  1.7912,  1.6603],\n",
       "           [-0.3121, -0.1298, -0.4755,  ..., -0.5612, -0.5930, -0.5189],\n",
       "           [ 0.1177, -0.0728,  0.0378,  ...,  1.4984,  1.4912,  1.3668],\n",
       "           ...,\n",
       "           [-0.1612, -0.1109, -0.5512,  ..., -0.8498, -0.9593, -0.8753],\n",
       "           [ 0.5893, -0.1940,  0.1466,  ...,  1.5916,  1.6073,  1.4635],\n",
       "           [-0.2162, -0.4143, -0.0553,  ..., -0.7105, -0.8215, -0.7455]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.3654, -0.2995, -0.9506,  ..., -0.4015, -0.8239, -1.1277],\n",
       "           [-0.0173,  0.1020, -0.0041,  ..., -0.4289,  0.3768, -0.3743],\n",
       "           [ 0.0869,  1.5112,  0.4000,  ..., -0.7364, -1.5341, -1.4038],\n",
       "           ...,\n",
       "           [-0.3039, -0.2487, -0.1270,  ..., -0.4018,  0.1727, -0.4961],\n",
       "           [-0.2455, -0.8903, -0.0212,  ..., -0.6071, -0.9084, -1.1512],\n",
       "           [ 0.1560,  0.0065,  0.1004,  ..., -0.2827,  0.2556, -0.4280]],\n",
       " \n",
       "          [[-0.4287, -0.4325, -0.3622,  ..., -0.4878,  0.1869,  0.1863],\n",
       "           [ 0.0983,  0.2215, -0.6120,  ...,  0.4284, -0.3179, -0.3147],\n",
       "           [ 0.4033,  0.6115, -0.9893,  ..., -0.5531,  0.2719,  0.2729],\n",
       "           ...,\n",
       "           [-0.0068, -0.5900, -0.9677,  ...,  0.3312, -0.1382, -0.1335],\n",
       "           [ 0.4566, -0.7664, -0.6232,  ..., -0.1677, -0.0582, -0.0590],\n",
       "           [-0.3890, -0.1050,  0.9283,  ...,  0.2603, -0.0935, -0.0901]],\n",
       " \n",
       "          [[ 0.4216,  0.2408,  0.0501,  ..., -1.7845,  1.1346, -0.1847],\n",
       "           [-0.3448, -0.7994, -0.8797,  ...,  1.0760, -0.4960,  0.2305],\n",
       "           [ 0.1921,  0.0731,  0.3088,  ..., -1.2507,  0.3667, -0.0665],\n",
       "           ...,\n",
       "           [-0.2758,  0.3265, -0.8562,  ...,  0.8706, -0.8405,  0.2675],\n",
       "           [ 0.2942, -0.2111,  0.0552,  ..., -0.9598,  0.4722, -0.0488],\n",
       "           [ 0.8834, -0.7023,  0.4765,  ...,  1.0736, -0.6751,  0.2580]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-1.5152e+00,  5.4517e-03,  1.2864e+00,  ..., -8.9910e-02,\n",
       "            -5.0960e-01, -9.5722e-02],\n",
       "           [-1.8336e+00,  3.7345e-01,  3.3168e-01,  ..., -1.1534e+00,\n",
       "            -1.0667e+00,  8.3804e-01],\n",
       "           [-7.7209e-01, -1.8782e-01, -1.5676e+00,  ..., -3.9127e-01,\n",
       "            -5.2398e-01, -1.4477e-01],\n",
       "           ...,\n",
       "           [-1.3019e+00, -4.4331e-01,  6.3659e-01,  ..., -1.8738e+00,\n",
       "            -1.8308e+00,  1.5365e+00],\n",
       "           [-1.3147e+00, -3.9034e-01,  3.0371e-01,  ..., -1.6009e+00,\n",
       "            -1.8799e+00,  1.3437e+00],\n",
       "           [-1.3619e-01,  1.7464e-01,  4.2202e-03,  ..., -1.9501e+00,\n",
       "            -1.7553e+00,  1.6801e+00]],\n",
       " \n",
       "          [[-2.3673e-01, -6.0349e-02, -4.1532e-02,  ..., -5.7674e-01,\n",
       "            -1.5687e-01,  1.5066e-01],\n",
       "           [ 7.4984e-01, -2.2421e-02, -1.2585e+00,  ..., -1.1756e+00,\n",
       "            -3.9360e-01, -5.3772e-01],\n",
       "           [ 4.5830e-03,  4.0021e-01, -6.4968e-01,  ..., -5.0270e-01,\n",
       "            -1.6040e-01,  1.3537e-01],\n",
       "           ...,\n",
       "           [ 1.2454e+00, -1.0337e-02, -9.7692e-01,  ..., -1.0137e+00,\n",
       "            -7.0291e-01, -8.6653e-01],\n",
       "           [ 1.1562e+00, -5.4503e-01, -5.6521e-01,  ..., -1.1982e+00,\n",
       "            -6.5465e-01, -7.6123e-01],\n",
       "           [-8.6098e-02, -6.5162e-01,  5.9947e-01,  ..., -1.0317e+00,\n",
       "            -1.4038e-01, -8.5733e-01]],\n",
       " \n",
       "          [[ 2.3778e-01,  1.4497e-01, -1.8667e-01,  ...,  1.2104e+00,\n",
       "             5.3901e-01,  4.5862e-01],\n",
       "           [ 5.5787e-02, -3.5750e-01, -3.1628e-01,  ...,  1.1215e+00,\n",
       "             6.2163e-01,  6.0466e-01],\n",
       "           [-1.7163e-01, -3.6217e-01, -2.2185e-02,  ...,  1.3198e+00,\n",
       "             6.5987e-01,  5.1232e-01],\n",
       "           ...,\n",
       "           [ 2.5210e-01,  3.7445e-01, -8.0229e-02,  ...,  1.2443e+00,\n",
       "             6.8195e-01,  8.5897e-01],\n",
       "           [-2.5010e-02, -1.9028e-01,  2.5290e-02,  ...,  1.3882e+00,\n",
       "             7.0613e-01,  9.1498e-01],\n",
       "           [-3.4805e-01, -7.7127e-02, -4.4340e-02,  ...,  1.1844e+00,\n",
       "             6.8606e-01,  8.7454e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.4966e-01, -2.1249e-02,  1.9443e-02,  ...,  5.1522e-01,\n",
       "            -2.6378e-01,  1.8894e-01],\n",
       "           [ 1.1591e-01, -7.5338e-02,  1.4517e-02,  ...,  2.3263e-02,\n",
       "            -2.0692e-01,  1.4156e-01],\n",
       "           [ 7.4774e-01, -2.4990e-01, -8.8429e-02,  ...,  2.1526e-01,\n",
       "            -4.1313e-01,  2.3915e-01],\n",
       "           ...,\n",
       "           [ 9.0100e-03, -5.1563e-01,  5.0672e-01,  ..., -4.9205e-01,\n",
       "             3.0678e-01, -1.1965e-01],\n",
       "           [-5.3508e-01, -2.8680e-01,  7.3844e-01,  ..., -4.5011e-01,\n",
       "            -5.9039e-03,  2.5659e-01],\n",
       "           [-3.7754e-01,  3.2806e-01,  3.4806e-01,  ..., -5.4965e-01,\n",
       "             4.0430e-01, -4.1118e-01]],\n",
       " \n",
       "          [[ 5.1370e-01, -3.9155e-02, -2.3980e-02,  ..., -6.3193e-01,\n",
       "             4.9796e-01, -9.4552e-03],\n",
       "           [-9.8026e-02, -5.7883e-01,  1.0328e-02,  ...,  3.0078e-01,\n",
       "            -4.3257e-01,  3.9208e-01],\n",
       "           [-4.0993e-01, -2.8238e-02,  6.6260e-02,  ..., -3.2088e-01,\n",
       "             2.3164e-01,  1.1576e-01],\n",
       "           ...,\n",
       "           [-3.9973e-02,  3.9909e-01, -3.8492e-01,  ...,  7.3014e-01,\n",
       "            -1.1920e+00,  3.6690e-01],\n",
       "           [-8.3863e-01,  2.9269e-03, -8.2317e-01,  ...,  7.2947e-01,\n",
       "            -1.1484e+00,  3.4617e-01],\n",
       "           [-9.4437e-01, -3.8410e-01, -8.8573e-01,  ...,  7.3116e-01,\n",
       "            -1.1818e+00,  2.3540e-01]],\n",
       " \n",
       "          [[ 4.1003e-03,  1.1132e-01, -5.5593e-04,  ...,  7.6629e-01,\n",
       "            -8.5982e-01,  5.2231e-01],\n",
       "           [-4.8816e-01,  3.9032e-01,  3.0044e-01,  ...,  1.3935e+00,\n",
       "            -1.4306e+00,  9.6623e-01],\n",
       "           [-4.6645e-02,  2.7251e-01,  3.0165e-01,  ...,  9.2336e-01,\n",
       "            -9.4843e-01,  5.7345e-01],\n",
       "           ...,\n",
       "           [-8.7356e-01,  2.0682e-01, -2.1174e-01,  ...,  1.8831e+00,\n",
       "            -1.9049e+00,  1.3286e+00],\n",
       "           [ 2.6050e-01,  9.7159e-02,  1.1453e-01,  ...,  1.8601e+00,\n",
       "            -1.8256e+00,  1.3092e+00],\n",
       "           [ 7.7270e-01,  7.6556e-02,  1.2311e-01,  ...,  1.8381e+00,\n",
       "            -1.8332e+00,  1.2878e+00]]]], device='cuda:0'),\n",
       " tensor([[[[ 1.4507, -0.9928, -0.4399,  ..., -0.0056, -0.5313, -0.2134],\n",
       "           [-0.6625, -0.3657,  0.4824,  ...,  0.6392,  1.3319, -1.5148],\n",
       "           [-2.2699,  0.3866,  0.2154,  ..., -0.3827,  0.7711, -0.2952],\n",
       "           ...,\n",
       "           [ 0.3120, -0.7036, -0.0116,  ...,  1.1827,  0.8684, -1.4083],\n",
       "           [-0.7727, -0.3199,  0.2505,  ...,  0.7764,  0.7882, -0.8659],\n",
       "           [-1.2217, -0.6910,  0.7637,  ...,  1.5034,  1.4726, -1.8121]],\n",
       " \n",
       "          [[-0.1721, -0.0896, -0.5453,  ..., -0.3792, -1.6802,  1.7829],\n",
       "           [ 0.1139,  0.3445,  0.1107,  ..., -0.2469, -1.6980,  1.8451],\n",
       "           [-0.2269, -0.0604, -0.2894,  ..., -0.4739, -2.1471,  2.2862],\n",
       "           ...,\n",
       "           [ 0.4398,  0.1320,  0.4945,  ..., -0.0478, -1.9421,  2.2444],\n",
       "           [ 0.5578,  0.5035,  0.4925,  ..., -0.1816, -2.0626,  2.3606],\n",
       "           [ 0.3572,  0.5169,  0.2396,  ..., -0.1924, -2.0217,  2.3369]],\n",
       " \n",
       "          [[ 0.9110,  0.1388,  0.4453,  ...,  2.0710, -1.4403,  1.7219],\n",
       "           [ 2.2000, -1.1293,  0.3588,  ...,  0.3201, -2.5771,  2.0732],\n",
       "           [ 2.1174, -1.7963,  0.0753,  ...,  2.5869, -2.3039,  2.6184],\n",
       "           ...,\n",
       "           [ 1.1268,  0.8678,  0.4174,  ...,  1.4195, -3.8478,  2.1942],\n",
       "           [ 1.0413,  0.2931,  0.4187,  ...,  0.5513, -2.5118,  3.4535],\n",
       "           [-0.2062, -0.6361,  0.5850,  ...,  1.0907, -2.8379,  2.8055]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0632, -0.1077, -0.0690,  ..., -0.4529, -0.2365, -0.4725],\n",
       "           [ 0.3483, -0.2230, -0.0440,  ...,  0.5725,  0.6319, -0.5057],\n",
       "           [ 0.6087, -0.1616,  0.1505,  ..., -0.2050,  0.7146,  0.0620],\n",
       "           ...,\n",
       "           [ 0.0384,  0.0334, -0.1149,  ..., -0.2087,  0.4199, -0.0302],\n",
       "           [ 0.4726,  0.2132,  0.1790,  ..., -0.3145,  0.1610, -0.8053],\n",
       "           [ 0.6049,  0.2954,  0.5729,  ...,  0.0089,  0.1840, -0.2383]],\n",
       " \n",
       "          [[ 1.4657, -1.2726, -0.4933,  ...,  0.8649, -1.0896,  0.2309],\n",
       "           [-1.0349,  0.4331, -0.2579,  ..., -0.6307, -0.7298, -0.2658],\n",
       "           [-2.8576,  1.6521, -0.5505,  ...,  1.8709, -0.1963, -0.6897],\n",
       "           ...,\n",
       "           [-0.0591,  0.0102, -0.0387,  ..., -0.2974, -0.3379, -0.0304],\n",
       "           [-1.6965, -0.9773, -0.4621,  ..., -0.0861,  1.2071,  0.3333],\n",
       "           [-1.6254, -0.9267, -0.4790,  ..., -1.2372,  0.3150, -0.2953]],\n",
       " \n",
       "          [[ 1.6788,  0.4400, -0.2374,  ..., -0.4515, -2.3290, -1.3120],\n",
       "           [ 0.7684, -0.3819, -0.8638,  ..., -1.2694, -2.2510, -0.1302],\n",
       "           [-0.7323, -1.4880, -1.0912,  ..., -0.0807, -2.4537,  0.0852],\n",
       "           ...,\n",
       "           [ 1.1688, -0.3511, -0.2429,  ..., -2.4398, -2.7704, -1.2872],\n",
       "           [-0.4355, -0.0258, -0.6758,  ..., -2.1477, -3.3996, -0.5073],\n",
       "           [-1.7876,  0.5715,  0.0771,  ..., -1.7128, -3.0212, -0.3577]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 3.7969e-01, -7.9772e-02, -8.4076e-01,  ..., -2.6176e+00,\n",
       "            -1.2831e+00,  2.7257e+00],\n",
       "           [-9.1018e-01, -5.0723e-02,  6.3779e-01,  ..., -3.2456e+00,\n",
       "            -9.9424e-01,  3.2959e+00],\n",
       "           [-4.3578e-01, -7.3285e-01,  8.4581e-01,  ..., -3.0187e+00,\n",
       "            -1.5850e+00,  3.0997e+00],\n",
       "           ...,\n",
       "           [-2.8479e-01,  1.2753e+00, -9.0253e-01,  ..., -3.3866e+00,\n",
       "            -1.9112e+00,  3.4387e+00],\n",
       "           [-5.1076e-01,  1.1060e+00,  6.9814e-01,  ..., -3.6154e+00,\n",
       "            -4.7921e-01,  3.7435e+00],\n",
       "           [-6.4945e-01, -2.5467e-01,  8.9207e-01,  ..., -3.8174e+00,\n",
       "            -6.8512e-01,  3.9198e+00]],\n",
       " \n",
       "          [[-1.0085e-01, -7.9439e-01, -1.2749e-01,  ...,  9.4982e-01,\n",
       "             1.1372e+00,  1.0619e+00],\n",
       "           [-3.4962e-01, -2.9332e-01,  1.1484e-01,  ...,  1.1893e+00,\n",
       "             9.6555e-01,  1.0451e+00],\n",
       "           [-4.1836e-01,  5.9130e-02,  3.3654e-01,  ...,  1.2784e+00,\n",
       "             1.3918e+00,  1.3294e+00],\n",
       "           ...,\n",
       "           [-3.8222e-01, -5.4469e-02,  2.0481e-01,  ...,  1.1596e+00,\n",
       "             1.1613e+00,  1.5959e+00],\n",
       "           [-4.7719e-01,  2.9416e-02,  2.2850e-01,  ...,  1.0275e+00,\n",
       "             1.4195e+00,  1.0562e+00],\n",
       "           [-2.1999e-02,  2.0920e-01,  2.1085e-01,  ...,  8.6608e-01,\n",
       "             1.0640e+00,  1.1985e+00]],\n",
       " \n",
       "          [[-2.2661e+00,  6.1453e-01, -1.9354e-01,  ..., -5.2076e+00,\n",
       "            -5.7479e-01,  3.3596e-01],\n",
       "           [-1.7083e+00, -3.9575e-01, -7.9267e-01,  ..., -5.2890e+00,\n",
       "             2.7381e-01,  7.7605e-01],\n",
       "           [ 3.1542e-01, -1.0729e+00, -7.1834e-01,  ..., -5.4694e+00,\n",
       "            -2.0807e-01, -3.1043e-01],\n",
       "           ...,\n",
       "           [-9.9996e-01,  7.8745e-01, -5.7873e-01,  ..., -5.6431e+00,\n",
       "            -1.9178e-01, -1.3646e+00],\n",
       "           [-2.7952e-01,  4.8419e-01, -7.8579e-01,  ..., -6.3526e+00,\n",
       "             1.6109e-01,  4.1564e-01],\n",
       "           [ 1.1911e+00,  4.9239e-01, -8.5551e-01,  ..., -6.4583e+00,\n",
       "             1.9268e-01, -7.8038e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.7526e-01,  5.7386e-01, -8.6614e-02,  ..., -3.2170e+00,\n",
       "             2.8108e-01,  2.6845e-01],\n",
       "           [-1.6142e-01, -1.0476e-01,  6.1527e-01,  ..., -3.4921e+00,\n",
       "             1.2167e-01, -4.4432e-01],\n",
       "           [-7.8121e-02,  8.4320e-01,  2.3545e-01,  ..., -3.6303e+00,\n",
       "            -1.0033e-01, -1.1378e-01],\n",
       "           ...,\n",
       "           [ 3.9897e-01, -5.1925e-02,  4.9134e-01,  ..., -4.2394e+00,\n",
       "             9.7995e-01, -2.6837e-01],\n",
       "           [-4.8292e-03, -2.4683e-01,  2.8186e-01,  ..., -4.8240e+00,\n",
       "             1.5104e+00,  3.3090e-01],\n",
       "           [-3.2929e-01, -2.6516e-01,  2.7623e-01,  ..., -4.8014e+00,\n",
       "             1.3416e+00, -3.1542e-01]],\n",
       " \n",
       "          [[ 7.0262e-01, -9.4697e-01,  6.9302e-01,  ...,  2.3489e-01,\n",
       "             9.1697e-01, -7.3570e-01],\n",
       "           [-3.5027e-01, -9.6966e-01,  3.5448e-01,  ..., -4.7304e-01,\n",
       "             9.7871e-01,  5.6172e-01],\n",
       "           [-1.2524e+00,  1.5359e-01, -1.7124e-01,  ..., -4.4143e-01,\n",
       "            -1.6256e-01,  2.6013e-01],\n",
       "           ...,\n",
       "           [-1.2881e-01,  2.1761e-01,  3.7192e-02,  ..., -1.2097e+00,\n",
       "             2.4729e-01, -6.2172e-01],\n",
       "           [-3.4136e-01,  7.8646e-02, -5.9281e-01,  ..., -1.1275e-01,\n",
       "             1.1813e+00, -6.6508e-01],\n",
       "           [-3.2775e-01, -3.8753e-01, -1.3131e-01,  ..., -1.6487e+00,\n",
       "             5.9981e-01, -1.2114e+00]],\n",
       " \n",
       "          [[-1.0948e+00, -2.4290e-01,  1.6415e-01,  ..., -1.9800e+00,\n",
       "             2.1727e+00,  4.8248e+00],\n",
       "           [-4.0364e-01,  3.4214e-01,  2.3525e-01,  ..., -1.6683e+00,\n",
       "             2.2633e+00,  4.8790e+00],\n",
       "           [ 6.2417e-01,  2.9248e-01, -3.0783e-01,  ..., -2.7207e+00,\n",
       "             4.1460e+00,  5.5078e+00],\n",
       "           ...,\n",
       "           [-4.8482e-02, -1.9425e-01, -6.1085e-01,  ..., -2.4162e+00,\n",
       "             3.3783e+00,  4.7975e+00],\n",
       "           [ 1.1778e+00, -1.2724e-01, -2.8256e-01,  ..., -5.0356e+00,\n",
       "             3.0896e+00,  5.7734e+00],\n",
       "           [ 5.1328e-01, -1.0424e-02,  2.4471e-01,  ..., -4.3155e+00,\n",
       "             5.7190e+00,  6.5290e+00]]]], device='cuda:0'),\n",
       " tensor([[[[ 2.2449, -0.5350, -0.1203,  ...,  0.4008, -0.7451, -0.0879],\n",
       "           [ 2.7962,  1.0036, -1.0677,  ..., -1.2387, -1.2341, -1.2030],\n",
       "           [ 0.7532,  1.5608, -0.2584,  ..., -0.1958, -1.0790, -0.5672],\n",
       "           ...,\n",
       "           [ 1.7719, -0.1601,  0.1903,  ..., -0.4731, -0.3981, -0.3554],\n",
       "           [ 1.0693, -0.5211,  0.4540,  ..., -0.8779, -0.3006, -1.7982],\n",
       "           [-0.0470,  0.0858,  0.0736,  ..., -1.7157, -1.2374, -0.9132]],\n",
       " \n",
       "          [[-0.4100,  0.2639,  0.1105,  ..., -1.5513, -0.8316, -0.6912],\n",
       "           [ 0.1725,  1.0662,  0.5008,  ..., -1.6375, -0.2323, -0.0902],\n",
       "           [ 0.4043,  0.1486,  0.5179,  ..., -1.0723, -1.1604, -0.7072],\n",
       "           ...,\n",
       "           [-0.1643,  0.3303,  0.1734,  ..., -4.1953,  0.8162, -2.5231],\n",
       "           [ 0.2787,  0.6470,  0.4202,  ..., -2.3798,  0.0788, -1.0046],\n",
       "           [ 0.4259,  0.2966,  0.9950,  ..., -2.3893, -0.8685, -2.2800]],\n",
       " \n",
       "          [[ 0.1369,  0.3068, -0.2348,  ...,  0.5025,  1.2150, -0.4785],\n",
       "           [ 0.0092,  0.0163,  0.6644,  ...,  1.4805,  1.2856,  0.1625],\n",
       "           [ 0.3073,  0.0060,  0.5625,  ...,  0.6126,  0.1346, -0.4840],\n",
       "           ...,\n",
       "           [-0.1549,  0.4263, -0.0290,  ..., -0.2855, -0.9538, -0.8325],\n",
       "           [ 0.6240, -0.1379, -0.2612,  ..., -0.4052,  0.0658, -0.1065],\n",
       "           [ 0.5635, -0.2702, -0.2910,  ..., -0.8447, -0.4440,  0.5798]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.2632,  0.4086, -0.0239,  ...,  1.0187,  1.0152,  1.0981],\n",
       "           [ 0.3139,  0.7365, -0.5260,  ..., -0.0191, -0.3126,  0.6548],\n",
       "           [ 0.7067,  1.0088, -0.3100,  ...,  1.5235,  0.4872,  1.4373],\n",
       "           ...,\n",
       "           [ 0.3649,  0.0557, -0.8733,  ...,  1.2656,  1.1530,  2.0860],\n",
       "           [ 0.3612, -0.2892, -0.4087,  ...,  1.0680,  1.0876,  1.2824],\n",
       "           [ 0.2760, -0.0124, -0.3367,  ...,  1.3729,  0.0432,  1.6717]],\n",
       " \n",
       "          [[-0.1905,  0.3152,  0.1234,  ...,  1.1992,  1.8235,  0.9243],\n",
       "           [ 1.0496,  0.0134, -0.3562,  ...,  3.1336, -0.2016,  1.5692],\n",
       "           [ 0.8438, -0.0862,  0.3374,  ...,  0.4821,  0.7797,  0.7158],\n",
       "           ...,\n",
       "           [-0.2946,  0.4939,  0.2929,  ...,  1.6649,  0.5376,  3.5540],\n",
       "           [ 0.4224,  0.0375,  0.3881,  ...,  0.8896,  1.5663,  0.7883],\n",
       "           [ 0.7074,  0.1024,  0.2131,  ..., -0.1915,  1.6252,  1.6493]],\n",
       " \n",
       "          [[-0.8817,  0.5448,  0.0352,  ...,  0.7184,  1.9264, -0.3863],\n",
       "           [-1.9826,  0.4280, -0.4952,  ...,  0.6660,  1.2386,  1.2782],\n",
       "           [-0.6994,  0.9685, -0.2675,  ...,  0.5495,  1.5137, -0.3452],\n",
       "           ...,\n",
       "           [-0.1376, -0.4776, -0.5552,  ..., -0.2802,  0.6698,  1.1287],\n",
       "           [-0.3476, -0.3984, -0.4416,  ..., -0.5081,  1.8179,  0.4131],\n",
       "           [-0.1700, -0.2095, -0.1717,  ...,  0.4838, -1.0346,  1.2074]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 1.9515e+00, -1.1370e-01, -7.5222e-02,  ..., -1.4926e-01,\n",
       "            -3.0424e-01,  2.2137e-01],\n",
       "           [-2.2872e-02, -5.0076e-01,  8.7706e-01,  ..., -8.8531e-01,\n",
       "             1.5701e-01,  6.7527e-01],\n",
       "           [-1.2009e+00, -1.5700e-01,  8.4069e-01,  ..., -7.0201e-01,\n",
       "             2.9587e-01,  2.7075e-01],\n",
       "           ...,\n",
       "           [ 4.1653e-01,  4.2476e-01, -3.0642e-02,  ...,  1.0645e+00,\n",
       "            -4.5918e-01, -4.6202e-01],\n",
       "           [-2.9479e-01,  2.5522e-01, -3.9018e-01,  ...,  1.8802e-01,\n",
       "            -6.3591e-01,  6.4895e-01],\n",
       "           [-8.2891e-01,  1.7091e-01,  5.3789e-02,  ...,  1.2049e+00,\n",
       "            -8.1259e-01, -5.3599e-01]],\n",
       " \n",
       "          [[ 1.3937e-01,  1.0655e-01, -9.5558e-02,  ...,  8.4858e-01,\n",
       "             9.7214e-01,  1.2372e+00],\n",
       "           [ 9.1685e-01,  5.4209e-01, -2.2460e-01,  ...,  7.1957e-01,\n",
       "             3.9886e-01,  9.3561e-01],\n",
       "           [ 5.9492e-01, -1.2056e-01, -2.2232e-01,  ...,  4.8162e-01,\n",
       "             7.3627e-01,  1.0515e+00],\n",
       "           ...,\n",
       "           [ 5.2943e-01, -1.5161e-01, -5.8173e-01,  ..., -7.5222e-01,\n",
       "            -8.2840e-02,  1.2337e+00],\n",
       "           [ 7.4635e-01, -5.8837e-01, -2.1771e-01,  ..., -5.9244e-01,\n",
       "            -6.3108e-01,  7.6928e-01],\n",
       "           [ 9.0750e-01,  2.6490e-03, -3.5073e-01,  ..., -7.4789e-02,\n",
       "            -6.7246e-01,  1.0173e+00]],\n",
       " \n",
       "          [[-1.3387e+00,  3.8078e-01,  2.5564e-01,  ...,  1.2879e+00,\n",
       "            -9.1288e-01,  3.2837e+00],\n",
       "           [ 1.2997e+00,  1.4722e-01,  2.2832e-01,  ...,  6.1048e-01,\n",
       "            -4.9739e-01,  2.9814e+00],\n",
       "           [ 1.1363e+00, -1.4818e+00, -1.3329e+00,  ...,  8.3345e-01,\n",
       "            -9.0819e-01,  3.5852e+00],\n",
       "           ...,\n",
       "           [ 7.8112e-01,  1.2667e-02,  4.5746e-01,  ...,  1.7481e+00,\n",
       "            -2.5034e+00,  3.5237e+00],\n",
       "           [ 1.8324e+00,  6.9479e-01, -1.1078e+00,  ...,  6.4212e-01,\n",
       "            -1.9297e+00,  3.8659e+00],\n",
       "           [ 3.8886e-01,  2.1726e-01, -1.0417e+00,  ...,  3.9558e-01,\n",
       "            -1.9852e+00,  3.4210e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 9.0917e-01,  1.9453e+00, -1.0443e-01,  ...,  1.8424e+00,\n",
       "             4.6446e-01,  2.1793e-02],\n",
       "           [-1.7784e-01,  1.4656e-01, -1.0240e-01,  ...,  2.5117e+00,\n",
       "             1.6564e+00, -8.4653e-01],\n",
       "           [-4.0811e-01,  3.0621e-01, -9.8845e-02,  ...,  2.0122e+00,\n",
       "             1.0448e+00,  1.8796e-01],\n",
       "           ...,\n",
       "           [-2.1206e-01, -1.5388e-01,  7.8486e-02,  ...,  4.2751e+00,\n",
       "             1.2950e+00,  2.7210e-01],\n",
       "           [ 1.0708e-01, -3.0483e-03,  5.0395e-01,  ...,  3.4833e+00,\n",
       "             1.1468e+00, -1.1240e+00],\n",
       "           [ 7.6427e-02, -1.0007e-01, -8.6549e-01,  ...,  2.6075e+00,\n",
       "             1.3517e+00, -9.2430e-01]],\n",
       " \n",
       "          [[-5.7649e-01, -7.6144e-01,  2.2523e-01,  ...,  7.9410e-01,\n",
       "            -3.5563e-01,  1.2409e+00],\n",
       "           [ 1.9679e-01, -5.6185e-01,  7.3501e-01,  ...,  6.6004e-01,\n",
       "             2.0444e-01,  7.4066e-01],\n",
       "           [ 1.3664e-01,  7.4116e-02,  1.6819e+00,  ...,  1.8040e+00,\n",
       "            -2.6451e-01,  1.7748e+00],\n",
       "           ...,\n",
       "           [-2.9924e-01, -1.3972e-01,  4.1014e-01,  ...,  8.3473e-01,\n",
       "             6.7407e-01,  1.9012e+00],\n",
       "           [-1.1218e-01, -2.8487e-01,  1.4219e+00,  ...,  1.7681e+00,\n",
       "             1.2991e+00,  2.1398e+00],\n",
       "           [ 3.4218e-01,  1.3701e-01,  2.5105e-01,  ...,  1.1573e+00,\n",
       "             1.4135e+00,  8.2033e-01]],\n",
       " \n",
       "          [[ 5.9954e-01,  1.5467e-01,  2.5091e-01,  ..., -2.0062e+00,\n",
       "             3.7670e-01, -2.3188e+00],\n",
       "           [-1.8612e-01, -1.1081e+00, -8.1987e-01,  ..., -1.9388e+00,\n",
       "            -7.3122e-01, -1.2569e+00],\n",
       "           [-7.6454e-01, -9.5337e-01, -2.5792e-01,  ..., -2.5846e+00,\n",
       "             1.0687e+00, -1.9238e+00],\n",
       "           ...,\n",
       "           [ 8.2020e-01,  6.2017e-01, -4.0876e-01,  ..., -3.3407e+00,\n",
       "            -2.6725e-01, -3.1017e+00],\n",
       "           [ 2.2757e-01,  1.3821e+00,  6.7202e-01,  ..., -3.4277e+00,\n",
       "            -4.6425e-01, -3.5645e+00],\n",
       "           [-8.6694e-01, -3.0706e-01,  4.8525e-01,  ..., -3.1142e+00,\n",
       "             4.5776e-01, -3.2801e+00]]]], device='cuda:0'),\n",
       " tensor([[[[ 8.5885e-01, -1.2295e+00,  1.0321e+00,  ..., -1.5711e+00,\n",
       "             2.3805e-01, -2.1194e+00],\n",
       "           [ 2.3806e-02, -2.4700e+00,  3.4529e-01,  ..., -1.5203e-01,\n",
       "            -6.6986e-01, -2.4225e+00],\n",
       "           [ 6.9242e-01, -3.9418e-01,  2.6957e-01,  ..., -1.0190e+00,\n",
       "            -9.7249e-02, -2.6423e+00],\n",
       "           ...,\n",
       "           [-4.3682e-01, -3.1427e-01,  4.0500e-01,  ..., -4.3421e-01,\n",
       "            -9.0256e-01, -3.2458e+00],\n",
       "           [ 2.2015e-01, -1.9608e-01, -4.7190e-01,  ..., -1.1028e+00,\n",
       "            -3.7716e-01, -2.8009e+00],\n",
       "           [ 2.5090e-01, -8.4495e-01, -3.1076e-01,  ..., -9.6650e-02,\n",
       "            -6.7873e-01, -3.3593e+00]],\n",
       " \n",
       "          [[-3.3269e-01,  6.7021e-02, -2.0325e-01,  ..., -3.6041e+00,\n",
       "             1.2997e+00,  3.8823e-02],\n",
       "           [ 1.3154e+00,  4.9830e-01,  7.7294e-02,  ..., -3.1452e+00,\n",
       "            -3.3217e-01, -1.4687e-01],\n",
       "           [ 1.0175e+00,  6.5563e-01,  7.9283e-01,  ..., -3.5727e+00,\n",
       "            -5.7982e-01, -5.6274e-01],\n",
       "           ...,\n",
       "           [ 2.7422e-01,  2.3328e-01,  7.8093e-02,  ..., -4.5754e+00,\n",
       "            -1.4323e-02, -1.7861e-01],\n",
       "           [ 1.0065e+00, -8.9436e-01,  3.6750e-01,  ..., -4.4917e+00,\n",
       "             6.1901e-01, -7.7714e-01],\n",
       "           [ 6.6589e-01,  1.0505e-01,  3.0345e-01,  ..., -3.2582e+00,\n",
       "             3.7946e-01,  3.5451e-01]],\n",
       " \n",
       "          [[-8.9549e-01, -8.0076e-02, -3.5586e-02,  ...,  2.3150e-01,\n",
       "             1.3773e+00,  1.0351e+00],\n",
       "           [-5.5685e-01,  9.5342e-02,  1.3910e-01,  ...,  5.9416e-01,\n",
       "            -5.3608e-01,  1.0550e+00],\n",
       "           [-1.8165e-01,  4.9891e-01, -1.7193e-02,  ..., -1.1744e-01,\n",
       "             2.1086e-01, -2.6550e-01],\n",
       "           ...,\n",
       "           [-5.5983e-01,  3.5960e-01, -1.0984e+00,  ..., -5.7983e-01,\n",
       "             1.4468e-01,  1.3795e+00],\n",
       "           [-2.1597e-01, -2.0409e-01, -4.1597e-01,  ..., -1.5874e+00,\n",
       "             2.1202e-01,  1.1947e+00],\n",
       "           [-4.9816e-02, -1.8117e-01,  4.4728e-01,  ..., -1.0267e+00,\n",
       "             5.4710e-01,  4.7829e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.2059e+00, -7.4346e-01,  8.7966e-01,  ..., -5.1583e-02,\n",
       "            -5.2921e-01,  1.9691e-01],\n",
       "           [-8.3493e-03,  6.4846e-01,  2.6358e-01,  ..., -5.9048e-01,\n",
       "            -1.0159e+00,  1.2488e+00],\n",
       "           [ 1.1203e+00,  8.6811e-03,  3.6577e-01,  ...,  7.9867e-02,\n",
       "            -8.8516e-01, -7.5613e-02],\n",
       "           ...,\n",
       "           [ 1.5533e-01, -2.6372e-01, -1.3677e-01,  ..., -6.1409e-01,\n",
       "             1.0216e-01,  1.9114e+00],\n",
       "           [-1.5352e-01, -1.4709e-01,  6.9365e-01,  ...,  5.7984e-01,\n",
       "            -3.2080e-01,  2.9735e-01],\n",
       "           [-6.3212e-01,  2.3667e-01,  5.8581e-01,  ...,  3.6144e-01,\n",
       "            -2.4784e-01,  1.7598e+00]],\n",
       " \n",
       "          [[-4.5304e-01,  6.0054e-01, -4.3160e-01,  ...,  1.6658e+00,\n",
       "            -2.9301e+00, -5.0841e-01],\n",
       "           [-5.8168e-01,  1.2131e-01, -2.1968e-02,  ...,  1.5876e+00,\n",
       "            -4.4534e-01, -1.8083e+00],\n",
       "           [ 4.6392e-02, -8.5110e-01, -5.9440e-01,  ..., -1.2334e+00,\n",
       "            -2.5413e+00, -2.6739e+00],\n",
       "           ...,\n",
       "           [-6.0350e-01, -6.3170e-02, -2.5282e-02,  ...,  2.4487e+00,\n",
       "            -3.1433e-01,  8.8050e-01],\n",
       "           [ 3.3150e-01, -2.0700e-01, -4.2137e-01,  ...,  2.4758e+00,\n",
       "            -3.7696e+00, -5.4691e+00],\n",
       "           [ 5.4941e-01, -1.7922e-01, -4.9301e-03,  ...,  3.2843e+00,\n",
       "            -2.4077e-01, -7.1086e-01]],\n",
       " \n",
       "          [[-6.3620e-01,  3.9002e-01, -8.0998e-01,  ...,  7.8357e-01,\n",
       "             8.5692e-01,  7.8587e-02],\n",
       "           [-2.1406e-01,  2.2736e-01, -5.8281e-01,  ..., -2.3212e+00,\n",
       "            -4.8848e-01, -4.2281e-01],\n",
       "           [ 3.1150e-01,  6.4533e-01, -3.3401e-02,  ...,  6.6273e-01,\n",
       "            -2.8917e-01, -2.9513e-01],\n",
       "           ...,\n",
       "           [-4.4172e-01, -2.9605e-01, -1.2226e+00,  ...,  1.0354e-01,\n",
       "             4.6047e-01,  7.0835e-01],\n",
       "           [-2.8386e-01, -1.9521e-01, -5.6045e-01,  ...,  6.3320e-01,\n",
       "            -1.3177e+00,  5.7045e-01],\n",
       "           [ 6.9047e-02,  2.9403e-01,  6.0415e-01,  ...,  2.6148e-01,\n",
       "            -1.3030e+00, -1.8788e-01]]]], device='cuda:0'),\n",
       " tensor([[[[-2.2204e-01,  4.6240e-01, -8.7334e-02,  ..., -1.2352e+00,\n",
       "            -5.9936e-02,  2.8442e-03],\n",
       "           [-9.8230e-01, -1.7784e-02, -3.4273e-01,  ..., -1.9180e-01,\n",
       "            -5.9553e-01,  1.4252e+00],\n",
       "           [-8.4861e-01,  1.0024e-01, -6.2604e-01,  ..., -1.9431e+00,\n",
       "            -1.0021e+00, -6.5946e-01],\n",
       "           ...,\n",
       "           [-8.5186e-01,  1.4842e-01,  2.5405e-01,  ..., -6.4002e-01,\n",
       "            -2.4135e-01,  1.9955e-01],\n",
       "           [-9.9401e-02, -1.2896e-01,  4.0275e-01,  ..., -1.1704e+00,\n",
       "             4.1614e-01, -1.7686e-01],\n",
       "           [ 3.8627e-01, -2.3039e-01, -4.5134e-01,  ..., -9.4598e-01,\n",
       "             1.2990e-01, -8.9478e-01]],\n",
       " \n",
       "          [[-6.5536e-01,  5.6119e-01,  5.8773e-01,  ...,  5.9632e-01,\n",
       "            -3.8617e-01,  3.2245e-01],\n",
       "           [ 1.7850e+00, -1.2765e-01,  1.3289e+00,  ...,  4.4661e-01,\n",
       "             1.6304e-01, -1.0600e+00],\n",
       "           [ 2.4994e-01,  7.4163e-01,  8.2494e-02,  ..., -4.5387e-01,\n",
       "             2.1432e-01,  5.9433e-01],\n",
       "           ...,\n",
       "           [ 7.9053e-01,  4.8697e-01,  8.3904e-01,  ..., -7.7960e-01,\n",
       "             1.9850e+00, -8.1026e-01],\n",
       "           [ 1.3649e+00,  6.0865e-01,  2.2907e-01,  ..., -4.2475e-01,\n",
       "             1.0793e+00, -8.1432e-01],\n",
       "           [ 1.3235e-01, -3.4351e-02,  3.2601e-01,  ..., -5.4376e-01,\n",
       "             5.0985e-01, -8.9437e-01]],\n",
       " \n",
       "          [[ 2.8548e-01,  7.3629e-01, -3.1304e-01,  ...,  2.7513e+00,\n",
       "             4.1633e-01, -1.9024e+00],\n",
       "           [-1.3454e-01,  2.9357e-01, -2.3235e-01,  ...,  2.5624e+00,\n",
       "            -8.4664e-01, -5.2707e-01],\n",
       "           [-6.0670e-01,  2.4152e-01,  4.9457e-02,  ...,  1.3870e+00,\n",
       "             1.6327e-01,  1.8172e-01],\n",
       "           ...,\n",
       "           [ 5.2704e-01, -6.5091e-01,  1.1769e-02,  ...,  2.3628e-01,\n",
       "             1.2557e+00, -1.0355e-01],\n",
       "           [-8.6558e-01, -5.5718e-01,  1.3250e+00,  ...,  1.0554e+00,\n",
       "             7.1276e-01,  1.2789e+00],\n",
       "           [ 3.4476e-01, -7.6437e-02,  2.5734e-01,  ...,  1.4230e+00,\n",
       "             6.5603e-01,  2.2760e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.1120e-01,  5.3723e-01,  5.3405e-02,  ..., -8.9738e-01,\n",
       "            -3.3212e-01,  1.7474e+00],\n",
       "           [ 1.6664e+00,  8.8819e-01,  1.2254e-01,  ...,  3.3355e-01,\n",
       "             4.3511e-01, -1.4874e+00],\n",
       "           [ 1.1019e+00, -6.1379e-01,  6.4856e-02,  ..., -1.3658e-01,\n",
       "             4.9439e-01,  2.4091e+00],\n",
       "           ...,\n",
       "           [ 1.9369e+00, -5.7341e-01,  4.4317e-01,  ..., -2.9839e-01,\n",
       "            -2.9978e-01,  7.9589e-01],\n",
       "           [ 7.8397e-01,  3.6022e-01,  8.7347e-01,  ..., -9.1080e-01,\n",
       "            -1.2369e+00,  1.4784e+00],\n",
       "           [-1.3655e-01, -4.0131e-01, -8.8836e-01,  ...,  8.7688e-01,\n",
       "            -1.3643e+00,  9.2883e-01]],\n",
       " \n",
       "          [[ 5.9008e-02,  5.6924e-01,  2.6129e-01,  ...,  4.4829e+00,\n",
       "            -2.1972e-01, -1.2636e+00],\n",
       "           [ 3.8935e-01,  8.2997e-01,  3.4974e-01,  ...,  3.8244e+00,\n",
       "             4.1133e-01, -1.2816e+00],\n",
       "           [-5.7266e-01,  6.1685e-02, -2.8754e-01,  ...,  5.2179e+00,\n",
       "             1.1420e+00, -6.9016e-01],\n",
       "           ...,\n",
       "           [ 7.5087e-01,  4.8951e-02,  6.2987e-02,  ...,  3.7657e+00,\n",
       "            -3.9076e-01, -1.8479e+00],\n",
       "           [-1.3232e-01, -4.4770e-01, -6.4651e-01,  ...,  3.6950e+00,\n",
       "            -2.4381e-01, -1.5040e+00],\n",
       "           [-5.0072e-01, -5.8381e-01, -9.0519e-01,  ...,  4.2240e+00,\n",
       "            -2.0173e-01, -2.0578e+00]],\n",
       " \n",
       "          [[ 8.8032e-01, -3.4053e-02,  3.6564e-01,  ...,  4.7741e+00,\n",
       "            -1.4683e+00, -8.6991e-01],\n",
       "           [ 7.4947e-01, -1.9047e-01, -1.3723e-01,  ...,  5.5850e+00,\n",
       "            -2.7848e+00, -2.2235e+00],\n",
       "           [-1.3884e+00, -5.4496e-01,  2.2509e-01,  ...,  5.9076e+00,\n",
       "            -1.8591e+00, -9.6740e-01],\n",
       "           ...,\n",
       "           [-1.0742e+00,  6.5752e-01, -1.3203e-01,  ...,  7.1662e+00,\n",
       "            -3.3094e+00,  1.8504e+00],\n",
       "           [-1.2306e+00,  3.8470e-01, -5.2190e-01,  ...,  7.9014e+00,\n",
       "            -4.3310e+00,  1.5806e+00],\n",
       "           [-7.0696e-01,  2.6839e-01, -5.6625e-01,  ...,  7.5951e+00,\n",
       "            -2.5375e+00,  2.0403e+00]]]], device='cuda:0'),\n",
       " tensor([[[[ 6.6825e-01,  2.7591e-02, -1.2120e-01,  ..., -1.0741e+00,\n",
       "            -3.0434e-01,  8.2013e-01],\n",
       "           [-4.0113e-01,  5.1529e-01,  2.4874e-01,  ..., -1.2721e+00,\n",
       "            -7.3459e-01, -6.3588e-01],\n",
       "           [-7.1555e-01,  1.4345e-01, -1.2172e-01,  ..., -2.0224e+00,\n",
       "            -1.1271e+00,  1.0583e+00],\n",
       "           ...,\n",
       "           [ 5.1426e-02, -3.2096e-01, -4.2231e-01,  ...,  9.1649e-01,\n",
       "            -2.0138e+00, -1.1014e-01],\n",
       "           [-9.7037e-01,  3.2160e-01, -1.0305e-01,  ...,  1.2005e-01,\n",
       "            -1.9362e+00,  2.4442e+00],\n",
       "           [-9.9344e-01, -1.4876e-01, -2.6710e-01,  ...,  5.4485e-01,\n",
       "            -2.5409e+00,  5.9370e-01]],\n",
       " \n",
       "          [[ 1.6476e+00,  2.8094e-01, -1.1617e+00,  ...,  4.8602e-01,\n",
       "             1.3902e+00, -2.4465e-01],\n",
       "           [ 1.2089e+00, -2.6788e-02, -9.2599e-01,  ...,  2.4619e-01,\n",
       "             6.1336e-01, -1.0199e+00],\n",
       "           [-5.4070e-01,  2.1713e-01, -2.2316e-01,  ...,  1.1102e+00,\n",
       "             1.2503e+00, -7.3489e-01],\n",
       "           ...,\n",
       "           [ 1.0124e+00, -4.7252e-01,  1.8892e-01,  ...,  9.6293e-01,\n",
       "             1.1709e+00, -7.6129e-01],\n",
       "           [ 6.3166e-01, -1.6398e-01, -5.2246e-01,  ...,  9.0400e-01,\n",
       "             3.3278e-01, -5.6113e-01],\n",
       "           [-1.5363e-01,  3.5409e-01, -7.1314e-02,  ...,  6.8405e-02,\n",
       "            -5.9190e-01, -8.1028e-01]],\n",
       " \n",
       "          [[ 7.9803e-01, -9.0041e-01, -2.1234e-01,  ..., -5.6057e-01,\n",
       "            -8.1189e-01, -4.9513e-01],\n",
       "           [ 2.0766e+00, -4.6257e-01, -7.2003e-01,  ...,  9.0855e-01,\n",
       "             2.3795e-01,  2.8478e-01],\n",
       "           [ 1.2018e+00,  2.9457e-01, -2.3716e-01,  ..., -8.6215e-01,\n",
       "            -1.0469e+00, -2.5589e-01],\n",
       "           ...,\n",
       "           [ 1.0396e-01,  1.3106e+00,  3.9688e-01,  ...,  1.0533e+00,\n",
       "            -8.1700e-01, -1.2462e+00],\n",
       "           [-2.9767e-01,  8.6820e-01,  3.4992e-01,  ...,  1.0875e+00,\n",
       "            -5.1366e-01, -6.5560e-02],\n",
       "           [ 2.7013e-01,  2.1479e-01,  3.2649e-01,  ...,  1.4833e+00,\n",
       "            -3.6578e-01, -1.6649e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.6538e+00,  5.2653e-01, -3.3263e-01,  ...,  1.7580e-01,\n",
       "             6.1364e-02, -2.3620e+00],\n",
       "           [-1.6443e+00, -3.8954e-01, -5.8247e-01,  ..., -3.9556e-01,\n",
       "             9.3624e-01, -4.6985e-01],\n",
       "           [-7.0567e-01, -1.2760e-01, -9.0094e-01,  ..., -1.1948e+00,\n",
       "             1.8529e-01, -1.4593e-01],\n",
       "           ...,\n",
       "           [-3.5291e-01,  4.9472e-02, -6.7199e-01,  ...,  7.1487e-01,\n",
       "            -1.1742e+00, -3.7709e-01],\n",
       "           [-6.7394e-01,  3.3270e-01,  6.2739e-02,  ...,  2.6530e-01,\n",
       "            -7.6690e-01, -8.1388e-01],\n",
       "           [-2.4816e-01, -3.4557e-01,  2.3123e-01,  ...,  4.2049e-02,\n",
       "            -6.0966e-01,  8.9294e-02]],\n",
       " \n",
       "          [[-1.7954e+00,  8.8955e-02,  2.8525e-01,  ...,  2.6400e-01,\n",
       "            -1.5230e-01,  8.9241e-01],\n",
       "           [-1.4004e+00, -7.9789e-02, -1.1947e-01,  ..., -4.5661e-02,\n",
       "            -2.3854e+00,  2.3965e+00],\n",
       "           [-4.3836e-01,  2.9787e-01,  4.0098e-01,  ...,  9.5227e-01,\n",
       "            -1.8024e-02,  1.8694e+00],\n",
       "           ...,\n",
       "           [-5.9720e-01, -4.0896e-01, -5.7706e-02,  ...,  1.2553e-01,\n",
       "             1.2882e+00,  4.3298e-01],\n",
       "           [ 2.9739e-01,  5.4064e-02,  2.2874e-01,  ..., -4.1066e-01,\n",
       "             8.9066e-01,  5.8559e-01],\n",
       "           [-3.5433e-01, -1.2750e-01,  2.1155e-01,  ..., -1.2772e+00,\n",
       "             1.0842e+00, -4.8461e-02]],\n",
       " \n",
       "          [[ 4.6495e-01,  6.0036e-01,  1.8118e-01,  ..., -3.8030e+00,\n",
       "            -5.8067e-01, -9.6768e-01],\n",
       "           [ 3.0958e-01,  3.8874e-01, -1.5697e-01,  ..., -6.3320e+00,\n",
       "            -1.3452e-01, -1.0660e+00],\n",
       "           [-5.8295e-01, -8.9984e-03,  7.2424e-04,  ..., -3.8291e+00,\n",
       "            -9.1457e-01, -2.8487e-01],\n",
       "           ...,\n",
       "           [ 8.7873e-01, -4.5605e-01,  2.0274e-01,  ..., -7.1160e+00,\n",
       "             2.3493e-01, -4.5672e-01],\n",
       "           [-5.4326e-02, -4.2484e-01, -3.5491e-02,  ..., -6.1324e+00,\n",
       "             8.0332e-02, -9.7288e-01],\n",
       "           [-5.0412e-01,  4.6393e-01,  3.0871e-01,  ..., -6.7250e+00,\n",
       "             3.8664e-02,  4.7630e-02]]]], device='cuda:0'),\n",
       " tensor([[[[ 6.2119e-01, -1.3291e-01,  1.0848e+00,  ...,  1.4751e+00,\n",
       "             1.8720e-01, -4.6421e+00],\n",
       "           [ 6.0654e-01, -6.7119e-01,  1.3598e-01,  ...,  2.2364e-01,\n",
       "            -2.0224e+00, -6.6157e+00],\n",
       "           [-9.9117e-01,  3.1772e-01,  4.2821e-01,  ...,  1.0424e+00,\n",
       "            -1.3376e-01, -5.4787e+00],\n",
       "           ...,\n",
       "           [ 6.2139e-01, -3.0578e-01, -1.2265e-01,  ..., -8.5610e-02,\n",
       "            -1.8262e+00, -8.4001e+00],\n",
       "           [ 9.0604e-01,  1.1811e-01,  1.8042e-02,  ...,  3.2210e-01,\n",
       "            -1.8386e+00, -8.3685e+00],\n",
       "           [-2.9658e-01, -6.4820e-01,  6.6086e-01,  ...,  3.3410e-01,\n",
       "            -1.7901e+00, -7.5216e+00]],\n",
       " \n",
       "          [[ 1.2042e+00,  3.2998e-01, -8.5099e-01,  ...,  9.9520e-01,\n",
       "            -6.0133e-02, -1.2588e+00],\n",
       "           [ 1.0381e+00,  7.6841e-01, -4.1162e-01,  ...,  1.6216e+00,\n",
       "             6.6832e-02, -9.4097e-01],\n",
       "           [-9.2918e-01,  7.8536e-01,  4.6194e-01,  ...,  8.0517e-01,\n",
       "            -5.4642e-01,  1.0179e+00],\n",
       "           ...,\n",
       "           [ 8.0366e-01, -3.8019e-01, -3.7349e-02,  ...,  1.0248e+00,\n",
       "            -1.4597e-01, -1.1942e+00],\n",
       "           [ 1.5784e-01,  3.0285e-01,  1.8703e-01,  ...,  7.6845e-01,\n",
       "            -5.4821e-01, -3.1366e-01],\n",
       "           [-2.7248e-01,  3.2065e-01, -5.9951e-01,  ...,  1.0638e+00,\n",
       "            -2.0658e+00, -2.1225e-01]],\n",
       " \n",
       "          [[ 1.2160e+00,  1.4194e-01,  1.6883e-01,  ...,  2.1189e+00,\n",
       "            -1.6006e+00, -1.9715e+00],\n",
       "           [ 1.4355e+00,  3.6888e-01,  9.5927e-03,  ...,  3.7421e+00,\n",
       "            -1.3306e-01,  2.6660e-01],\n",
       "           [-5.6705e-01, -4.9015e-01, -8.3965e-01,  ...,  1.1199e+00,\n",
       "            -2.9439e-01, -5.2368e-01],\n",
       "           ...,\n",
       "           [ 5.7560e-01, -1.0555e+00,  4.2841e-01,  ..., -6.9739e-02,\n",
       "            -2.2366e+00, -2.0240e+00],\n",
       "           [-2.2816e-01, -7.5516e-01, -1.3119e-01,  ...,  3.3932e-02,\n",
       "            -7.0217e-01, -2.2174e+00],\n",
       "           [-8.1752e-01, -2.5621e-02, -1.9039e-01,  ...,  1.0820e+00,\n",
       "             2.9555e-01, -1.9239e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.9214e-01, -6.2801e-01,  1.0644e-01,  ...,  1.0381e+00,\n",
       "            -7.4319e-01,  1.4279e+00],\n",
       "           [-1.1869e+00, -2.9236e-01, -3.3878e-01,  ...,  1.9038e+00,\n",
       "            -2.2773e+00,  1.5496e+00],\n",
       "           [-4.9430e-01, -5.9053e-01, -1.0317e-01,  ...,  1.2451e+00,\n",
       "            -2.1312e+00,  1.4058e+00],\n",
       "           ...,\n",
       "           [ 8.2313e-02,  2.2765e-01,  1.5400e-01,  ...,  1.9373e-01,\n",
       "            -2.0115e+00,  4.0438e+00],\n",
       "           [-8.4558e-01,  5.6038e-01,  5.5444e-02,  ..., -6.5725e-01,\n",
       "            -1.3556e+00,  2.8138e+00],\n",
       "           [-5.8552e-01,  1.1152e-01, -2.5470e-01,  ..., -3.2081e-01,\n",
       "            -1.7088e+00,  3.4117e+00]],\n",
       " \n",
       "          [[ 8.6548e-01,  7.9166e-01, -1.0270e+00,  ..., -1.7707e-01,\n",
       "             5.0054e-01, -1.3881e+00],\n",
       "           [ 1.7227e+00,  4.8240e-01, -4.1952e-01,  ...,  3.0106e+00,\n",
       "            -1.4170e+00, -5.2278e-01],\n",
       "           [ 4.9249e-01, -4.4134e-01,  1.0941e+00,  ...,  1.4300e+00,\n",
       "             1.6737e+00, -1.0242e+00],\n",
       "           ...,\n",
       "           [ 1.5583e-01, -4.6230e-01,  3.7665e-01,  ..., -1.4373e+00,\n",
       "             1.5800e-01, -3.0264e-01],\n",
       "           [-2.1830e-01,  4.3844e-01,  6.9107e-01,  ..., -1.2500e+00,\n",
       "             4.5515e-02, -5.2288e-01],\n",
       "           [-1.1392e+00,  4.7573e-01,  9.9057e-03,  ...,  1.8779e-02,\n",
       "             1.2317e+00,  4.4824e-02]],\n",
       " \n",
       "          [[-1.3447e+00, -8.6179e-01,  5.9283e-01,  ...,  1.8142e+00,\n",
       "             2.5036e-01, -1.1816e+00],\n",
       "           [-3.5128e+00, -1.5341e+00,  9.5504e-01,  ...,  4.7717e-01,\n",
       "            -8.9972e-01,  9.6928e-01],\n",
       "           [-1.9275e+00, -3.2273e-01,  2.6998e-01,  ...,  1.4729e+00,\n",
       "            -3.6921e-01,  5.2829e-03],\n",
       "           ...,\n",
       "           [-1.4147e+00, -3.3994e-01,  8.1821e-01,  ...,  2.9668e+00,\n",
       "             1.0657e+00, -2.2160e+00],\n",
       "           [-1.3414e-01,  4.5076e-02,  1.8896e-01,  ...,  2.2826e+00,\n",
       "             6.7175e-01, -2.4740e+00],\n",
       "           [ 5.9742e-01,  5.9121e-01, -7.2763e-01,  ...,  9.1639e-01,\n",
       "             8.0075e-01, -1.2394e+00]]]], device='cuda:0'),\n",
       " tensor([[[[ 7.7318e-01,  1.2709e-01,  7.9880e-02,  ..., -6.7836e-01,\n",
       "            -8.4407e-01, -9.6633e-01],\n",
       "           [ 1.0984e+00,  5.5708e-02, -1.3475e+00,  ..., -2.2088e+00,\n",
       "            -2.1095e-01, -1.0045e+00],\n",
       "           [-6.8588e-01,  4.3408e-02, -1.3454e-01,  ..., -2.1144e+00,\n",
       "            -1.0402e+00, -1.9107e+00],\n",
       "           ...,\n",
       "           [ 3.4479e-01,  2.3144e-01,  5.5461e-01,  ..., -4.5448e+00,\n",
       "            -2.1664e-01, -8.8324e-01],\n",
       "           [-4.7229e-01,  3.0819e-01,  6.0437e-01,  ..., -4.8488e+00,\n",
       "            -1.2461e+00, -1.1171e+00],\n",
       "           [-1.0449e-01,  4.4657e-01, -1.4512e-01,  ..., -4.7686e+00,\n",
       "            -9.8806e-01, -1.0162e+00]],\n",
       " \n",
       "          [[ 4.5756e-01,  3.4001e-01,  7.9628e-01,  ...,  1.0742e+00,\n",
       "             2.6588e-01, -2.0752e-02],\n",
       "           [-2.4090e+00,  1.0470e-01, -1.5281e-01,  ...,  1.3464e+00,\n",
       "            -5.6826e-01,  3.9984e-01],\n",
       "           [-3.7887e+00,  6.8225e-02, -4.4485e-01,  ...,  1.1699e+00,\n",
       "             2.6460e-01,  5.2272e-01],\n",
       "           ...,\n",
       "           [-1.1057e+00,  1.1386e-01, -6.4225e-01,  ...,  3.0662e-01,\n",
       "             1.3105e+00,  7.4958e-01],\n",
       "           [-2.9036e+00,  1.7515e-01, -4.0886e-01,  ...,  1.9413e+00,\n",
       "             2.2439e+00,  9.1790e-01],\n",
       "           [-2.3346e+00,  5.0807e-01, -5.5796e-01,  ...,  7.8575e-01,\n",
       "             1.2119e+00, -4.3055e-01]],\n",
       " \n",
       "          [[ 3.9209e-01,  8.2627e-01,  5.7186e-01,  ..., -2.6788e-01,\n",
       "             3.9625e-01,  1.3142e+00],\n",
       "           [ 3.6152e-01, -3.8699e-01,  2.2796e-01,  ..., -3.9082e+00,\n",
       "            -6.9537e-01,  2.1179e+00],\n",
       "           [ 6.0307e-01,  1.8863e-01, -2.3972e-01,  ..., -1.6614e+00,\n",
       "             8.4840e-02,  1.2397e+00],\n",
       "           ...,\n",
       "           [-7.8696e-02,  2.9212e-02, -5.8747e-01,  ...,  6.9564e-02,\n",
       "            -1.1167e+00,  2.1638e+00],\n",
       "           [ 5.0640e-01, -6.8045e-01,  5.4810e-03,  ..., -6.4422e-01,\n",
       "            -6.5385e-01,  2.9855e+00],\n",
       "           [ 1.1638e+00,  4.5171e-01, -3.1724e-01,  ..., -3.2602e+00,\n",
       "            -8.7792e-01,  2.0455e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.1529e-02, -1.0204e+00, -4.2205e-01,  ..., -1.9605e+00,\n",
       "            -3.3081e+00,  7.6869e-03],\n",
       "           [ 3.1087e-01, -2.5414e-01,  1.3045e-01,  ...,  9.8415e-01,\n",
       "            -3.0052e+00,  2.7956e+00],\n",
       "           [-4.4195e-01, -8.6480e-01,  9.1880e-02,  ..., -1.2847e+00,\n",
       "            -3.8380e+00, -1.7187e+00],\n",
       "           ...,\n",
       "           [ 1.0469e+00, -2.2446e-01, -2.7726e-01,  ..., -2.6982e+00,\n",
       "            -7.4126e+00, -2.3271e+00],\n",
       "           [ 2.3571e-01,  4.5611e-01,  2.6566e-01,  ..., -1.1029e+00,\n",
       "            -8.2442e+00, -1.9514e+00],\n",
       "           [ 1.2811e-02, -8.4388e-01,  7.1050e-01,  ..., -2.0687e+00,\n",
       "            -8.3849e+00, -1.6262e+00]],\n",
       " \n",
       "          [[ 1.2429e+00, -2.8982e-01, -1.6024e-01,  ..., -3.3721e-01,\n",
       "             8.6513e-01, -3.2306e-01],\n",
       "           [ 2.3049e+00, -8.1397e-01, -9.7533e-02,  ...,  3.3566e+00,\n",
       "            -1.0155e+00,  2.6106e+00],\n",
       "           [ 1.8928e-01, -1.0320e+00,  6.5583e-02,  ...,  1.3957e+00,\n",
       "            -1.7236e+00,  1.1277e+00],\n",
       "           ...,\n",
       "           [ 4.4006e-01,  2.8855e-01, -1.1977e-01,  ...,  6.4547e-01,\n",
       "            -3.4848e+00,  9.3189e-01],\n",
       "           [-1.5172e-01,  1.0767e-01,  5.0330e-01,  ...,  9.6858e-01,\n",
       "            -2.8323e+00,  1.6795e+00],\n",
       "           [ 4.8891e-02,  2.7794e-01,  6.5019e-01,  ...,  2.1429e+00,\n",
       "            -3.4130e+00,  1.1795e+00]],\n",
       " \n",
       "          [[-1.1200e+00, -1.6472e-01,  3.4692e-01,  ...,  2.4867e-01,\n",
       "             1.2192e+00,  1.1446e+00],\n",
       "           [-1.5745e+00, -4.5775e-01,  4.9774e-01,  ...,  2.6271e-01,\n",
       "             1.3274e-01,  9.3528e-01],\n",
       "           [ 4.6833e-01, -4.9315e-01,  5.6801e-01,  ..., -2.9866e-01,\n",
       "             5.0124e-01,  9.3841e-01],\n",
       "           ...,\n",
       "           [-5.6666e-01, -6.1636e-01, -5.6623e-02,  ..., -1.0208e+00,\n",
       "             1.3958e+00,  5.0199e-01],\n",
       "           [-2.8317e-01, -6.9572e-01, -2.8967e-01,  ..., -1.6760e+00,\n",
       "             1.2464e+00,  7.0154e-01],\n",
       "           [ 1.7137e-01, -1.0172e+00, -1.9342e-01,  ..., -6.1729e-01,\n",
       "            -1.1182e-02, -8.3592e-01]]]], device='cuda:0'),\n",
       " tensor([[[[-7.7304e-01,  3.8709e-01,  5.0564e-02,  ..., -9.3732e-01,\n",
       "             2.4675e-01,  9.5038e-01],\n",
       "           [-1.7888e+00,  1.5506e-01,  9.3745e-01,  ..., -4.7163e-01,\n",
       "             8.8699e-02,  7.4490e-01],\n",
       "           [-3.4432e-01,  2.1838e-01,  4.5697e-01,  ..., -9.1569e-02,\n",
       "             5.1253e-02,  2.1863e-01],\n",
       "           ...,\n",
       "           [-1.9654e-01,  2.7690e-01,  2.0205e-01,  ..., -1.0252e+00,\n",
       "            -5.5984e-02, -5.8967e-01],\n",
       "           [-1.5780e-01, -5.5716e-01,  6.0167e-01,  ..., -7.7775e-01,\n",
       "            -5.2652e-01, -1.6000e-02],\n",
       "           [-6.4339e-01, -6.3504e-02,  6.6010e-01,  ..., -2.7298e-01,\n",
       "             2.7966e-01,  9.6764e-02]],\n",
       " \n",
       "          [[ 4.9703e-01,  6.9578e-01, -8.2521e-01,  ...,  1.2935e+00,\n",
       "            -2.1064e+00, -1.9887e+00],\n",
       "           [ 1.0723e+00,  4.5966e-01, -2.9902e-01,  ...,  1.4676e+00,\n",
       "            -1.7199e+00, -1.4988e+00],\n",
       "           [ 5.7234e-01, -4.9663e-02, -2.9646e-01,  ..., -4.1036e-02,\n",
       "            -2.0597e+00, -2.5239e+00],\n",
       "           ...,\n",
       "           [-3.1411e-01,  2.8859e-01,  1.7658e-01,  ..., -1.6051e+00,\n",
       "             3.3336e-01, -1.9482e+00],\n",
       "           [-5.3295e-01,  3.6921e-03,  4.4748e-01,  ..., -2.3428e+00,\n",
       "            -8.6335e-01, -2.7493e+00],\n",
       "           [ 4.7874e-01,  4.7028e-01,  9.8650e-01,  ..., -2.4307e+00,\n",
       "            -1.4849e+00, -1.8587e+00]],\n",
       " \n",
       "          [[-6.4105e-01,  1.0725e+00, -5.0737e-01,  ...,  3.3362e+00,\n",
       "            -1.7827e+00,  6.7607e+00],\n",
       "           [-2.9079e-01,  3.0432e-01,  3.4526e-01,  ...,  3.5548e+00,\n",
       "             1.3493e-01,  6.9793e+00],\n",
       "           [ 1.4905e-01, -6.8368e-01,  4.2517e-01,  ...,  4.1937e+00,\n",
       "            -1.1299e+00,  7.4916e+00],\n",
       "           ...,\n",
       "           [ 2.9434e-01, -1.0080e-01, -2.2398e-02,  ...,  1.3420e+00,\n",
       "            -1.5836e-01,  8.5662e+00],\n",
       "           [-1.7911e-01,  3.6818e-01,  7.0823e-01,  ...,  3.5158e+00,\n",
       "            -5.6283e+00,  9.0364e+00],\n",
       "           [-4.3897e-01,  2.7978e-01,  5.5565e-01,  ...,  2.3394e+00,\n",
       "            -3.0784e+00,  8.6526e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.3385e+00, -2.2338e-01,  2.2169e-01,  ..., -4.5533e-01,\n",
       "            -1.4550e+00, -1.5515e+00],\n",
       "           [ 4.2937e-01, -1.4126e-01,  3.7956e-01,  ...,  1.1022e-01,\n",
       "            -2.1453e+00, -8.7862e-01],\n",
       "           [-1.0256e+00,  2.2381e-01,  2.2323e-02,  ..., -7.1667e-01,\n",
       "            -2.1943e+00, -8.7526e-01],\n",
       "           ...,\n",
       "           [ 2.4036e-01,  2.6451e-01, -1.9917e-01,  ..., -2.4293e+00,\n",
       "            -3.3682e-01, -9.0062e-01],\n",
       "           [-4.6883e-01,  3.6932e-01, -4.6436e-02,  ..., -2.7977e+00,\n",
       "            -1.1514e+00, -1.3124e+00],\n",
       "           [-5.4535e-01,  3.1865e-01, -2.9993e-01,  ..., -2.2560e+00,\n",
       "            -1.2966e+00, -1.8662e+00]],\n",
       " \n",
       "          [[-6.0121e-01, -6.3572e-01, -2.9590e-01,  ..., -6.1859e-01,\n",
       "            -3.8982e+00, -1.8989e-01],\n",
       "           [ 4.5011e-01, -7.6234e-01, -8.0792e-02,  ..., -1.5707e+00,\n",
       "            -4.8184e+00, -1.3779e+00],\n",
       "           [ 1.4258e+00, -6.0329e-02,  6.6667e-01,  ..., -1.1740e+00,\n",
       "            -4.6957e+00, -8.7729e-01],\n",
       "           ...,\n",
       "           [-8.8285e-01, -7.6635e-01,  9.1124e-01,  ...,  1.4100e+00,\n",
       "            -8.1111e+00, -1.2909e+00],\n",
       "           [ 4.9012e-01, -2.1996e-01,  4.3176e-01,  ...,  4.2632e-01,\n",
       "            -9.6552e+00, -1.4773e+00],\n",
       "           [ 9.2272e-01, -3.6041e-01, -2.8873e-01,  ..., -1.9894e+00,\n",
       "            -9.8605e+00, -6.4002e-01]],\n",
       " \n",
       "          [[ 7.1215e-03,  9.3189e-02, -4.5319e-02,  ..., -1.5834e-02,\n",
       "            -9.4609e-02,  3.2608e-01],\n",
       "           [ 8.2776e-01, -3.5558e-01, -7.6834e-01,  ...,  4.7816e-01,\n",
       "             3.1781e+00,  6.2820e-01],\n",
       "           [ 9.0278e-01, -1.3319e-01, -7.9264e-01,  ...,  8.0373e-01,\n",
       "             9.2450e-02, -2.7323e-01],\n",
       "           ...,\n",
       "           [ 5.9277e-01,  3.0428e-01,  6.8576e-01,  ..., -2.2520e-01,\n",
       "            -3.3543e+00,  1.3896e+00],\n",
       "           [ 2.6780e-02, -6.1946e-01,  8.9202e-02,  ..., -2.4564e-01,\n",
       "            -4.3277e+00,  1.7163e+00],\n",
       "           [ 2.5185e-01, -8.7125e-01, -5.5656e-01,  ..., -1.0247e-01,\n",
       "            -1.9033e+00,  1.1565e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 8.4585e-01,  5.5565e-01,  2.6956e-02,  ..., -2.5465e-02,\n",
       "             5.9706e-01, -5.4993e-01],\n",
       "           [ 7.9563e-01, -3.1150e-01, -2.2649e-01,  ...,  4.3638e-01,\n",
       "            -5.9227e-01, -4.8034e-01],\n",
       "           [-6.7371e-01,  3.1179e-01, -4.2799e-01,  ...,  7.6742e-01,\n",
       "             2.2290e-01,  3.5807e-01],\n",
       "           ...,\n",
       "           [ 7.1048e-01, -1.3863e+00,  4.1917e-01,  ...,  4.2887e-01,\n",
       "            -2.8123e-01,  7.5306e-01],\n",
       "           [-5.0186e-01, -1.6523e-01,  2.3935e-01,  ...,  1.5425e-01,\n",
       "            -6.3313e-01, -9.3953e-02],\n",
       "           [-8.0232e-01,  6.0928e-01, -7.0955e-01,  ...,  7.0120e-01,\n",
       "            -7.2427e-01, -2.6748e-01]],\n",
       " \n",
       "          [[ 9.4534e-01,  5.0434e-01,  6.0746e-01,  ..., -1.1551e+00,\n",
       "             2.0124e+00, -1.4825e+00],\n",
       "           [-3.2801e-01, -2.5535e-01, -2.8811e-01,  ..., -1.1318e+00,\n",
       "             1.6346e+00,  1.2261e+00],\n",
       "           [-1.8700e+00, -1.7770e-01, -3.1477e-01,  ..., -2.4156e+00,\n",
       "             1.1309e+00, -1.1135e+00],\n",
       "           ...,\n",
       "           [ 1.1348e-01,  2.8535e-01, -3.0941e-01,  ..., -1.3997e+00,\n",
       "            -9.7630e-01, -2.5071e-01],\n",
       "           [-4.7391e-01, -2.8432e-01,  1.4622e-01,  ..., -9.6663e-01,\n",
       "            -7.8776e-01, -1.0442e+00],\n",
       "           [-5.4561e-01, -4.1490e-01,  1.0420e-01,  ..., -2.8055e+00,\n",
       "            -5.2649e-01, -1.8850e+00]],\n",
       " \n",
       "          [[ 2.2580e-02, -6.2013e-02,  1.0032e-01,  ...,  5.9156e-01,\n",
       "            -2.7870e+00,  2.9349e-01],\n",
       "           [-3.8363e-02, -1.1528e-02, -1.3409e-01,  ...,  2.6512e-01,\n",
       "            -1.9818e+00,  6.5460e-01],\n",
       "           [-4.0625e-01, -2.6170e-01, -2.1243e-03,  ..., -1.3511e+00,\n",
       "            -3.6910e+00, -4.2036e-02],\n",
       "           ...,\n",
       "           [ 6.1653e-01, -1.8780e-01,  4.2212e-01,  ...,  2.4177e+00,\n",
       "            -2.5583e+00,  3.6920e+00],\n",
       "           [-1.7950e-01, -2.8769e-01,  3.1399e-01,  ..., -3.7949e-01,\n",
       "            -5.9613e+00,  2.9867e-02],\n",
       "           [-2.9188e-01,  4.2484e-02,  1.0570e-01,  ...,  1.2554e+00,\n",
       "            -4.9354e+00,  3.5271e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.3591e+00, -3.3353e-01,  5.8340e-01,  ...,  5.8773e-01,\n",
       "            -9.5564e-01,  1.1168e-01],\n",
       "           [-4.7176e-01, -8.0852e-01, -1.2892e-01,  ...,  9.2350e-01,\n",
       "            -2.3145e+00, -8.1550e-01],\n",
       "           [ 1.7943e+00, -8.9958e-01, -8.0701e-03,  ..., -3.7510e-01,\n",
       "            -1.6266e+00, -1.5055e+00],\n",
       "           ...,\n",
       "           [-5.3376e-01,  4.3962e-01, -2.4063e-01,  ..., -8.7556e-01,\n",
       "            -2.7078e+00, -1.6594e+00],\n",
       "           [ 6.9621e-01, -8.6268e-02,  5.7669e-01,  ..., -1.6125e-01,\n",
       "            -2.2641e+00, -1.5265e+00],\n",
       "           [ 4.7962e-01,  6.0797e-01,  2.0338e-02,  ..., -4.5200e-01,\n",
       "            -2.2056e+00, -6.8805e-01]],\n",
       " \n",
       "          [[ 1.6628e+00, -7.7941e-02,  2.8718e-01,  ..., -1.5629e+00,\n",
       "            -1.1000e+00,  2.9287e+00],\n",
       "           [ 7.0765e-01,  1.7746e-01, -2.9228e-01,  ..., -1.2720e+00,\n",
       "            -1.4630e+00,  3.4566e+00],\n",
       "           [-1.2646e+00,  9.0511e-02, -1.3615e-01,  ..., -1.3045e+00,\n",
       "            -1.5183e+00,  3.4804e+00],\n",
       "           ...,\n",
       "           [ 7.4803e-01, -2.4768e-01,  3.7391e-01,  ..., -2.1697e-01,\n",
       "             2.0224e-01,  4.6024e+00],\n",
       "           [ 3.5322e-01,  1.9328e-01,  4.6059e-01,  ...,  6.2146e-01,\n",
       "            -1.7050e-01,  4.6857e+00],\n",
       "           [-5.1665e-01,  4.6930e-01,  2.2672e-02,  ...,  1.0497e+00,\n",
       "             4.5325e-01,  4.2236e+00]],\n",
       " \n",
       "          [[ 1.3667e+00,  3.7680e-01, -5.6877e-02,  ...,  1.6212e+00,\n",
       "             8.4389e-01,  1.7375e+00],\n",
       "           [ 3.2363e+00, -4.0862e-01, -2.8694e-01,  ...,  5.3391e-01,\n",
       "             2.2956e+00,  2.6734e+00],\n",
       "           [ 1.5015e+00, -2.7469e-01, -7.8390e-01,  ..., -1.1571e+00,\n",
       "             1.0762e+00,  2.4680e+00],\n",
       "           ...,\n",
       "           [ 7.0387e-01, -1.1259e+00, -2.4251e-01,  ..., -1.2535e+00,\n",
       "            -5.3098e-01,  2.6501e+00],\n",
       "           [ 4.5481e-01, -5.5387e-01,  7.7265e-01,  ..., -7.4381e-01,\n",
       "            -9.8540e-01,  2.5191e+00],\n",
       "           [-2.0617e-01,  5.5581e-01,  5.7330e-01,  ..., -1.8592e+00,\n",
       "             1.1093e+00,  2.7920e+00]]]], device='cuda:0'),\n",
       " tensor([[[[ 1.7864e+00, -3.8206e-01, -4.1313e-01,  ..., -4.8882e-01,\n",
       "            -3.5981e-01, -4.4713e-01],\n",
       "           [ 1.6555e+00,  1.7982e-01,  7.8005e-02,  ...,  8.9659e-01,\n",
       "            -3.7435e-01, -1.1194e-01],\n",
       "           [-5.7147e-01,  2.1133e-01, -8.3056e-01,  ..., -1.0846e+00,\n",
       "             1.2492e+00, -9.3325e-01],\n",
       "           ...,\n",
       "           [ 1.8045e+00, -3.0714e-01, -2.5925e-01,  ...,  1.6182e+00,\n",
       "             1.4991e+00, -1.1652e+00],\n",
       "           [-1.5025e-01,  1.0284e-01,  1.4317e-01,  ...,  1.6333e+00,\n",
       "             5.3744e-01, -8.2982e-02],\n",
       "           [-9.0275e-01,  3.6623e-01,  5.5967e-03,  ...,  1.1504e+00,\n",
       "             9.5525e-01, -5.1741e-01]],\n",
       " \n",
       "          [[ 4.8188e-01, -6.9420e-01, -4.8319e-02,  ..., -3.5832e-03,\n",
       "            -1.0515e+00,  1.1493e+00],\n",
       "           [-8.1399e-01,  1.1912e-01, -6.0968e-01,  ...,  9.0787e-01,\n",
       "             5.9168e-02,  1.7688e+00],\n",
       "           [-1.7951e+00,  1.9749e-01, -1.2639e-01,  ...,  7.6273e-01,\n",
       "            -1.0200e-01,  1.9122e+00],\n",
       "           ...,\n",
       "           [-3.4997e-01, -2.6886e-01,  3.0201e-01,  ..., -1.3855e+00,\n",
       "             1.1985e+00,  3.3358e-01],\n",
       "           [-5.0511e-01, -3.9167e-01,  4.0065e-01,  ..., -1.5344e+00,\n",
       "             3.1857e-01,  3.8427e-01],\n",
       "           [-1.1126e-01, -4.4689e-01, -4.5904e-01,  ..., -7.7036e-01,\n",
       "             1.2463e+00,  7.1199e-01]],\n",
       " \n",
       "          [[-1.5295e-01, -2.1938e-01, -6.5177e-01,  ...,  1.3025e+00,\n",
       "            -1.8586e+00,  7.3168e-01],\n",
       "           [-1.3465e+00,  2.5940e-01, -6.2605e-03,  ..., -3.7536e-01,\n",
       "             1.3152e+00,  2.6610e+00],\n",
       "           [-1.1472e+00, -2.2200e-01, -6.2361e-01,  ..., -1.2089e-01,\n",
       "             1.5164e+00,  1.5370e+00],\n",
       "           ...,\n",
       "           [-8.2106e-02,  7.0053e-01, -1.0474e+00,  ..., -2.7763e-01,\n",
       "             2.6140e+00,  1.2831e+00],\n",
       "           [-2.9243e-01, -9.5020e-02, -5.7344e-01,  ..., -7.9854e-02,\n",
       "             3.5778e+00,  1.9807e+00],\n",
       "           [-6.0630e-01, -3.2863e-01, -3.6052e-01,  ..., -1.9522e-01,\n",
       "             3.5106e+00,  2.0180e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.0122e+00, -2.2136e-01,  1.7301e-01,  ..., -1.1136e-01,\n",
       "            -7.8403e-01,  2.3727e+00],\n",
       "           [-5.6493e-01, -1.4250e-01,  2.2795e-01,  ..., -3.3904e-01,\n",
       "             8.1366e-01,  2.3497e+00],\n",
       "           [-1.2865e+00, -3.7615e-01, -8.6728e-01,  ..., -8.7860e-02,\n",
       "             8.0273e-01,  2.2389e+00],\n",
       "           ...,\n",
       "           [-4.6644e-01,  1.3294e-01, -7.6121e-01,  ...,  7.5695e-01,\n",
       "            -1.6024e+00,  8.8459e-01],\n",
       "           [-1.3663e+00,  1.2081e+00, -6.0105e-01,  ...,  7.3610e-01,\n",
       "            -2.2853e+00,  1.6361e+00],\n",
       "           [-5.1124e-02,  4.7206e-01,  8.0205e-01,  ..., -9.8944e-01,\n",
       "            -2.5446e+00,  9.6370e-01]],\n",
       " \n",
       "          [[-1.5108e+00, -1.7683e-02,  1.3425e-01,  ..., -7.3930e-01,\n",
       "             1.4396e+00,  1.1392e+00],\n",
       "           [-5.3804e-01,  1.9723e-01, -8.5479e-02,  ...,  4.2372e-01,\n",
       "             1.1245e+00,  5.5875e-01],\n",
       "           [ 6.4431e-01,  1.1964e+00,  1.8375e-01,  ..., -3.3209e-02,\n",
       "             1.8513e+00, -3.2197e-01],\n",
       "           ...,\n",
       "           [ 4.8443e-01, -4.1976e-02,  2.1991e-02,  ..., -6.0442e-01,\n",
       "             3.6944e+00,  1.9635e+00],\n",
       "           [ 3.5636e-01,  5.5074e-01,  1.2958e-02,  ..., -7.8009e-01,\n",
       "             3.1597e+00,  2.2583e+00],\n",
       "           [-4.4583e-02,  3.5004e-01,  5.6146e-01,  ...,  1.2127e-01,\n",
       "             3.4600e+00,  1.2064e+00]],\n",
       " \n",
       "          [[ 2.5149e-01, -8.2930e-02, -6.2443e-01,  ..., -2.4191e+00,\n",
       "             9.0095e-02,  8.4866e-02],\n",
       "           [-1.4041e+00, -1.2284e+00,  6.9522e-02,  ..., -2.4458e+00,\n",
       "             7.2334e-01,  3.2681e+00],\n",
       "           [-1.8708e+00, -1.6075e+00, -3.5652e-01,  ..., -1.8349e+00,\n",
       "            -5.9713e-02,  4.8507e-01],\n",
       "           ...,\n",
       "           [-5.0353e-01, -5.6416e-01,  5.1667e-01,  ...,  1.1785e+00,\n",
       "            -7.5874e-01, -1.4323e+00],\n",
       "           [-8.2885e-01,  3.6927e-01,  6.6280e-01,  ...,  2.8137e-01,\n",
       "            -1.8315e+00, -1.5528e+00],\n",
       "           [-7.0487e-01,  8.1692e-03, -8.0154e-02,  ...,  7.6067e-02,\n",
       "            -2.3675e+00, -8.7780e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 2.3758, -0.0440,  0.4663,  ..., -0.3606, -1.6143,  1.0865],\n",
       "           [ 2.8180,  0.1818,  0.3837,  ...,  0.1699, -1.7795,  1.4341],\n",
       "           [-0.5930, -0.7162, -0.5404,  ..., -0.1147, -1.2810,  1.8267],\n",
       "           ...,\n",
       "           [ 1.3762,  0.4234, -0.6655,  ...,  1.2203, -1.4681,  0.2999],\n",
       "           [-0.7664,  0.1297, -1.1356,  ...,  0.8874, -1.0128,  1.1304],\n",
       "           [-1.7643,  0.1651, -1.0738,  ...,  0.9427, -0.4620,  0.2022]],\n",
       " \n",
       "          [[ 1.8649,  0.5097,  0.2020,  ...,  2.8436, -1.0463, -1.4088],\n",
       "           [ 1.8954,  0.8616, -0.2298,  ...,  3.6500, -0.6636, -1.6658],\n",
       "           [ 0.4196, -0.2411, -0.1152,  ...,  4.6639, -0.5626, -1.6026],\n",
       "           ...,\n",
       "           [ 0.6512, -0.0954, -0.4924,  ...,  6.0337, -1.0818, -1.6225],\n",
       "           [ 0.2610, -0.0887, -0.3225,  ...,  7.3701, -2.2661, -2.6501],\n",
       "           [-0.0947, -0.4067,  0.0959,  ...,  6.6037, -1.0820, -3.2854]],\n",
       " \n",
       "          [[ 1.5527, -0.6959,  1.0076,  ...,  0.5464, -1.0217,  2.0090],\n",
       "           [ 1.5913, -0.3978, -0.8094,  ..., -0.8513, -2.2738,  1.5595],\n",
       "           [-0.8346, -0.2999, -0.1084,  ...,  0.8220, -1.3450,  1.0574],\n",
       "           ...,\n",
       "           [ 0.3364,  0.9093, -0.5288,  ...,  0.7182,  0.7828, -1.8347],\n",
       "           [-0.1788, -0.2131, -0.2462,  ...,  0.6530,  1.4754, -1.4258],\n",
       "           [-0.5459, -0.3694, -1.3441,  ..., -0.4545,  1.0268, -0.2298]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.7238, -0.7456,  0.4334,  ...,  1.6192, -1.9304, -1.1346],\n",
       "           [-1.0081, -0.5614,  0.5562,  ...,  0.0911, -2.3648, -2.4753],\n",
       "           [ 0.6689, -0.8379, -0.7960,  ...,  2.9231, -2.9968, -0.4314],\n",
       "           ...,\n",
       "           [ 0.7228,  0.6538,  0.3181,  ...,  7.4328, -7.7610,  4.0024],\n",
       "           [ 0.4291,  0.0479, -0.4958,  ...,  8.4673, -8.4147,  3.9062],\n",
       "           [ 0.4350, -0.4682, -1.1284,  ...,  6.6308, -6.2969,  3.4698]],\n",
       " \n",
       "          [[ 1.9499,  0.1805,  0.0181,  ..., -1.0501,  1.4578,  2.1865],\n",
       "           [ 2.0179, -0.0450, -0.4848,  ..., -0.6573,  0.0750,  0.8975],\n",
       "           [-0.3485, -0.3047,  0.7312,  ..., -0.6354, -0.2384,  1.8513],\n",
       "           ...,\n",
       "           [ 0.6897, -0.4966,  0.1153,  ..., -1.0595,  0.9076,  2.2609],\n",
       "           [ 0.6871, -0.9133, -0.0757,  ..., -1.0019,  0.7822,  2.8509],\n",
       "           [ 0.1592, -0.4339,  0.0512,  ..., -0.7165,  0.2781,  2.2384]],\n",
       " \n",
       "          [[-0.1350, -1.1683, -1.1590,  ..., -0.4309, -0.6198,  0.6639],\n",
       "           [ 0.8878, -0.0244, -0.1856,  ..., -0.3742, -1.1748,  0.9291],\n",
       "           [ 3.3621,  0.2373, -0.5689,  ..., -0.5860, -1.0877,  1.4478],\n",
       "           ...,\n",
       "           [-0.3960,  1.1912,  0.3947,  ...,  0.4828,  0.8945, -1.1027],\n",
       "           [ 0.8268,  0.6395,  0.4971,  ..., -0.0858,  0.8393, -0.5227],\n",
       "           [ 0.9060, -1.2285,  0.2394,  ...,  0.2472,  0.4468, -1.0497]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-5.0277e-02, -3.3270e-01,  2.1282e-01,  ..., -1.3065e+00,\n",
       "            -6.2679e-01,  1.2275e+00],\n",
       "           [-3.1536e-01, -2.8422e-01,  6.2416e-01,  ...,  2.6992e+00,\n",
       "             9.7274e-01, -3.5824e-01],\n",
       "           [-3.5548e-01,  2.5985e-01,  5.8943e-01,  ...,  4.0076e-01,\n",
       "            -5.4139e-02,  8.5779e-01],\n",
       "           ...,\n",
       "           [ 2.6059e-01, -7.6569e-01,  3.1733e-01,  ..., -1.8862e+00,\n",
       "            -1.5653e-01,  1.4715e+00],\n",
       "           [ 4.6166e-01, -9.2582e-01,  7.6328e-01,  ..., -2.6264e+00,\n",
       "            -4.0749e-01,  2.8606e+00],\n",
       "           [ 5.8445e-01, -4.7749e-01,  5.2991e-01,  ..., -1.0991e+00,\n",
       "             2.4837e-01,  1.6981e+00]],\n",
       " \n",
       "          [[-3.2530e-01,  2.0481e+00,  1.8135e-01,  ...,  6.2225e-01,\n",
       "            -3.1127e-02, -1.8821e+00],\n",
       "           [-9.0545e-01,  9.1221e-01,  2.0195e-01,  ..., -8.5568e-01,\n",
       "             1.4018e-01, -7.9615e-01],\n",
       "           [-5.6164e-02, -8.8023e-01,  4.2048e-01,  ..., -7.4317e-01,\n",
       "            -1.0642e+00, -1.5139e+00],\n",
       "           ...,\n",
       "           [ 1.1523e+00,  4.5212e-01, -1.9006e+00,  ..., -4.8947e-02,\n",
       "            -1.0470e+00,  7.3669e-02],\n",
       "           [-5.4000e-01,  4.0410e-01, -7.9273e-02,  ..., -1.4501e-01,\n",
       "            -1.4519e+00, -7.3789e-01],\n",
       "           [-1.6342e+00,  1.1762e-01, -8.1446e-01,  ..., -2.0958e-01,\n",
       "            -1.0125e+00, -1.0323e-01]],\n",
       " \n",
       "          [[-3.4640e-01,  4.0079e-01,  8.0877e-02,  ..., -1.0450e+00,\n",
       "             2.6676e+00, -9.6779e-01],\n",
       "           [ 1.2755e+00,  7.4391e-01,  3.8418e-01,  ...,  7.8266e-01,\n",
       "             3.1688e-01,  4.4959e-01],\n",
       "           [ 1.8185e+00, -9.6215e-02,  7.2583e-02,  ...,  1.2845e+00,\n",
       "             2.2410e+00, -8.3528e-01],\n",
       "           ...,\n",
       "           [ 9.7990e-01, -7.2585e-01, -3.4486e-01,  ..., -7.2063e-01,\n",
       "            -3.7117e-02, -2.8906e-01],\n",
       "           [ 2.1655e-01, -1.6115e-01, -8.1550e-01,  ..., -1.4734e-01,\n",
       "             8.7322e-01, -3.4979e-01],\n",
       "           [ 5.1065e-01,  6.6169e-01, -2.6348e-01,  ...,  7.8819e-01,\n",
       "             6.3267e-01, -1.3742e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 8.1637e-01, -9.5714e-01, -2.9944e-01,  ..., -1.6192e-01,\n",
       "             2.6611e+00, -6.3093e-01],\n",
       "           [ 5.2082e-01, -3.8548e-01,  4.8662e-01,  ..., -1.6896e+00,\n",
       "             1.3001e+00,  9.8515e-01],\n",
       "           [-1.5242e+00, -2.3970e-01,  6.2433e-01,  ..., -2.2170e-01,\n",
       "             2.6785e+00,  8.7198e-02],\n",
       "           ...,\n",
       "           [ 1.7353e-01, -4.1474e-01, -9.2409e-01,  ...,  1.8046e+00,\n",
       "             3.0752e+00, -9.4510e-01],\n",
       "           [-7.9068e-01, -2.6447e-01, -1.1901e-01,  ...,  1.5613e+00,\n",
       "             2.6085e+00, -2.8986e-01],\n",
       "           [-3.7940e-01,  2.8059e-01,  5.3879e-01,  ...,  9.3394e-01,\n",
       "             1.9072e+00,  2.1810e-01]],\n",
       " \n",
       "          [[-8.8294e-01, -6.4771e-01, -3.0646e-01,  ..., -8.6632e-01,\n",
       "            -1.7984e+00,  7.8249e+00],\n",
       "           [-7.3620e-01,  9.6222e-02, -2.0165e-01,  ...,  2.2017e-01,\n",
       "            -1.0874e+00,  8.1347e+00],\n",
       "           [ 1.5247e+00,  8.7731e-01, -6.1809e-01,  ...,  1.0263e-01,\n",
       "            -1.5250e+00,  8.7399e+00],\n",
       "           ...,\n",
       "           [-1.0391e+00,  4.5643e-01, -3.9446e-02,  ..., -2.9186e-01,\n",
       "            -8.8510e-01,  1.1199e+01],\n",
       "           [-2.0595e-02, -9.7602e-03,  6.7567e-02,  ...,  5.1133e-01,\n",
       "            -1.6485e+00,  1.2163e+01],\n",
       "           [ 6.2902e-01,  5.7135e-01, -1.7839e-01,  ..., -3.1529e-02,\n",
       "            -7.2195e-01,  1.0160e+01]],\n",
       " \n",
       "          [[-5.4195e-01, -1.0339e+00, -4.0739e-01,  ...,  3.3958e+00,\n",
       "             1.2268e+00, -3.8863e-01],\n",
       "           [-1.9990e-01, -4.0098e-01, -9.2705e-01,  ...,  3.2638e-01,\n",
       "            -2.8867e-01, -7.6965e-01],\n",
       "           [ 1.6164e+00,  5.7497e-01, -6.7833e-02,  ...,  1.9935e+00,\n",
       "             8.3792e-01,  1.4458e-01],\n",
       "           ...,\n",
       "           [-2.4620e-01,  4.4055e-01, -3.9134e-01,  ...,  1.2498e+00,\n",
       "             3.3335e+00,  1.7308e+00],\n",
       "           [ 8.4852e-02,  2.3882e-01, -4.3766e-01,  ...,  6.8476e-01,\n",
       "             3.8566e+00,  2.3345e+00],\n",
       "           [ 4.0519e-01,  4.5421e-01,  3.5141e-01,  ...,  3.5195e-01,\n",
       "             1.8523e+00,  2.1416e+00]]]], device='cuda:0'),\n",
       " tensor([[[[-7.3734e-01,  3.1526e-01,  4.1446e-01,  ...,  1.5995e+00,\n",
       "            -1.3965e-01,  7.0591e-01],\n",
       "           [-9.1566e-01, -4.3132e-01,  5.6064e-01,  ...,  3.6972e-01,\n",
       "             1.2972e+00, -4.6972e-01],\n",
       "           [-9.0246e-01, -2.8754e-01, -5.7175e-01,  ...,  2.6054e+00,\n",
       "            -1.0074e+00,  2.8461e-03],\n",
       "           ...,\n",
       "           [-2.5568e-01, -2.7135e-02, -2.2280e-01,  ...,  3.0090e+00,\n",
       "            -2.5845e+00,  6.3423e-01],\n",
       "           [-3.1322e-02,  4.6999e-01, -3.4279e-01,  ...,  3.9314e+00,\n",
       "            -1.2186e+00,  2.0639e+00],\n",
       "           [ 2.0750e-02,  1.3429e-01,  6.0266e-01,  ...,  3.2067e+00,\n",
       "            -1.2937e+00,  1.7932e+00]],\n",
       " \n",
       "          [[-3.9480e-01, -4.3998e-02,  2.4731e-01,  ...,  2.0164e+00,\n",
       "             2.0868e+00,  1.7586e-01],\n",
       "           [ 1.2638e+00,  3.4897e-01,  9.9410e-01,  ...,  1.4653e+00,\n",
       "             1.5853e+00,  2.2863e+00],\n",
       "           [ 1.6998e+00,  1.0804e+00,  8.8326e-01,  ...,  2.3166e+00,\n",
       "             2.2343e+00,  3.4734e-01],\n",
       "           ...,\n",
       "           [ 2.0847e-01, -5.3168e-01,  1.3698e-01,  ...,  1.5981e+00,\n",
       "             3.1520e+00, -7.9203e-01],\n",
       "           [ 1.8952e-01, -1.7590e-01, -1.2580e-01,  ...,  1.9152e+00,\n",
       "            -2.1496e-01, -1.1296e+00],\n",
       "           [ 5.7520e-01, -1.9040e-01,  8.7221e-03,  ...,  9.4553e-01,\n",
       "             2.6563e+00, -2.6426e-02]],\n",
       " \n",
       "          [[-1.1928e-01, -7.1454e-01, -7.4137e-01,  ..., -2.6418e+00,\n",
       "             2.0113e+00, -7.1824e-01],\n",
       "           [-2.9466e+00, -5.8726e-01, -5.3625e-01,  ..., -1.2008e+00,\n",
       "             3.0818e-01, -1.8427e+00],\n",
       "           [-2.1767e+00,  3.1212e-01,  6.2506e-01,  ..., -7.0523e-01,\n",
       "             1.2036e+00,  1.1239e+00],\n",
       "           ...,\n",
       "           [-6.9422e-01,  8.7040e-01, -4.2513e-01,  ..., -4.8293e-01,\n",
       "             1.9297e+00, -2.0145e-02],\n",
       "           [-3.7957e-01, -1.1152e-01, -7.9518e-02,  ..., -5.3612e-01,\n",
       "             1.9291e+00,  8.8576e-02],\n",
       "           [ 7.4081e-02, -3.4538e-01,  6.4517e-01,  ...,  1.1877e+00,\n",
       "             9.5666e-01,  6.0533e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.9495e+00,  8.2865e-01,  1.1709e+00,  ...,  1.9611e+00,\n",
       "            -1.4033e+00, -1.6314e+00],\n",
       "           [-2.1738e-01,  9.2786e-01,  1.0431e+00,  ...,  7.8626e-01,\n",
       "            -1.0228e+00, -1.8449e+00],\n",
       "           [-2.2119e+00, -1.6652e-01,  8.8090e-01,  ...,  1.4194e+00,\n",
       "            -6.2857e-01, -7.9871e-01],\n",
       "           ...,\n",
       "           [ 2.3064e-01, -1.4653e-01,  5.0634e-01,  ...,  4.8742e-01,\n",
       "             1.5141e-01, -9.6064e-01],\n",
       "           [-5.7459e-01,  3.2441e-01,  3.1453e-01,  ...,  1.5201e+00,\n",
       "            -2.7415e-01, -2.7334e+00],\n",
       "           [-2.1894e-01,  1.0234e-01,  5.0405e-01,  ...,  2.5531e-01,\n",
       "            -2.0395e-01, -2.5577e+00]],\n",
       " \n",
       "          [[-5.5537e-01, -1.4252e-01, -1.9609e-01,  ..., -1.2925e+00,\n",
       "            -4.5211e-01,  2.2515e+00],\n",
       "           [ 9.2576e-01, -3.9055e-01,  8.4631e-02,  ..., -3.5162e+00,\n",
       "             5.9409e-01,  2.5510e+00],\n",
       "           [ 1.8734e+00, -1.2110e-01, -1.8174e-01,  ..., -1.9586e+00,\n",
       "             1.6930e+00,  4.0980e+00],\n",
       "           ...,\n",
       "           [ 1.1147e-01,  2.3006e-01,  2.1564e-02,  ..., -5.0237e-01,\n",
       "             7.1695e-01,  7.8465e+00],\n",
       "           [ 5.9202e-01, -4.6060e-01, -2.6095e-01,  ..., -6.9319e-01,\n",
       "             3.2542e-01,  8.3101e+00],\n",
       "           [ 7.9589e-01,  1.1461e-01, -7.2435e-01,  ..., -1.9943e+00,\n",
       "             1.9725e+00,  7.7322e+00]],\n",
       " \n",
       "          [[ 6.9856e-01, -1.3109e+00,  6.2588e-01,  ...,  2.9831e-01,\n",
       "             1.0497e+00,  8.9632e-01],\n",
       "           [ 2.0740e+00, -7.9019e-01,  2.0024e-01,  ...,  4.7740e-01,\n",
       "            -5.5980e-01,  9.1576e-01],\n",
       "           [ 1.3600e+00, -4.1307e-01, -2.1123e-02,  ...,  8.9353e-01,\n",
       "             3.3876e-01,  2.0151e+00],\n",
       "           ...,\n",
       "           [ 5.2583e-01,  9.8441e-01,  3.8499e-02,  ..., -1.2707e+00,\n",
       "             2.7815e+00,  1.5634e-01],\n",
       "           [ 2.7580e-01,  1.8061e-01,  1.6667e-01,  ..., -1.1517e+00,\n",
       "             4.1400e+00,  1.7754e+00],\n",
       "           [ 7.4772e-02,  1.6546e-02, -2.9757e-01,  ..., -1.4354e+00,\n",
       "             3.1521e+00,  1.5163e+00]]]], device='cuda:0'),\n",
       " tensor([[[[-0.2025, -0.4542, -0.3780,  ..., -0.3603,  2.1365,  0.3905],\n",
       "           [-0.2861, -0.3764,  0.0154,  ..., -3.3120,  2.9814,  1.1218],\n",
       "           [-1.2983,  0.1776, -0.3855,  ..., -0.4912,  0.8607,  2.1326],\n",
       "           ...,\n",
       "           [-0.3572, -0.1638, -0.0887,  ..., -1.8269, -0.0225,  0.7420],\n",
       "           [ 0.1909,  0.2074, -0.0900,  ..., -2.6527,  0.0189,  0.0686],\n",
       "           [ 0.3452,  0.0385, -0.0959,  ..., -2.8763,  1.6558,  0.2914]],\n",
       " \n",
       "          [[ 1.5040,  0.9061, -0.7412,  ..., -0.9066,  0.8517,  1.2455],\n",
       "           [-0.1389,  0.2411,  0.1668,  ...,  0.2993,  0.0650,  0.8887],\n",
       "           [-0.7690, -0.9453,  1.7339,  ...,  0.1762, -0.3784, -0.3786],\n",
       "           ...,\n",
       "           [ 0.6448, -0.0373, -0.0567,  ..., -1.0603, -0.4743,  1.3725],\n",
       "           [ 1.3233, -0.5211,  0.5188,  ..., -0.0358, -1.5450,  1.0486],\n",
       "           [ 0.3135, -0.0965, -0.6434,  ..., -0.4759, -1.0679, -0.8648]],\n",
       " \n",
       "          [[-0.7342,  0.3708, -0.7348,  ...,  1.9289,  1.0923, -0.2921],\n",
       "           [ 0.4581,  0.5509, -0.5381,  ...,  0.3001,  1.4013, -2.9144],\n",
       "           [ 1.7395,  0.0845, -0.0217,  ...,  1.6130,  0.0515, -0.8007],\n",
       "           ...,\n",
       "           [-0.2881,  0.3196,  0.3326,  ...,  2.3653, -1.6865,  1.0813],\n",
       "           [ 0.4034,  0.5212,  0.2460,  ...,  4.1130, -1.8358,  1.9755],\n",
       "           [ 0.6811,  0.3734,  0.7629,  ...,  3.9387, -2.7372,  0.2595]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.4481, -1.2273,  1.2470,  ..., -0.1660,  1.0649,  1.4053],\n",
       "           [ 1.6483, -2.1362, -0.4002,  ...,  1.0222,  0.1568,  2.0807],\n",
       "           [ 0.4426, -0.9781,  0.0829,  ...,  1.1167, -0.5490,  2.1675],\n",
       "           ...,\n",
       "           [ 0.7571,  1.5218, -0.3366,  ...,  0.1250, -0.5276,  0.6451],\n",
       "           [ 0.9166, -0.8639,  0.0238,  ..., -0.0925,  1.1471,  0.2755],\n",
       "           [-1.2287, -1.2263,  0.2403,  ..., -0.0061, -0.2084,  0.8192]],\n",
       " \n",
       "          [[ 0.5811, -0.3129, -0.0379,  ..., -0.2390,  0.3334, -2.8438],\n",
       "           [ 0.5205, -0.2805,  0.1991,  ...,  1.1298, -0.6343, -1.6149],\n",
       "           [-1.6510,  0.1787, -0.3374,  ..., -0.5133,  0.4017, -0.7357],\n",
       "           ...,\n",
       "           [ 0.2879, -0.8117,  0.0047,  ..., -2.8387, -1.7377,  1.1338],\n",
       "           [ 0.5428, -0.8966,  0.3267,  ..., -3.3290, -0.8171,  1.2502],\n",
       "           [ 0.3637, -0.8402,  0.6607,  ..., -2.0170, -0.1833,  0.5767]],\n",
       " \n",
       "          [[ 0.9035,  0.2012, -0.5494,  ...,  0.8625,  0.9913, -0.7811],\n",
       "           [ 1.7222,  0.2436,  0.1823,  ..., -1.7535,  3.0715, -1.0051],\n",
       "           [ 1.0192, -0.0313,  0.5563,  ..., -0.9977,  3.3126, -0.3718],\n",
       "           ...,\n",
       "           [ 0.5161, -0.4906, -0.0751,  ...,  2.0892,  3.5920,  1.7933],\n",
       "           [ 0.7871,  0.4083,  0.6105,  ..., -0.1347,  2.7632,  0.5768],\n",
       "           [ 0.1077,  0.0263,  0.1083,  ..., -1.0879,  3.8764,  1.2591]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-1.5519e-02, -2.9690e-01, -5.9499e-01,  ..., -1.2428e+00,\n",
       "             2.2726e-01, -7.1190e-01],\n",
       "           [ 4.3712e-01,  2.3155e-01,  5.7452e-02,  ..., -1.2923e+00,\n",
       "             6.3819e-01,  6.4291e-02],\n",
       "           [ 1.5037e+00,  4.0437e-02,  5.0915e-02,  ..., -6.8578e-01,\n",
       "            -2.0538e-01,  2.4014e-02],\n",
       "           ...,\n",
       "           [-2.6985e-01,  5.2264e-01, -2.3669e-02,  ..., -2.3780e+00,\n",
       "            -5.7561e-01, -1.0854e-01],\n",
       "           [ 4.3871e-02,  4.9671e-01, -4.3407e-01,  ..., -2.1885e+00,\n",
       "            -1.3076e+00, -1.0979e+00],\n",
       "           [ 1.1964e+00,  8.5992e-01,  8.0716e-01,  ..., -1.4334e+00,\n",
       "             9.9559e-02, -7.7017e-01]],\n",
       " \n",
       "          [[-6.5196e-01,  3.0370e-02,  1.3109e-02,  ..., -7.4351e-01,\n",
       "             5.6600e-01, -1.5215e+00],\n",
       "           [ 2.2096e-01,  2.0324e-01,  1.0527e+00,  ...,  8.0678e-01,\n",
       "            -1.7200e+00,  1.3979e+00],\n",
       "           [ 1.2354e+00,  8.8705e-01,  9.6939e-01,  ..., -7.3935e-01,\n",
       "            -9.3896e-01, -1.8820e-01],\n",
       "           ...,\n",
       "           [-2.3265e-01, -1.8215e-01, -4.1862e-01,  ..., -3.9607e+00,\n",
       "             1.1009e+00, -1.3108e+00],\n",
       "           [ 3.1028e-01, -4.0877e-01,  5.2853e-01,  ..., -5.3083e+00,\n",
       "             4.1309e-01, -7.4892e-01],\n",
       "           [ 2.0352e-01, -2.3087e-01, -2.2978e-02,  ..., -2.8135e+00,\n",
       "            -1.0061e+00,  1.9048e-01]],\n",
       " \n",
       "          [[ 6.6414e-01,  1.6574e-01,  2.2677e-01,  ..., -8.3268e-01,\n",
       "            -5.2752e-01,  1.3442e-01],\n",
       "           [-9.8487e-01, -5.9302e-01,  1.1882e+00,  ..., -9.5420e-01,\n",
       "            -1.0414e+00,  2.0693e-01],\n",
       "           [-2.1948e+00, -2.6941e-01, -5.0068e-01,  ..., -7.3763e-01,\n",
       "            -9.9880e-01,  7.0801e-01],\n",
       "           ...,\n",
       "           [-2.8660e-01, -1.1418e+00,  1.2686e+00,  ...,  2.5743e-01,\n",
       "            -1.5511e+00,  2.5482e+00],\n",
       "           [-4.1566e-01, -9.0863e-01,  1.7141e-01,  ..., -3.8621e-01,\n",
       "             3.2677e-01,  9.5027e-01],\n",
       "           [ 7.2164e-01, -1.0650e+00,  2.1682e-01,  ...,  7.7893e-02,\n",
       "             2.7090e-01,  2.1107e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.0697e+00, -1.0105e+00, -5.0491e-01,  ..., -6.0483e-01,\n",
       "            -3.5738e-02,  1.4106e+00],\n",
       "           [ 4.6832e-01,  5.7364e-01, -1.4557e+00,  ..., -1.3995e+00,\n",
       "            -3.7676e-01,  1.9151e+00],\n",
       "           [ 1.1367e+00, -3.4210e-01,  1.9288e-01,  ...,  9.1522e-03,\n",
       "            -9.4110e-01,  2.6683e+00],\n",
       "           ...,\n",
       "           [-1.9440e-02, -1.3677e-01, -4.5278e-01,  ...,  2.7528e-01,\n",
       "            -4.5620e-01,  2.5158e-01],\n",
       "           [ 1.8148e-01,  4.2008e-01, -4.5995e-01,  ...,  7.2600e-01,\n",
       "             1.4991e-01,  5.3923e-01],\n",
       "           [ 3.9130e-01,  6.1700e-01, -3.2872e-01,  ...,  1.1597e+00,\n",
       "            -6.7502e-01,  1.2654e-01]],\n",
       " \n",
       "          [[-6.0467e-01, -7.0128e-01,  5.1527e-01,  ...,  1.6190e-01,\n",
       "             4.6607e+00,  2.0560e-02],\n",
       "           [ 1.0463e+00, -5.9286e-01,  2.5231e-01,  ..., -3.5176e-02,\n",
       "             5.1672e+00,  1.2406e+00],\n",
       "           [ 1.7344e+00,  2.9444e-01,  3.0428e-01,  ..., -1.4196e+00,\n",
       "             5.5812e+00, -6.7009e-01],\n",
       "           ...,\n",
       "           [ 8.3381e-01, -5.0182e-01,  1.2395e+00,  ..., -3.1490e+00,\n",
       "             8.8639e+00,  2.2534e-02],\n",
       "           [ 3.6287e-01,  7.0753e-02,  5.2405e-01,  ..., -2.2194e+00,\n",
       "             9.2698e+00, -1.0166e+00],\n",
       "           [-5.8007e-02,  5.7602e-02, -8.4292e-02,  ..., -5.6685e-01,\n",
       "             9.3622e+00, -4.2000e-01]],\n",
       " \n",
       "          [[ 1.2332e+00,  1.6579e-02, -2.4016e-01,  ..., -2.3941e+00,\n",
       "            -1.8215e+00, -2.1229e+00],\n",
       "           [ 8.5196e-01, -2.7543e-01,  1.7095e-01,  ...,  1.1440e-01,\n",
       "            -3.9918e+00,  1.8006e+00],\n",
       "           [-2.2980e-01,  7.3470e-01, -7.2922e-02,  ..., -2.0758e+00,\n",
       "             1.4824e-01,  9.5763e-01],\n",
       "           ...,\n",
       "           [ 1.0447e+00,  8.0551e-01, -9.4305e-01,  ..., -5.1240e-01,\n",
       "             1.2866e-01, -1.3910e+00],\n",
       "           [ 8.9419e-02,  4.6091e-01, -5.1674e-01,  ..., -2.3445e+00,\n",
       "            -3.7242e-01,  1.7373e-02],\n",
       "           [-1.7677e-01, -3.3418e-01, -2.0034e-01,  ..., -1.7295e+00,\n",
       "            -8.2828e-01,  6.9066e-01]]]], device='cuda:0'),\n",
       " tensor([[[[-2.1961, -0.7544, -0.0775,  ...,  0.8737, -0.5069, -1.0173],\n",
       "           [-2.4426, -1.0615, -0.0202,  ...,  0.5363, -2.2404, -1.5089],\n",
       "           [ 0.1413, -1.1380, -0.6894,  ...,  1.1526, -2.8933, -2.4311],\n",
       "           ...,\n",
       "           [-0.7776,  0.2353, -0.3547,  ..., -0.0783,  0.4627, -0.8050],\n",
       "           [-0.4379,  0.1379,  0.1963,  ...,  0.3157,  0.1459, -1.2402],\n",
       "           [ 0.5223,  0.2116,  0.6782,  ...,  0.1258, -1.6618, -1.5561]],\n",
       " \n",
       "          [[ 1.2875,  0.0250, -0.3129,  ...,  0.3215,  0.6509, -0.6019],\n",
       "           [ 1.5371, -0.0310, -0.5908,  ..., -2.2325, -0.4345,  1.0413],\n",
       "           [ 0.2256, -0.9654, -0.8645,  ...,  0.2220,  0.5982, -1.1755],\n",
       "           ...,\n",
       "           [ 1.2019,  0.2595,  0.2487,  ..., -0.6201, -0.6908, -0.9224],\n",
       "           [ 0.0297, -0.2410,  0.0391,  ..., -1.0870,  0.1540, -1.5939],\n",
       "           [-0.4004,  0.1750, -0.6593,  ..., -0.8069, -0.6532,  0.2810]],\n",
       " \n",
       "          [[-2.3353, -0.0192, -0.0146,  ...,  0.4061,  0.9187, -0.1928],\n",
       "           [-2.5081,  0.6365,  0.0708,  ...,  0.7522, -1.0115,  0.4938],\n",
       "           [ 0.0515, -0.0773, -0.2614,  ..., -0.4874,  0.5901, -1.3777],\n",
       "           ...,\n",
       "           [-0.6301, -1.1244,  0.0558,  ..., -0.4790,  0.9974, -0.4926],\n",
       "           [ 0.1832, -1.5383,  0.2179,  ..., -0.5246,  0.2191, -0.6223],\n",
       "           [ 0.7182,  0.4214, -0.7975,  ..., -1.1083,  1.1009,  0.2301]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.1650,  0.3806, -0.5495,  ...,  1.4987, -0.1247, -0.4602],\n",
       "           [ 0.8905, -0.4271,  0.0703,  ...,  0.2915,  0.2266, -0.4514],\n",
       "           [ 0.6693,  0.8355,  0.0251,  ...,  0.8934,  0.4846, -0.2663],\n",
       "           ...,\n",
       "           [-0.3127, -0.1333, -0.5789,  ...,  0.9960,  0.6261, -0.8853],\n",
       "           [ 0.4750, -0.4011, -0.0924,  ...,  2.4274,  0.0696,  0.3266],\n",
       "           [ 0.4649, -0.4517,  0.0069,  ...,  0.7776,  1.6047, -0.9354]],\n",
       " \n",
       "          [[ 0.3408, -0.6378, -0.4499,  ...,  0.8679,  0.3969, -0.7124],\n",
       "           [ 1.2377, -0.6812, -0.4811,  ...,  0.3222,  0.0051,  1.2443],\n",
       "           [ 0.8260, -0.3366, -1.0098,  ...,  0.8866,  1.5423,  0.0106],\n",
       "           ...,\n",
       "           [ 0.5066, -0.2358, -1.0696,  ..., -1.8736,  1.3121,  0.6074],\n",
       "           [-0.3733,  0.4552, -0.9552,  ..., -0.9554,  1.6898,  1.6600],\n",
       "           [ 0.0076,  0.1463, -0.4192,  ..., -0.5233,  0.3900,  1.4216]],\n",
       " \n",
       "          [[-0.4536,  0.8077, -0.4890,  ..., -1.2747, -2.7787,  3.2513],\n",
       "           [ 1.4618,  0.4078,  0.3439,  ...,  1.6461, -0.4236,  2.8247],\n",
       "           [ 1.8857, -0.0883,  1.2724,  ..., -1.3161, -1.0183,  1.9150],\n",
       "           ...,\n",
       "           [-0.1624, -0.3447, -0.0950,  ...,  1.2802, -1.2535, -0.7453],\n",
       "           [ 0.3917,  0.6812,  0.0889,  ...,  0.9543, -1.0101,  0.0582],\n",
       "           [ 0.0754, -0.7941, -0.0926,  ...,  1.6221,  0.4627,  0.1415]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-6.1883e-02,  1.0779e-01, -1.2537e-01,  ...,  2.9685e-01,\n",
       "            -1.0282e+00,  3.8512e+00],\n",
       "           [-6.5864e-02, -4.1788e-01, -1.4038e-01,  ..., -3.0388e-01,\n",
       "             2.3383e-01,  1.3882e+00],\n",
       "           [ 7.5948e-02, -7.8114e-01, -3.3275e-01,  ...,  1.8137e+00,\n",
       "            -1.3375e+00,  2.3983e+00],\n",
       "           ...,\n",
       "           [ 4.0771e-01,  3.5746e-01,  4.5177e-01,  ...,  5.0231e-01,\n",
       "            -8.4343e-01,  4.5532e+00],\n",
       "           [ 2.5339e-01,  3.1420e-01,  3.0607e-01,  ..., -5.9216e-01,\n",
       "            -1.1561e+00,  6.9187e+00],\n",
       "           [ 3.5395e-01,  1.8484e-01, -5.9337e-01,  ..., -1.1203e+00,\n",
       "            -7.4797e-01,  5.2820e+00]],\n",
       " \n",
       "          [[ 3.8751e-01,  9.3906e-02,  5.5782e-02,  ...,  2.6586e-01,\n",
       "            -3.2490e+00, -2.8427e-01],\n",
       "           [-1.6287e-01,  7.1013e-01, -3.5257e-01,  ..., -6.8131e-01,\n",
       "            -4.1723e+00, -5.9707e-01],\n",
       "           [-5.8007e-01,  4.5456e-01, -4.6154e-01,  ...,  3.2140e-01,\n",
       "            -4.3893e+00, -1.1957e-01],\n",
       "           ...,\n",
       "           [ 2.5473e-01,  7.4627e-02,  2.4181e-01,  ...,  1.2605e-01,\n",
       "            -5.9493e+00, -1.3215e-02],\n",
       "           [-1.1787e-01,  3.1849e-01, -2.6524e-02,  ...,  1.3061e-01,\n",
       "            -9.0646e+00,  1.2590e+00],\n",
       "           [-4.7382e-01,  1.7676e-01, -6.0219e-01,  ..., -2.1508e+00,\n",
       "            -8.0636e+00,  6.2097e-01]],\n",
       " \n",
       "          [[-2.0020e-01, -1.5462e-01,  2.2438e-01,  ...,  1.4159e+00,\n",
       "            -3.8112e-01, -1.7871e+00],\n",
       "           [-1.5603e+00, -6.5402e-01, -6.3389e-01,  ...,  1.0623e+00,\n",
       "            -2.0361e-01, -1.8421e+00],\n",
       "           [-1.3610e+00,  1.2727e-01, -2.9602e-01,  ...,  1.0554e+00,\n",
       "            -6.7036e-01, -2.8710e+00],\n",
       "           ...,\n",
       "           [ 2.1530e-01, -1.7356e-01, -1.9047e-01,  ..., -7.3301e-01,\n",
       "             7.7239e-01, -9.7043e-01],\n",
       "           [-8.0506e-01,  1.0164e-01,  3.2123e-01,  ..., -7.8233e-02,\n",
       "             1.3371e-01, -7.0361e-01],\n",
       "           [-1.1555e+00,  4.1271e-01,  2.7296e-01,  ..., -2.7628e-01,\n",
       "            -3.5400e-01, -1.0182e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.4433e-01,  1.2521e-02,  4.4503e-02,  ...,  3.4620e+00,\n",
       "            -1.2727e+00, -1.5737e+00],\n",
       "           [ 5.6876e-01, -7.0230e-01,  3.9507e-01,  ...,  2.4770e+00,\n",
       "             1.3349e+00, -1.3740e+00],\n",
       "           [ 5.1908e-01, -6.4867e-01,  2.0468e-01,  ...,  2.6242e+00,\n",
       "            -4.4720e-01, -6.4981e-01],\n",
       "           ...,\n",
       "           [ 4.0801e-01,  2.2527e-01, -8.4516e-02,  ...,  5.1830e+00,\n",
       "            -2.2057e+00,  2.5626e+00],\n",
       "           [ 3.2389e-01,  4.5204e-01, -3.1891e-01,  ...,  5.7269e+00,\n",
       "            -2.1369e+00,  1.5446e+00],\n",
       "           [ 4.0372e-01,  5.1777e-01,  7.8332e-02,  ...,  5.1119e+00,\n",
       "            -1.4194e+00,  1.4821e+00]],\n",
       " \n",
       "          [[ 1.2481e+00, -2.3554e-01, -3.1930e-01,  ..., -2.6014e+00,\n",
       "             1.2574e+00,  6.1811e-01],\n",
       "           [ 1.2642e+00, -1.6093e-01, -6.7293e-01,  ...,  1.3845e+00,\n",
       "            -3.8301e-01,  1.2202e+00],\n",
       "           [-1.5634e-01, -2.4486e-01, -7.6042e-01,  ...,  5.7183e-01,\n",
       "            -1.3279e-01, -7.0340e-01],\n",
       "           ...,\n",
       "           [ 5.3295e-01, -5.6255e-01, -1.0309e-02,  ..., -1.1952e+00,\n",
       "             1.3383e+00, -3.1893e+00],\n",
       "           [ 3.5905e-01, -9.5059e-01,  3.0282e-01,  ..., -1.3677e+00,\n",
       "             3.6601e+00, -4.5701e+00],\n",
       "           [-3.0677e-01, -1.4068e-01, -1.2454e-01,  ...,  9.4438e-01,\n",
       "             1.6051e+00, -3.5354e+00]],\n",
       " \n",
       "          [[-3.7725e-01, -4.2550e-01,  2.1582e-01,  ...,  2.6756e-01,\n",
       "             8.5265e-01, -4.2035e+00],\n",
       "           [-2.0715e+00, -3.1638e-01,  1.5822e-01,  ..., -1.3547e+00,\n",
       "             7.1544e-02, -3.6839e+00],\n",
       "           [-1.0796e+00,  1.3168e+00, -5.0682e-01,  ..., -2.5703e-01,\n",
       "            -5.8445e-03, -4.4812e+00],\n",
       "           ...,\n",
       "           [-3.4312e-01, -3.2240e-01, -5.8100e-01,  ..., -1.1966e-02,\n",
       "             4.4096e-01, -4.6209e+00],\n",
       "           [-2.9697e-01, -6.5422e-01, -7.7933e-01,  ..., -1.0907e+00,\n",
       "             1.4657e+00, -4.9800e+00],\n",
       "           [-2.9075e-01, -3.6188e-01, -7.2099e-01,  ..., -6.4580e-01,\n",
       "             2.1290e+00, -3.8813e+00]]]], device='cuda:0'),\n",
       " tensor([[[[ 1.3211e+00,  3.2273e-01,  4.2436e-02,  ..., -1.7523e-01,\n",
       "            -4.6656e-01,  1.0439e+00],\n",
       "           [ 1.5729e+00, -8.1013e-02, -3.2763e-01,  ...,  1.1320e+00,\n",
       "            -4.1897e-01,  1.4169e+00],\n",
       "           [-4.9744e-01,  1.7894e-01, -5.6089e-01,  ..., -5.0182e-01,\n",
       "            -5.7819e-01, -4.7508e-01],\n",
       "           ...,\n",
       "           [ 2.3142e-01,  4.2644e-01,  3.3596e-01,  ..., -9.0366e-02,\n",
       "             3.1779e-01,  1.0982e+00],\n",
       "           [ 3.4808e-01, -9.0341e-02, -3.1785e-01,  ..., -8.7386e-02,\n",
       "             2.3269e+00,  5.1070e-01],\n",
       "           [-2.4778e-01, -4.4763e-01, -1.1787e-01,  ..., -1.2131e-01,\n",
       "             7.8783e-01,  4.6727e-01]],\n",
       " \n",
       "          [[ 1.3455e-01, -2.6522e-01,  1.8155e-01,  ..., -1.7843e+00,\n",
       "            -4.0985e-01,  2.7776e+00],\n",
       "           [-1.3036e-03, -1.1872e-01,  3.4606e-01,  ..., -1.0336e+00,\n",
       "             5.5349e-01,  3.2030e+00],\n",
       "           [ 1.5766e-01,  1.6504e-01,  5.8742e-02,  ..., -1.7954e+00,\n",
       "            -1.1418e+00,  6.0435e-01],\n",
       "           ...,\n",
       "           [-1.5096e-01, -1.0136e-01, -2.3898e-01,  ..., -1.5454e+00,\n",
       "            -2.3779e+00, -1.6488e+00],\n",
       "           [-2.2933e-01,  3.2097e-01, -1.2415e-01,  ..., -6.1380e+00,\n",
       "            -3.9504e+00,  2.5415e-01],\n",
       "           [ 6.2369e-02, -2.5297e-01, -1.4167e-01,  ..., -3.2428e+00,\n",
       "            -3.5627e+00, -3.5609e-02]],\n",
       " \n",
       "          [[-2.4332e-01,  2.6813e-01,  1.7585e-01,  ...,  7.5532e-02,\n",
       "             5.1479e-01,  2.9261e-01],\n",
       "           [-6.9597e-01,  6.5398e-01, -7.7784e-01,  ..., -4.8302e-02,\n",
       "             9.1931e-01,  1.1881e+00],\n",
       "           [ 3.2064e-02,  4.4260e-01,  2.4532e-01,  ..., -1.2437e+00,\n",
       "            -1.3145e+00,  1.5535e+00],\n",
       "           ...,\n",
       "           [-6.8217e-02, -6.8341e-01, -7.0543e-01,  ...,  7.8638e-01,\n",
       "            -3.3541e-01,  1.3865e+00],\n",
       "           [ 4.1045e-01, -1.0981e-01, -6.5750e-01,  ..., -6.8571e-01,\n",
       "            -8.4860e-01,  1.4057e+00],\n",
       "           [ 9.2061e-01,  1.1970e-01,  3.7311e-01,  ..., -2.9895e-01,\n",
       "            -8.3557e-01,  1.4246e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0857e-01, -1.8384e-02, -2.6392e-02,  ..., -4.9262e+00,\n",
       "             5.3178e-01, -1.8566e-01],\n",
       "           [ 4.3937e-01, -4.6841e-01, -1.7124e-02,  ..., -5.4096e+00,\n",
       "             1.6697e+00, -2.8324e+00],\n",
       "           [-3.5733e-01, -2.1381e-01, -8.2562e-02,  ..., -5.7388e+00,\n",
       "             2.5025e+00,  5.5511e-01],\n",
       "           ...,\n",
       "           [ 4.9828e-01,  3.6571e-01,  3.2585e-01,  ..., -6.5749e+00,\n",
       "            -4.8399e-01,  1.3546e+00],\n",
       "           [ 2.6665e-01,  1.4214e-01, -1.3147e-01,  ..., -8.8184e+00,\n",
       "             6.6627e-01,  2.9308e+00],\n",
       "           [-6.3836e-01, -4.8442e-01, -3.0468e-01,  ..., -9.3287e+00,\n",
       "            -4.7479e-01,  6.0001e-01]],\n",
       " \n",
       "          [[-1.5577e-01,  2.2704e-01, -1.7853e-01,  ...,  5.7394e+00,\n",
       "            -1.7650e-01,  1.3354e+00],\n",
       "           [-9.9616e-01, -1.4442e-01,  5.9611e-02,  ...,  7.7972e+00,\n",
       "             4.4671e+00, -1.2138e+00],\n",
       "           [-1.1908e+00, -3.4288e-01,  2.0059e-01,  ...,  5.1926e+00,\n",
       "             2.5091e-01,  1.5999e+00],\n",
       "           ...,\n",
       "           [ 2.7959e-01, -2.5694e-01, -7.3007e-02,  ...,  8.1666e+00,\n",
       "             1.3750e-01,  8.1540e-01],\n",
       "           [-6.0433e-01, -1.9468e-01,  2.0475e-01,  ...,  8.1520e+00,\n",
       "             1.3647e+00,  4.6293e+00],\n",
       "           [-2.6822e-01, -4.1215e-01, -2.0750e-01,  ...,  1.1335e+01,\n",
       "             9.0998e-01,  3.2927e+00]],\n",
       " \n",
       "          [[-6.5545e-01, -9.3346e-01, -1.0817e-02,  ..., -1.5148e+00,\n",
       "            -1.4170e+00,  6.1171e-01],\n",
       "           [-3.4483e-01, -5.5374e-01,  3.9615e-01,  ..., -2.7469e+00,\n",
       "            -2.7168e+00, -3.4301e-01],\n",
       "           [-9.7432e-01,  1.4583e-01,  8.9381e-01,  ..., -1.4501e+00,\n",
       "            -2.3015e+00,  1.0755e-01],\n",
       "           ...,\n",
       "           [-1.9509e-01,  6.6335e-01,  2.3417e-01,  ..., -2.7101e-01,\n",
       "            -1.4623e+00,  2.2802e+00],\n",
       "           [-9.9782e-01,  5.5414e-02, -5.9935e-01,  ..., -1.1393e+00,\n",
       "            -2.9889e+00,  1.3154e+00],\n",
       "           [-3.6035e-01,  2.2240e-01, -1.3661e-01,  ..., -1.3435e+00,\n",
       "            -1.7860e+00,  3.0291e-01]]]], device='cuda:0'),\n",
       " tensor([[[[-9.5270e-01,  2.5712e-01,  1.1747e-02,  ..., -7.5231e-01,\n",
       "             1.7750e+00,  1.2670e+00],\n",
       "           [-5.9215e-01,  2.2245e-01,  1.0679e+00,  ..., -4.8968e-01,\n",
       "             1.9572e+00,  8.7103e-01],\n",
       "           [-8.2506e-01,  1.1887e+00,  6.0344e-01,  ..., -1.3957e+00,\n",
       "             1.6316e+00,  1.1209e+00],\n",
       "           ...,\n",
       "           [-2.2043e-01, -3.0050e-01,  1.0863e+00,  ...,  4.1240e-01,\n",
       "             5.6135e-01,  1.6702e+00],\n",
       "           [-8.4575e-01, -4.1547e-01, -3.6296e-01,  ...,  2.0261e-01,\n",
       "             1.3946e+00,  1.5319e+00],\n",
       "           [ 9.0315e-01,  4.8839e-01,  3.7381e-03,  ...,  2.0414e+00,\n",
       "             3.2274e-01,  1.8974e-01]],\n",
       " \n",
       "          [[ 8.0726e-03, -8.2434e-01, -1.3324e-01,  ...,  5.2162e-01,\n",
       "            -4.2369e-01,  7.2496e-01],\n",
       "           [-3.9520e-01, -4.6488e-01, -8.1724e-01,  ..., -4.3501e-02,\n",
       "            -1.5993e+00,  3.6394e-01],\n",
       "           [-1.9891e-01,  3.8417e-01, -1.3472e+00,  ...,  1.1028e+00,\n",
       "            -1.7575e+00,  1.8767e+00],\n",
       "           ...,\n",
       "           [ 1.6034e-01,  1.2633e-01, -1.0372e+00,  ...,  1.0991e+00,\n",
       "             2.0938e-01,  8.4382e-01],\n",
       "           [-5.7242e-01, -5.9385e-01, -1.1560e+00,  ...,  4.1246e-01,\n",
       "            -2.6046e-01,  1.5171e-01],\n",
       "           [ 9.5550e-02, -8.2449e-02, -4.9753e-01,  ...,  1.8849e+00,\n",
       "            -2.4128e-01,  1.1331e+00]],\n",
       " \n",
       "          [[ 3.2751e-01, -2.3722e+00, -8.5968e-01,  ..., -8.7976e-01,\n",
       "            -3.5371e+00, -6.3109e-02],\n",
       "           [ 1.6624e+00, -4.0315e-01,  5.0154e-01,  ..., -2.5562e-01,\n",
       "            -3.6260e+00, -4.0892e-01],\n",
       "           [ 4.5198e-01,  8.5837e-01,  7.0254e-01,  ...,  3.7746e-02,\n",
       "            -3.6927e+00, -3.5081e-01],\n",
       "           ...,\n",
       "           [ 4.6377e-01,  5.6828e-02,  1.7318e+00,  ..., -1.1840e-01,\n",
       "            -5.2941e+00,  4.6904e-01],\n",
       "           [ 9.0442e-02,  6.3262e-01, -9.6195e-02,  ..., -3.2342e-01,\n",
       "            -5.0399e+00,  5.9649e-01],\n",
       "           [-9.9054e-01, -1.0548e+00, -4.6934e-01,  ...,  4.5439e-01,\n",
       "            -5.8217e+00,  6.1847e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.8952e-01, -3.6227e-01, -1.5539e+00,  ..., -1.8133e+00,\n",
       "             3.4033e-01,  6.4196e-01],\n",
       "           [ 2.1264e-01, -9.3901e-01, -7.0718e-01,  ..., -9.8944e-01,\n",
       "            -3.9924e-02, -1.3583e+00],\n",
       "           [-6.9933e-01, -2.6170e-01,  2.6414e-03,  ..., -3.5620e+00,\n",
       "             6.1048e-01,  1.7958e+00],\n",
       "           ...,\n",
       "           [ 2.3322e-01,  2.4279e-01, -2.2871e-02,  ..., -2.7417e+00,\n",
       "             7.5474e-01,  3.1442e-01],\n",
       "           [-1.9326e-02, -4.8372e-01, -6.2083e-01,  ..., -1.6073e+00,\n",
       "            -7.5544e-01, -1.8910e-01],\n",
       "           [-3.6096e-01,  2.1690e-01,  6.8609e-02,  ..., -2.7360e+00,\n",
       "             2.2487e-01,  6.3264e-01]],\n",
       " \n",
       "          [[ 8.3692e-01,  4.9319e-01, -1.7410e-01,  ..., -3.1345e+00,\n",
       "            -1.7085e+00, -2.5387e+00],\n",
       "           [ 9.8198e-01,  6.0641e-02, -5.4389e-02,  ..., -4.8711e+00,\n",
       "            -6.0118e-01, -1.2999e+00],\n",
       "           [-2.1755e-01, -3.6855e-01, -2.5573e-01,  ..., -5.0477e+00,\n",
       "            -1.4809e+00, -1.3726e+00],\n",
       "           ...,\n",
       "           [ 5.3554e-01,  7.6294e-02,  2.3228e-01,  ..., -8.5079e+00,\n",
       "            -3.0598e+00, -1.5141e+00],\n",
       "           [ 6.4602e-01, -3.9172e-02, -1.1046e-01,  ..., -9.9973e+00,\n",
       "            -4.2617e+00, -3.8981e+00],\n",
       "           [ 6.3675e-02,  3.7415e-03, -7.2820e-01,  ..., -8.2800e+00,\n",
       "            -4.5984e+00, -4.1815e+00]],\n",
       " \n",
       "          [[ 2.4900e-03, -3.6264e-01, -2.5390e-01,  ...,  9.6050e-01,\n",
       "             3.5673e-01, -7.6154e-01],\n",
       "           [-2.3525e-01, -4.8761e-01, -1.0364e+00,  ...,  2.0844e+00,\n",
       "             5.8077e-01, -8.7315e-01],\n",
       "           [-2.8163e-01, -8.5460e-02, -3.4804e-01,  ...,  2.4483e+00,\n",
       "             4.5656e-01, -1.5121e+00],\n",
       "           ...,\n",
       "           [ 1.4438e-01, -3.2437e-01, -1.8425e-01,  ...,  3.9352e+00,\n",
       "             1.6093e+00, -1.2575e+00],\n",
       "           [-7.5524e-03, -4.4541e-01,  1.3561e-01,  ...,  4.2335e+00,\n",
       "             3.1367e+00, -2.0762e+00],\n",
       "           [ 1.0382e-01, -6.3795e-02, -1.4537e-01,  ...,  3.7604e+00,\n",
       "             3.4483e+00, -1.1025e+00]]]], device='cuda:0'),\n",
       " tensor([[[[-2.8253e-01, -5.7122e-01,  1.1923e-01,  ..., -8.0619e-01,\n",
       "            -7.1443e-01,  1.1793e+00],\n",
       "           [-1.6454e-01,  4.0830e-01, -1.4163e-02,  ...,  9.9607e-01,\n",
       "            -3.0273e+00, -2.5934e-01],\n",
       "           [ 2.8927e-01,  9.1717e-01,  8.3339e-01,  ...,  1.0615e+00,\n",
       "            -1.4457e+00,  1.4368e+00],\n",
       "           ...,\n",
       "           [-2.3746e-01, -1.6917e-01,  2.9696e-01,  ..., -3.5457e-01,\n",
       "             6.6206e-01,  3.2464e+00],\n",
       "           [-6.8610e-01, -5.6289e-02,  3.3700e-01,  ...,  3.2365e+00,\n",
       "             4.8898e-01,  4.3429e+00],\n",
       "           [-1.0103e-01, -2.5099e-01, -2.8881e-02,  ...,  2.8094e+00,\n",
       "             1.0289e+00,  3.7867e+00]],\n",
       " \n",
       "          [[-2.3726e+00, -3.9207e-01,  3.4076e-01,  ...,  1.1475e+00,\n",
       "            -2.2182e-01, -5.8901e-01],\n",
       "           [-2.4427e+00, -3.0302e-01, -5.1868e-01,  ..., -2.2799e+00,\n",
       "            -2.4371e-01,  4.8361e-01],\n",
       "           [ 3.4420e-01, -2.9522e-01,  1.2545e-01,  ...,  2.4767e+00,\n",
       "            -4.3063e-01, -1.7856e+00],\n",
       "           ...,\n",
       "           [-1.0009e+00,  1.5257e-01, -2.0933e-01,  ...,  1.3820e-01,\n",
       "            -1.8197e-01, -4.8498e-01],\n",
       "           [ 3.4508e-01, -3.0157e-01, -9.9413e-01,  ...,  2.5263e-01,\n",
       "             3.1682e-01,  8.4764e-01],\n",
       "           [ 1.6960e+00,  1.1197e+00, -4.1760e-01,  ...,  1.9236e-01,\n",
       "            -7.4652e-01,  1.2350e+00]],\n",
       " \n",
       "          [[ 9.8335e-01, -3.5834e-01, -4.5390e-01,  ...,  3.5380e-01,\n",
       "             6.6557e-01, -3.9322e-01],\n",
       "           [ 5.7160e-01,  3.5827e-01, -1.7052e+00,  ...,  1.5304e+00,\n",
       "             4.4798e-01, -1.0554e+00],\n",
       "           [-4.6629e-01,  8.3714e-01, -1.2498e+00,  ...,  1.9139e+00,\n",
       "            -2.6587e-01, -3.9103e-01],\n",
       "           ...,\n",
       "           [-4.6421e-01,  1.8618e-01, -1.0877e+00,  ...,  5.6201e-01,\n",
       "             8.3587e-01,  1.8327e-01],\n",
       "           [ 3.9315e-01, -9.0784e-02,  6.3752e-01,  ...,  2.3929e+00,\n",
       "             1.5590e+00,  1.0626e+00],\n",
       "           [ 6.7790e-02, -4.0312e-01, -9.4545e-01,  ...,  2.0856e+00,\n",
       "             1.1121e+00, -6.8611e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.6854e-01,  8.1203e-01, -6.9163e-01,  ..., -5.2534e-01,\n",
       "            -6.8906e-01, -2.9944e-01],\n",
       "           [ 5.4568e-01,  1.8020e-01,  1.8443e-02,  ..., -3.6599e-01,\n",
       "            -3.4856e-01, -4.3577e-01],\n",
       "           [ 3.8616e-01, -1.2659e+00,  8.5320e-01,  ..., -3.7738e-01,\n",
       "             1.4259e+00,  7.0454e-01],\n",
       "           ...,\n",
       "           [-6.9200e-01,  1.0618e-01, -4.0700e-01,  ..., -9.6596e-02,\n",
       "            -4.1277e-01,  2.7653e-02],\n",
       "           [ 6.8797e-01,  9.6908e-02,  2.8621e-01,  ...,  4.8321e-01,\n",
       "             6.6828e-01, -2.7395e-01],\n",
       "           [ 5.3454e-01,  6.2145e-01,  2.5532e-01,  ...,  4.3370e-01,\n",
       "            -3.0524e-01,  6.7932e-01]],\n",
       " \n",
       "          [[ 4.5034e-01,  5.3287e-01,  4.6839e-01,  ...,  8.9750e-01,\n",
       "             2.4018e-01, -1.5221e+00],\n",
       "           [ 3.1593e-02,  1.4114e-01,  2.3862e-01,  ...,  1.0892e+00,\n",
       "            -5.3380e-01, -2.5564e-01],\n",
       "           [ 1.2710e-01,  3.5341e-01, -4.0418e-01,  ...,  9.3142e-01,\n",
       "            -1.1422e+00, -1.1561e+00],\n",
       "           ...,\n",
       "           [-1.1079e-01, -2.9211e-01, -4.9867e-01,  ...,  1.3686e+00,\n",
       "            -1.7638e+00, -2.1671e+00],\n",
       "           [-5.4298e-01,  5.2860e-01, -4.6147e-01,  ...,  5.3217e-01,\n",
       "            -2.1058e+00, -2.4044e+00],\n",
       "           [-3.4942e-01, -6.5874e-02, -5.1050e-01,  ...,  2.1556e+00,\n",
       "            -1.1256e+00, -1.3210e+00]],\n",
       " \n",
       "          [[ 9.6001e-02,  6.6909e-02, -9.2446e-02,  ..., -1.5571e+00,\n",
       "            -1.6917e+00, -5.9333e-01],\n",
       "           [ 3.3845e-01, -3.3462e-01, -1.8718e-01,  ..., -3.0274e+00,\n",
       "            -1.0148e+00, -9.8233e-01],\n",
       "           [ 1.8602e-01,  3.3429e-01,  2.9651e-01,  ..., -2.1185e+00,\n",
       "            -2.6627e+00, -7.0373e-02],\n",
       "           ...,\n",
       "           [-2.2425e-01,  6.3662e-02, -9.8008e-02,  ..., -4.8000e-01,\n",
       "            -5.2230e+00, -1.6565e+00],\n",
       "           [-2.8630e-01,  1.0700e-01, -1.4393e-01,  ..., -1.2207e+00,\n",
       "            -5.3296e+00, -1.0358e+00],\n",
       "           [-1.2005e-01,  1.5004e-01,  4.0369e-03,  ..., -6.4159e-01,\n",
       "            -4.1762e+00, -2.6199e+00]]]], device='cuda:0'),\n",
       " tensor([[[[-3.6490e-01,  1.0681e+00, -3.1909e-02,  ..., -1.1931e+00,\n",
       "             1.4916e+00,  1.3171e+00],\n",
       "           [ 8.6349e-01,  4.4829e-01,  1.3685e-01,  ..., -1.5543e+00,\n",
       "             6.4998e-01,  4.0995e-02],\n",
       "           [ 1.3747e-01,  6.3723e-01, -5.5637e-01,  ..., -9.9019e-01,\n",
       "             1.7716e+00, -1.0260e-01],\n",
       "           ...,\n",
       "           [ 6.9024e-02,  3.7364e-01, -3.9621e-01,  ..., -3.3470e-01,\n",
       "             8.5177e-01,  1.2107e+00],\n",
       "           [ 4.4414e-01,  5.8457e-01, -3.7705e-01,  ..., -4.9140e-01,\n",
       "             1.4688e+00,  7.9709e-01],\n",
       "           [ 7.7604e-01,  3.8311e-01, -4.3508e-01,  ...,  5.2536e-01,\n",
       "             1.6668e+00,  9.9935e-01]],\n",
       " \n",
       "          [[-1.3223e+00, -8.9206e-01, -5.6738e-02,  ...,  2.9177e-01,\n",
       "             3.6995e-01, -5.9930e-01],\n",
       "           [-2.8628e+00, -6.1864e-01,  5.4095e-01,  ...,  4.3589e-01,\n",
       "            -1.2730e+00, -1.7837e+00],\n",
       "           [-1.3728e+00, -2.5378e-01,  8.5268e-01,  ..., -5.0333e-01,\n",
       "             3.1324e-01, -4.7366e-01],\n",
       "           ...,\n",
       "           [-3.5516e-01,  2.4436e-02,  1.2580e-02,  ...,  7.0474e-01,\n",
       "            -4.1640e-01,  1.9743e+00],\n",
       "           [ 1.6116e-01, -1.0045e-01, -5.1285e-03,  ...,  1.0045e+00,\n",
       "            -1.1560e+00,  2.1601e+00],\n",
       "           [-6.8584e-01,  4.2386e-02,  4.5187e-01,  ...,  1.8874e+00,\n",
       "            -1.6381e+00,  2.2301e+00]],\n",
       " \n",
       "          [[ 6.9075e-02,  2.2915e-01, -3.8514e-01,  ...,  9.2703e-01,\n",
       "            -1.3066e+00, -1.5438e+00],\n",
       "           [-7.5912e-01, -5.1363e-02,  2.8975e-01,  ...,  8.6624e-01,\n",
       "            -1.7598e+00, -1.6646e-01],\n",
       "           [-1.0304e+00,  1.8202e-01,  1.5915e-02,  ...,  9.1895e-01,\n",
       "            -1.6567e+00, -7.8182e-01],\n",
       "           ...,\n",
       "           [-1.6267e+00, -3.0091e-01, -2.8029e-03,  ...,  1.1383e+00,\n",
       "             2.8131e-01, -1.5999e+00],\n",
       "           [-1.1188e+00, -1.0329e+00, -2.5078e-01,  ...,  1.0982e+00,\n",
       "             6.9368e-01, -3.2107e-01],\n",
       "           [ 8.8225e-02, -7.9693e-01, -5.8854e-01,  ...,  1.2433e-01,\n",
       "             4.8078e-01, -2.6604e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.4634e-01,  1.7127e-01,  3.5821e-01,  ..., -3.1754e+00,\n",
       "             2.1099e-02,  1.1420e+00],\n",
       "           [-4.7937e-01, -2.8227e-01,  3.6758e-01,  ..., -4.1771e+00,\n",
       "             3.2221e-01,  1.8196e+00],\n",
       "           [-4.6957e-01, -4.3002e-01,  1.2374e-01,  ..., -2.4264e+00,\n",
       "             1.1706e-02,  3.5781e-01],\n",
       "           ...,\n",
       "           [-1.1093e-01,  5.4801e-01,  2.3021e-01,  ..., -7.2986e+00,\n",
       "            -1.5836e-01,  5.2644e-03],\n",
       "           [-9.3108e-02,  4.2636e-01, -4.0644e-01,  ..., -5.0084e+00,\n",
       "            -1.1919e-02,  1.8976e+00],\n",
       "           [-1.1171e+00,  5.4992e-01, -9.4332e-01,  ..., -6.4300e+00,\n",
       "             1.1541e-01,  2.3505e+00]],\n",
       " \n",
       "          [[ 1.8866e-01, -3.6239e-01, -1.4230e-01,  ...,  1.7872e-01,\n",
       "            -9.9409e-01, -2.5295e+00],\n",
       "           [-6.5550e-01,  3.4710e-01, -2.6889e-03,  ...,  1.9656e+00,\n",
       "             1.0872e+00, -3.8003e+00],\n",
       "           [-7.0637e-01,  7.6727e-01,  1.0235e+00,  ...,  2.0829e+00,\n",
       "             2.4952e-01, -2.7802e+00],\n",
       "           ...,\n",
       "           [-3.5374e-01,  7.3469e-02,  5.7369e-01,  ...,  2.3151e+00,\n",
       "             7.9814e-01,  5.6462e-01],\n",
       "           [-2.2612e-01,  3.0447e-01,  2.9894e-01,  ...,  1.6269e+00,\n",
       "             4.5079e-02,  1.5036e+00],\n",
       "           [ 1.4222e-01,  3.1685e-01,  3.9210e-01,  ...,  1.3992e+00,\n",
       "             3.6005e-01,  1.0297e+00]],\n",
       " \n",
       "          [[ 5.0688e-01, -4.6722e-02, -4.6204e-01,  ...,  6.1168e-01,\n",
       "            -5.6864e-01, -3.8681e-01],\n",
       "           [-9.5951e-01, -3.2312e-02,  2.5515e-01,  ...,  1.1900e+00,\n",
       "            -5.2125e-01,  3.3854e-01],\n",
       "           [-8.6127e-01,  1.1195e+00, -4.5294e-01,  ...,  4.3376e-01,\n",
       "            -1.5601e+00,  2.6491e-01],\n",
       "           ...,\n",
       "           [ 5.2183e-01, -9.9967e-01, -9.3046e-01,  ..., -1.4797e-01,\n",
       "             1.6355e-01,  7.0100e-02],\n",
       "           [ 8.7695e-01, -4.9254e-01, -8.0490e-01,  ...,  1.5615e+00,\n",
       "             1.0858e+00, -4.4297e-01],\n",
       "           [-9.2508e-01,  8.1769e-01, -3.1442e-01,  ...,  2.9049e-01,\n",
       "             2.0074e-02, -1.1617e+00]]]], device='cuda:0'),\n",
       " tensor([[[[-2.5154e-01, -2.0468e-02,  3.2412e-01,  ...,  7.1141e-02,\n",
       "             2.0242e+00,  5.7947e+00],\n",
       "           [-2.3471e-01, -1.4773e-01, -5.2471e-01,  ..., -6.7253e-01,\n",
       "             1.2005e+00,  4.3183e+00],\n",
       "           [-5.6403e-01, -1.2315e-01, -9.1523e-02,  ...,  3.9858e-01,\n",
       "             6.4538e-01,  5.5592e+00],\n",
       "           ...,\n",
       "           [ 2.1986e-01, -4.4328e-01, -1.1617e-01,  ...,  8.9542e-01,\n",
       "             3.6884e+00,  7.4592e+00],\n",
       "           [ 2.6425e-01,  2.0169e-01,  1.2719e-01,  ...,  1.6054e-01,\n",
       "             2.8812e+00,  8.4910e+00],\n",
       "           [-6.8681e-01, -1.4884e-01, -3.4190e-01,  ...,  7.5997e-01,\n",
       "             5.2968e+00,  8.0921e+00]],\n",
       " \n",
       "          [[ 7.4025e-01, -3.9078e-01, -1.3557e+00,  ...,  1.8025e-01,\n",
       "            -1.4861e+00,  6.2834e-01],\n",
       "           [-4.6859e-01, -4.8601e-01,  1.6017e-01,  ...,  5.2526e-01,\n",
       "            -1.7992e+00,  1.5644e+00],\n",
       "           [ 1.1540e-01, -1.5422e-01,  1.0534e+00,  ..., -5.3083e-02,\n",
       "            -1.5814e+00,  1.6465e-01],\n",
       "           ...,\n",
       "           [-5.5348e-02,  2.3517e-01, -5.1617e-01,  ...,  8.3143e-01,\n",
       "             5.5575e-01, -1.4033e-01],\n",
       "           [-8.0349e-01, -5.6593e-01,  5.0507e-01,  ..., -1.6477e+00,\n",
       "             1.9102e-01, -9.7891e-01],\n",
       "           [-2.1862e-01, -8.1478e-01,  1.1969e+00,  ...,  3.2105e-01,\n",
       "             5.0924e-01, -1.0782e-01]],\n",
       " \n",
       "          [[-5.7992e-01,  6.5146e-01, -6.0727e-01,  ...,  2.8661e-02,\n",
       "             1.0442e+00,  9.8015e-01],\n",
       "           [-6.4912e-01,  5.3970e-01, -1.2324e-01,  ...,  3.0030e-01,\n",
       "             1.6075e+00,  8.0646e-01],\n",
       "           [-1.6131e+00, -2.6005e-01,  1.0268e+00,  ..., -1.5819e-01,\n",
       "             2.9960e-01,  2.7434e+00],\n",
       "           ...,\n",
       "           [-1.2271e-02,  1.0242e+00,  2.8354e-01,  ...,  8.3770e-01,\n",
       "             1.7526e-01,  9.9583e-01],\n",
       "           [-1.4057e+00, -1.1840e-01,  9.4518e-01,  ..., -1.1765e-01,\n",
       "            -4.0297e-02,  1.6896e+00],\n",
       "           [-1.2352e-01, -4.1265e-01,  5.0412e-01,  ...,  4.3237e-01,\n",
       "             2.7077e-01,  1.1520e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 9.0439e-01, -1.3205e-01,  5.9105e-01,  ...,  6.9313e-01,\n",
       "            -1.8332e+00,  2.5171e-02],\n",
       "           [-3.3755e-01, -4.8766e-01, -1.3985e+00,  ...,  6.7680e-02,\n",
       "            -8.2895e-01, -2.0864e+00],\n",
       "           [-5.5853e-01,  9.3336e-02, -3.2473e-01,  ...,  1.1103e+00,\n",
       "            -5.4646e-01, -4.1680e-01],\n",
       "           ...,\n",
       "           [ 2.0085e+00,  1.0374e+00,  2.0933e-01,  ..., -1.3089e-01,\n",
       "            -5.7777e-01, -1.5611e+00],\n",
       "           [-1.3386e-01, -1.0774e+00, -2.4755e-01,  ..., -1.0247e+00,\n",
       "            -6.2346e-01, -6.5794e-01],\n",
       "           [-6.3713e-01, -1.5130e+00, -4.0169e-01,  ...,  5.4158e-01,\n",
       "            -4.2986e-01,  4.3004e-01]],\n",
       " \n",
       "          [[-3.9566e-01, -9.8025e-01, -3.0124e-01,  ..., -7.6914e-01,\n",
       "            -2.1292e-02, -5.7611e-01],\n",
       "           [-6.0272e-01, -4.4225e-01, -1.9063e-01,  ..., -3.4442e-02,\n",
       "            -1.0824e+00, -6.2483e-01],\n",
       "           [-2.8465e-02, -2.7916e-01, -9.6848e-01,  ..., -4.3913e-01,\n",
       "            -2.3838e-01,  7.5492e-01],\n",
       "           ...,\n",
       "           [-4.4182e-01,  7.9895e-01, -7.9912e-01,  ..., -1.1207e-01,\n",
       "             4.4985e-01,  4.3295e-01],\n",
       "           [-6.9121e-01,  7.4219e-01, -1.0144e+00,  ...,  1.4655e-01,\n",
       "            -8.1697e-03, -1.3774e-01],\n",
       "           [ 1.5098e-01, -1.2735e+00, -3.8757e-01,  ...,  6.5157e-02,\n",
       "            -5.5700e-01,  5.5388e-01]],\n",
       " \n",
       "          [[ 2.6254e-01,  1.0852e+00,  1.6958e-02,  ..., -4.2089e-01,\n",
       "             1.0661e-01, -6.8825e-01],\n",
       "           [-1.4574e-01,  1.4794e+00, -4.8315e-01,  ..., -1.7066e+00,\n",
       "             6.2937e-01, -7.6021e-01],\n",
       "           [ 5.2068e-01,  8.4893e-04, -6.6751e-01,  ..., -8.1126e-01,\n",
       "             8.0084e-01,  5.7215e-02],\n",
       "           ...,\n",
       "           [-6.3234e-02,  8.4543e-01, -2.7510e-01,  ...,  2.4854e-01,\n",
       "             2.1847e-01,  3.2695e-01],\n",
       "           [-6.4577e-01,  1.9546e-01,  5.4615e-01,  ..., -1.8742e-01,\n",
       "             7.0527e-01,  3.7647e-01],\n",
       "           [-4.2558e-01,  2.1897e-02,  4.0159e-01,  ...,  8.6277e-01,\n",
       "             2.6147e-01, -3.4455e-01]]]], device='cuda:0'),\n",
       " tensor([[[[-1.0008e+00,  4.2627e-01, -2.9566e-01,  ..., -1.0816e+00,\n",
       "             3.1400e-01, -1.3649e+00],\n",
       "           [ 2.7783e-01, -9.8035e-02, -1.5523e-01,  ...,  2.2399e-01,\n",
       "            -3.9337e-01, -6.0521e-01],\n",
       "           [ 5.6300e-01, -1.1256e+00,  3.2888e-01,  ..., -6.9337e-01,\n",
       "            -5.4106e-01, -8.9534e-01],\n",
       "           ...,\n",
       "           [-4.5854e-01,  1.7034e-01,  2.6355e-01,  ..., -6.9074e-01,\n",
       "            -1.0675e+00,  1.2290e+00],\n",
       "           [-8.5901e-02,  2.1496e-01,  2.0737e-01,  ..., -1.0086e+00,\n",
       "            -2.7965e+00, -8.6363e-01],\n",
       "           [ 9.7302e-01,  2.3801e-01,  2.2229e-01,  ..., -3.7616e-01,\n",
       "            -1.2340e+00, -3.0022e-01]],\n",
       " \n",
       "          [[-2.6517e-01,  2.7070e-01, -4.9870e-02,  ..., -1.1863e-01,\n",
       "            -5.0577e-01,  1.1728e+00],\n",
       "           [-5.7132e-01,  7.9026e-01,  1.0004e-01,  ...,  1.1441e-01,\n",
       "            -1.2749e+00,  6.1432e-01],\n",
       "           [ 2.3655e-01,  9.2177e-01, -1.5307e+00,  ...,  1.7574e-01,\n",
       "             5.1764e-01,  3.9830e-01],\n",
       "           ...,\n",
       "           [ 5.6775e-01,  8.9623e-01, -6.1273e-01,  ...,  6.1189e-01,\n",
       "            -7.3874e-01,  2.7407e+00],\n",
       "           [ 3.1266e-01,  1.3118e-01,  2.7726e-01,  ...,  6.6921e-01,\n",
       "            -1.2256e+00,  1.0343e+00],\n",
       "           [ 4.3298e-01,  9.6152e-01,  1.9390e-01,  ...,  6.9959e-01,\n",
       "            -1.0452e+00,  2.0254e+00]],\n",
       " \n",
       "          [[-1.6149e-01, -3.1666e-02,  3.0393e-01,  ..., -8.9907e-01,\n",
       "             1.3899e+00, -8.0822e-01],\n",
       "           [ 4.9656e-02,  2.7611e-01,  3.9222e-02,  ..., -1.3316e+00,\n",
       "            -4.8403e-01, -4.2985e-01],\n",
       "           [ 5.3764e-03, -3.3556e-01, -3.5390e-01,  ..., -4.0532e-01,\n",
       "             1.4158e+00, -7.4266e-01],\n",
       "           ...,\n",
       "           [ 5.9538e-01, -9.1235e-01,  7.0696e-01,  ..., -1.5964e+00,\n",
       "            -1.1054e+00,  2.3090e+00],\n",
       "           [ 2.8659e-01, -1.8789e-01, -2.3212e-01,  ..., -1.3353e+00,\n",
       "            -3.8361e-02, -3.8268e-02],\n",
       "           [ 1.1757e-01, -5.3868e-01,  6.3066e-01,  ..., -1.7918e+00,\n",
       "            -1.2299e+00,  4.5029e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.9798e-01,  1.0140e-01,  4.4276e-01,  ..., -9.3657e-01,\n",
       "             5.9420e-01,  2.1475e+00],\n",
       "           [ 4.4358e-01, -4.2246e-01,  9.6724e-02,  ..., -6.7548e-01,\n",
       "             3.0342e-01,  2.5597e+00],\n",
       "           [-2.1554e-01,  2.7052e-01,  7.6122e-01,  ...,  2.4550e-01,\n",
       "             7.8734e-01,  2.7207e+00],\n",
       "           ...,\n",
       "           [ 2.0509e-02,  2.3203e-01,  2.3804e-01,  ..., -7.1061e-01,\n",
       "            -1.3707e+00,  1.0336e+00],\n",
       "           [ 1.2060e+00, -8.3988e-01, -5.9785e-01,  ..., -2.0456e-01,\n",
       "            -3.3533e-01,  4.3941e-01],\n",
       "           [ 1.6821e-01, -1.1894e+00,  5.6730e-01,  ...,  3.5217e-01,\n",
       "            -7.8512e-01,  1.8681e+00]],\n",
       " \n",
       "          [[-8.2971e-01, -9.5040e-01,  1.0978e+00,  ..., -8.5535e+00,\n",
       "            -1.2186e+00, -1.2123e+00],\n",
       "           [ 8.9275e-01, -5.5381e-01, -1.7504e-01,  ..., -9.0336e+00,\n",
       "            -1.9945e+00,  1.2048e+00],\n",
       "           [ 3.5499e+00, -2.3658e+00, -1.0341e+00,  ..., -9.9835e+00,\n",
       "            -3.1701e+00, -1.7584e+00],\n",
       "           ...,\n",
       "           [-3.8878e-01, -3.6076e-01, -3.2049e-02,  ..., -1.2685e+01,\n",
       "            -9.2129e-01,  2.3822e+00],\n",
       "           [ 1.4087e+00,  8.4854e-01,  5.6010e-02,  ..., -1.1566e+01,\n",
       "             3.1171e+00,  2.1737e+00],\n",
       "           [ 1.0425e+00, -5.0976e-02,  5.1264e-01,  ..., -1.3567e+01,\n",
       "             1.3785e+00,  3.2067e+00]],\n",
       " \n",
       "          [[ 4.7653e-01,  4.0788e-02,  3.3054e-01,  ...,  9.5105e+00,\n",
       "            -6.4236e-01,  8.6361e-02],\n",
       "           [-4.6066e-01,  6.0183e-01, -1.1406e+00,  ...,  1.0385e+01,\n",
       "            -1.9476e-01, -1.8970e+00],\n",
       "           [ 6.7554e-01, -9.0630e-02, -7.4299e-01,  ...,  1.0406e+01,\n",
       "             6.4132e-02, -1.2893e+00],\n",
       "           ...,\n",
       "           [-8.8024e-02,  3.3365e-01, -1.0865e-01,  ...,  1.2198e+01,\n",
       "            -9.9273e-01, -2.4777e+00],\n",
       "           [-1.1408e-01,  4.1520e-01,  1.8088e-02,  ...,  1.2593e+01,\n",
       "             3.1063e-01, -3.0827e+00],\n",
       "           [-1.5073e-01,  1.0827e+00, -1.5925e-01,  ...,  1.3212e+01,\n",
       "            -5.6876e-01, -3.5731e+00]]]], device='cuda:0'),\n",
       " tensor([[[[-1.8441e-01,  1.0304e+00,  2.1913e-01,  ...,  4.3538e-01,\n",
       "            -1.1531e+00, -1.1923e-01],\n",
       "           [-1.8219e+00,  1.2997e+00, -3.7610e-01,  ...,  8.5536e-01,\n",
       "            -9.3091e-03,  1.4919e+00],\n",
       "           [-1.3869e+00, -7.6263e-01,  6.2999e-01,  ...,  7.7466e-01,\n",
       "            -9.0531e-01,  5.4026e-01],\n",
       "           ...,\n",
       "           [-1.3404e+00, -5.9663e-01, -3.1000e-01,  ..., -1.7509e-01,\n",
       "             3.3708e-01, -3.7878e-01],\n",
       "           [-3.2030e-01,  1.9467e-01,  6.7090e-01,  ..., -2.2974e-01,\n",
       "            -3.9959e-01, -4.5751e-02],\n",
       "           [-1.1179e+00,  3.7269e-02,  8.5429e-01,  ..., -7.1303e-01,\n",
       "            -2.0581e-01, -4.5302e-01]],\n",
       " \n",
       "          [[ 2.3987e-01, -7.3235e-01,  8.8699e-01,  ...,  1.2086e+00,\n",
       "            -7.7904e-01,  6.4160e-01],\n",
       "           [ 1.2714e+00, -3.1731e-01,  7.7396e-02,  ...,  1.3708e+00,\n",
       "            -9.1465e-01,  4.0533e-01],\n",
       "           [ 6.3707e-01, -6.8179e-01, -3.0703e-01,  ...,  1.0505e+00,\n",
       "            -2.2810e-01, -6.2040e-01],\n",
       "           ...,\n",
       "           [ 5.0458e-01,  5.5737e-01,  8.1552e-01,  ...,  4.1603e-01,\n",
       "            -1.0374e+00,  3.3345e-02],\n",
       "           [ 1.4457e+00,  1.1383e-01,  9.1450e-01,  ...,  6.5290e-01,\n",
       "            -5.9686e-01,  8.4977e-01],\n",
       "           [ 4.8313e-01, -3.9061e-01,  3.2194e-01,  ...,  7.4369e-01,\n",
       "            -9.6936e-01,  4.5604e-01]],\n",
       " \n",
       "          [[-1.2607e+00,  1.0596e+00,  5.3425e-01,  ..., -5.0941e-01,\n",
       "             2.0082e+00,  1.4986e+00],\n",
       "           [-1.9747e+00, -7.6475e-01,  5.3407e-01,  ..., -1.8801e+00,\n",
       "             1.0963e+00,  1.7910e+00],\n",
       "           [-7.7323e-01, -1.7199e+00,  3.8782e-01,  ..., -7.5081e-01,\n",
       "             1.5716e+00,  2.5418e+00],\n",
       "           ...,\n",
       "           [-6.3697e-01,  3.4108e-01,  4.2869e-01,  ..., -8.3628e-01,\n",
       "             1.2358e+00,  2.0007e+00],\n",
       "           [-4.9003e-01,  9.4120e-01,  9.2941e-01,  ..., -2.7942e-01,\n",
       "             1.2435e+00,  3.9511e+00],\n",
       "           [ 1.0021e-01,  5.3122e-01, -1.5112e-01,  ..., -1.2785e-01,\n",
       "             3.7136e-01,  2.7093e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.2870e-02, -5.5419e-01,  2.2492e-01,  ..., -1.2883e+00,\n",
       "            -1.5525e-01, -4.6594e-01],\n",
       "           [-1.9414e+00, -2.1281e-03,  1.3663e-02,  ..., -8.9280e-01,\n",
       "             2.4914e+00,  1.0511e-02],\n",
       "           [-1.5215e+00,  7.3010e-02,  3.6538e-01,  ..., -1.7431e+00,\n",
       "            -2.4343e-01, -2.2062e-01],\n",
       "           ...,\n",
       "           [-3.2310e-01,  9.0784e-02, -3.3929e-01,  ..., -1.8229e-01,\n",
       "            -1.5701e+00,  1.2751e+00],\n",
       "           [-6.2009e-01, -4.6796e-01, -2.0955e-01,  ...,  6.2425e-01,\n",
       "            -1.6664e+00, -3.8941e-01],\n",
       "           [-2.9746e-01, -2.0337e-01,  9.5198e-01,  ..., -5.6522e-01,\n",
       "            -2.0085e+00,  1.1081e+00]],\n",
       " \n",
       "          [[ 8.8080e-01, -3.9484e-02,  5.1630e-01,  ...,  3.5697e-01,\n",
       "            -7.9122e-01,  3.5909e-01],\n",
       "           [ 9.9390e-01,  5.2038e-01, -3.0385e-01,  ...,  1.1944e+00,\n",
       "            -7.6759e-01,  5.6558e-02],\n",
       "           [ 1.3989e-02, -9.1377e-01, -7.5414e-01,  ...,  4.3139e-02,\n",
       "            -1.1053e+00, -4.4127e-01],\n",
       "           ...,\n",
       "           [ 1.0774e+00, -4.3227e-01,  5.6074e-02,  ...,  5.8769e-01,\n",
       "            -1.8433e+00, -3.9529e-01],\n",
       "           [ 5.2350e-01, -2.5620e-01, -3.8976e-01,  ...,  2.6045e-01,\n",
       "            -2.5463e+00,  6.4770e-01],\n",
       "           [-4.9800e-01,  4.9802e-01, -6.2553e-01,  ...,  4.9048e-01,\n",
       "            -1.8504e+00, -1.1994e+00]],\n",
       " \n",
       "          [[ 1.4774e+00,  4.3475e-01, -4.7833e-01,  ..., -7.3557e-01,\n",
       "             3.4270e-01,  6.9590e+00],\n",
       "           [ 1.2174e+00, -7.0795e-02,  5.3280e-01,  ...,  7.1170e-01,\n",
       "            -1.2372e+00,  7.3472e+00],\n",
       "           [-1.1290e-01, -1.2354e-01,  3.3673e-01,  ...,  7.7264e-01,\n",
       "            -8.0022e-01,  7.7302e+00],\n",
       "           ...,\n",
       "           [ 1.6529e+00, -6.4526e-01,  7.4202e-01,  ..., -1.9346e+00,\n",
       "            -6.2173e-01,  9.1958e+00],\n",
       "           [-1.0830e+00,  5.2724e-01,  3.5915e-01,  ..., -4.9352e-01,\n",
       "            -5.3994e-01,  8.5232e+00],\n",
       "           [-1.3765e+00,  2.8685e-01,  5.2104e-01,  ..., -5.4880e-01,\n",
       "            -1.1682e+00,  9.1006e+00]]]], device='cuda:0'),\n",
       " tensor([[[[ 0.2946,  0.5839,  0.4118,  ..., -0.1611, -0.6337, -0.5395],\n",
       "           [ 0.7426,  0.0380, -0.0250,  ...,  0.2492, -1.5381,  1.0129],\n",
       "           [ 0.0676, -0.4574,  0.4041,  ...,  0.5216, -0.4648, -0.0900],\n",
       "           ...,\n",
       "           [ 0.3244,  0.1923,  0.0348,  ...,  0.1868, -1.6031, -0.3405],\n",
       "           [-0.0363,  0.0463,  0.1567,  ..., -0.0356, -0.7025,  1.3851],\n",
       "           [ 0.0723, -0.3072,  0.2530,  ...,  0.3433, -0.9986,  0.0414]],\n",
       " \n",
       "          [[ 1.3323,  0.7184,  0.6160,  ...,  1.1891, -0.9197, -0.2744],\n",
       "           [ 0.4896, -0.0330, -0.0586,  ...,  1.6043, -0.6922,  1.9195],\n",
       "           [ 0.6661,  0.0521,  0.8612,  ...,  0.8543, -1.1138,  0.6375],\n",
       "           ...,\n",
       "           [ 0.8472,  0.5310,  0.7701,  ..., -0.8263,  0.3926,  0.3440],\n",
       "           [ 0.4941,  0.3084,  0.3190,  ..., -1.0065, -0.1768, -0.0921],\n",
       "           [-0.0495, -0.9235, -0.4885,  ..., -1.1069, -0.9533, -0.5298]],\n",
       " \n",
       "          [[-0.3401,  0.3083, -0.0312,  ..., -0.4831, -1.2951, -0.3310],\n",
       "           [ 0.3646, -0.4073, -0.0576,  ..., -2.0255, -0.0966,  1.0121],\n",
       "           [ 0.1791,  0.8666, -0.7048,  ...,  0.7407,  0.4271,  0.6255],\n",
       "           ...,\n",
       "           [ 0.0201, -0.1122, -0.1528,  ..., -1.4906, -0.9324,  0.3052],\n",
       "           [ 1.0596, -0.4603, -1.3103,  ..., -1.0898, -0.4583,  0.5296],\n",
       "           [-0.1634, -0.4251, -0.1687,  ..., -1.9295, -0.2651, -1.1274]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.5440, -0.0419,  1.1813,  ...,  4.0469, -0.9841, -0.1662],\n",
       "           [-0.1251, -0.4455,  0.1590,  ...,  4.6711, -0.9019, -0.2062],\n",
       "           [-1.1379, -0.4088, -0.2510,  ...,  4.9145, -1.2712, -0.5871],\n",
       "           ...,\n",
       "           [ 0.2089,  0.8959, -0.1405,  ...,  5.1829, -0.6566, -1.2517],\n",
       "           [-0.1378,  0.3803, -0.0168,  ...,  5.4906, -2.2857, -1.6679],\n",
       "           [ 0.6179, -0.4740, -1.2667,  ...,  6.3121, -0.7136, -1.5065]],\n",
       " \n",
       "          [[ 0.0637,  0.4920,  0.2994,  ...,  1.2168, -0.7858,  1.0762],\n",
       "           [ 0.2660,  0.0386, -0.2133,  ...,  0.6564,  0.3614,  0.0383],\n",
       "           [ 2.9067,  1.1290,  1.1783,  ...,  0.5453, -0.2176,  0.8726],\n",
       "           ...,\n",
       "           [-0.7647, -1.0280, -0.1580,  ...,  0.5761,  1.4220,  0.5037],\n",
       "           [-0.6104, -1.9287,  0.4158,  ...,  0.6438, -0.3558,  0.2977],\n",
       "           [ 1.5890, -0.3090,  0.0184,  ...,  0.4299,  0.8079,  0.7208]],\n",
       " \n",
       "          [[ 1.1326, -0.4044,  1.3006,  ..., -0.7503,  0.3927, -1.3304],\n",
       "           [ 1.4772, -1.1476,  1.0378,  ..., -1.1897,  0.6487,  1.2916],\n",
       "           [ 0.2396, -2.3649,  0.6518,  ...,  0.0553, -0.7767,  1.6772],\n",
       "           ...,\n",
       "           [ 0.4708,  1.6461,  0.4510,  ..., -0.4346,  0.7044,  0.2574],\n",
       "           [ 0.3208,  1.1297,  0.2060,  ...,  0.7499,  1.7556,  3.0531],\n",
       "           [-0.7535, -0.7938, -0.5906,  ...,  0.6448,  1.3356,  0.9259]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-1.0603e+00, -7.9384e-01, -1.2908e+00,  ..., -1.0877e+00,\n",
       "             1.5155e+00,  8.7652e-01],\n",
       "           [-1.0789e+00, -1.1469e-02, -6.0938e-01,  ...,  5.6551e-02,\n",
       "             7.4531e-01, -1.8574e-01],\n",
       "           [-7.2269e-01,  8.8000e-01, -1.0981e+00,  ..., -4.9122e-01,\n",
       "            -4.2910e-01,  5.5165e-01],\n",
       "           ...,\n",
       "           [-5.6941e-01,  8.9862e-02, -5.9041e-01,  ..., -2.0512e+00,\n",
       "            -1.2976e-01,  7.6479e-01],\n",
       "           [-3.7522e-01,  2.1646e-01, -1.3631e+00,  ..., -3.6839e+00,\n",
       "             2.6073e-02,  4.5022e-01],\n",
       "           [-7.0355e-01, -7.1701e-01, -3.7279e-01,  ..., -8.3968e-01,\n",
       "             1.2938e-01, -2.5410e-01]],\n",
       " \n",
       "          [[ 3.3260e-01, -7.6650e-01, -4.4876e-01,  ..., -1.7528e+00,\n",
       "             1.3925e-01, -2.1217e+00],\n",
       "           [ 9.9095e-01, -2.0317e+00, -3.7032e-02,  ..., -2.4527e+00,\n",
       "             1.6627e-01, -9.5490e-01],\n",
       "           [ 1.1503e+00,  2.3699e-01,  2.7492e-01,  ..., -1.7289e+00,\n",
       "             1.7334e-01, -2.2258e+00],\n",
       "           ...,\n",
       "           [ 2.9517e-01,  1.1980e+00, -7.0108e-01,  ..., -2.4875e+00,\n",
       "            -7.2877e-01, -1.6376e+00],\n",
       "           [ 8.4005e-01,  3.0393e-01,  6.6413e-02,  ..., -1.6242e+00,\n",
       "             1.4481e-01, -2.2087e+00],\n",
       "           [ 2.2788e-01, -1.0674e+00, -3.4233e-01,  ..., -2.1109e+00,\n",
       "            -2.6168e-01, -2.5884e+00]],\n",
       " \n",
       "          [[ 3.9966e-01, -5.1324e-01,  5.0407e-01,  ..., -1.0023e+00,\n",
       "            -2.2988e+00,  2.0009e+00],\n",
       "           [ 1.6745e-01, -3.2367e-01,  3.5315e-01,  ..., -6.1447e-01,\n",
       "            -2.9619e+00,  2.7354e+00],\n",
       "           [ 1.4400e-01,  1.9958e-01, -2.8990e-01,  ...,  1.8054e-02,\n",
       "            -3.4841e+00,  1.9034e+00],\n",
       "           ...,\n",
       "           [ 5.4356e-02,  9.2755e-02, -8.8539e-02,  ..., -1.5770e+00,\n",
       "            -4.7334e+00,  4.4941e+00],\n",
       "           [ 1.9593e-01,  6.9977e-02, -7.1191e-02,  ..., -1.9837e+00,\n",
       "            -6.1070e+00,  3.4014e+00],\n",
       "           [-2.0939e-01, -4.6118e-01, -9.7194e-01,  ..., -4.1033e-01,\n",
       "            -5.8611e+00,  4.0293e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.9992e+00,  4.7259e-01,  2.8605e-01,  ..., -3.9179e-01,\n",
       "            -5.5693e-01,  2.2192e-01],\n",
       "           [ 5.3387e-01,  7.5093e-01,  5.5290e-01,  ...,  9.0704e-01,\n",
       "            -1.1900e+00,  7.6494e-01],\n",
       "           [-5.5169e-01, -3.9279e-01,  4.3057e-01,  ..., -3.9287e-01,\n",
       "             1.5114e+00,  1.4782e+00],\n",
       "           ...,\n",
       "           [ 1.0812e+00,  6.6149e-01,  2.4711e-01,  ..., -2.7759e-01,\n",
       "             8.5941e-01,  5.6943e-01],\n",
       "           [-7.0547e-01, -4.3701e-01,  2.2177e-01,  ..., -1.0232e+00,\n",
       "             1.6489e+00,  3.7241e-01],\n",
       "           [-1.7888e+00, -3.3255e-01, -6.4580e-02,  ..., -1.1156e+00,\n",
       "             3.8418e-01,  7.6205e-01]],\n",
       " \n",
       "          [[ 7.3628e-01, -2.5142e-01, -2.7251e-01,  ..., -1.9007e+00,\n",
       "             1.7524e+00, -1.2193e-01],\n",
       "           [ 6.7555e-01,  4.5497e-01,  2.3752e-03,  ..., -2.8924e+00,\n",
       "             1.2992e+00, -6.8527e-01],\n",
       "           [-3.6208e-01,  1.0380e+00,  4.8344e-01,  ..., -2.2403e+00,\n",
       "             8.6746e-01, -2.3568e-01],\n",
       "           ...,\n",
       "           [ 6.4162e-03, -5.1141e-01, -8.3324e-02,  ..., -6.0297e+00,\n",
       "             1.0869e+00, -1.1240e+00],\n",
       "           [-1.8911e-01, -4.6101e-01, -4.1571e-01,  ..., -4.0480e+00,\n",
       "            -2.0976e-01, -1.2707e+00],\n",
       "           [-5.0053e-01, -4.5711e-01,  1.8935e-01,  ..., -5.2077e+00,\n",
       "             2.4435e-01, -1.5184e+00]],\n",
       " \n",
       "          [[ 6.4483e-02, -4.3623e-01,  7.9649e-02,  ..., -6.3507e-01,\n",
       "            -3.7977e-01,  3.1549e+00],\n",
       "           [-5.4349e-02,  5.8220e-02, -2.6391e-01,  ..., -9.8086e-01,\n",
       "             8.6287e-01,  5.3794e+00],\n",
       "           [ 4.9588e-02, -3.1582e-01, -7.9562e-01,  ..., -1.2810e+00,\n",
       "             1.2300e+00,  5.0152e+00],\n",
       "           ...,\n",
       "           [ 4.7998e-01, -3.9495e-01,  2.5104e-01,  ..., -1.6631e+00,\n",
       "             4.9561e-01,  5.6530e+00],\n",
       "           [ 2.6320e-01, -2.6490e-01, -6.6769e-01,  ..., -3.2448e-01,\n",
       "             5.2045e-01,  6.0787e+00],\n",
       "           [-2.7188e-02,  4.0691e-01,  3.8962e-01,  ..., -6.4287e-01,\n",
       "             1.0350e+00,  5.7160e+00]]]], device='cuda:0'),\n",
       " tensor([[[[-6.7848e-01,  5.2643e-01, -2.4926e-01,  ...,  5.1437e-01,\n",
       "             3.2559e-01, -3.3862e+00],\n",
       "           [-1.4025e+00,  1.2862e-01, -3.3054e-01,  ..., -1.1763e+00,\n",
       "             1.0228e+00, -2.9176e-01],\n",
       "           [-6.2037e-01,  6.8321e-01,  3.6700e-01,  ...,  3.3115e-01,\n",
       "            -7.0678e-02, -2.0963e+00],\n",
       "           ...,\n",
       "           [-5.0150e-01, -1.1755e-02, -5.0791e-02,  ..., -8.3839e-01,\n",
       "             1.6571e+00, -7.3045e-01],\n",
       "           [-3.8442e-01,  4.8792e-01, -3.9512e-02,  ..., -1.2284e+00,\n",
       "             1.0142e+00,  5.6474e-01],\n",
       "           [ 2.7138e-01, -9.8893e-02,  1.2402e-01,  ..., -3.3431e-02,\n",
       "             2.2258e-01, -8.8994e-01]],\n",
       " \n",
       "          [[ 1.0149e+00, -9.4301e-01,  2.3695e-01,  ..., -1.2029e+00,\n",
       "            -8.5888e-01, -2.6336e-01],\n",
       "           [ 1.3545e+00, -3.0513e-01,  8.0808e-02,  ..., -9.1241e-01,\n",
       "            -1.9112e+00,  1.8375e+00],\n",
       "           [ 5.1475e-02,  2.6157e-01,  3.7246e-01,  ..., -1.0032e+00,\n",
       "            -7.8113e-01,  1.9985e+00],\n",
       "           ...,\n",
       "           [ 7.6673e-01,  4.2906e-01,  2.2292e-01,  ..., -1.9694e+00,\n",
       "            -7.4751e-01,  1.7263e+00],\n",
       "           [ 3.2096e-03,  4.6845e-01,  1.1575e+00,  ..., -5.4104e-01,\n",
       "            -2.1927e-01,  2.1996e+00],\n",
       "           [-1.7862e-01, -1.0515e+00,  3.8649e-01,  ..., -1.9501e+00,\n",
       "            -1.1042e+00,  2.6663e-01]],\n",
       " \n",
       "          [[ 1.6558e+00, -6.6628e-01,  1.7206e+00,  ...,  7.5579e-01,\n",
       "            -4.8094e-01,  3.6120e-01],\n",
       "           [ 2.7566e+00,  6.0564e-01,  1.8269e-01,  ..., -1.1935e+00,\n",
       "            -2.2601e+00, -2.4932e-01],\n",
       "           [-4.8440e-01,  8.3185e-01, -2.2785e-01,  ...,  1.0627e-01,\n",
       "            -1.8916e+00, -1.2304e+00],\n",
       "           ...,\n",
       "           [ 1.4184e+00,  3.2418e-01,  4.2805e-01,  ..., -8.8115e-01,\n",
       "            -1.8659e+00, -1.4743e+00],\n",
       "           [ 1.0140e+00,  1.1676e+00, -6.0427e-01,  ..., -3.4438e-01,\n",
       "            -1.4446e+00, -7.8422e-01],\n",
       "           [-4.2707e-01,  1.1924e+00,  7.2735e-02,  ..., -5.8351e-01,\n",
       "            -2.4842e+00, -9.4583e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.0479e-01, -1.4386e-01, -1.7889e-01,  ...,  9.0707e-01,\n",
       "             1.5307e+00, -1.9397e-01],\n",
       "           [-7.3708e-01, -3.3667e-01,  1.0279e+00,  ...,  7.8296e-01,\n",
       "             2.7466e+00,  4.9460e-01],\n",
       "           [-2.5437e-01, -1.6139e+00,  6.7853e-01,  ...,  1.4135e+00,\n",
       "             2.7964e+00, -4.9174e-01],\n",
       "           ...,\n",
       "           [ 1.7642e-01,  1.3838e+00,  5.0818e-01,  ...,  3.7788e-01,\n",
       "             7.9528e-01, -9.8706e-01],\n",
       "           [-9.7690e-01,  6.0991e-01,  4.1227e-01,  ..., -5.1329e-01,\n",
       "            -1.5779e-01,  5.3909e-01],\n",
       "           [-7.1832e-01,  2.1071e-01, -8.3067e-02,  ..., -1.4066e-01,\n",
       "             8.1726e-01,  2.2969e-01]],\n",
       " \n",
       "          [[ 1.1633e+00,  6.8413e-01, -1.0143e+00,  ...,  1.0466e+00,\n",
       "            -8.3937e-01, -8.9319e-02],\n",
       "           [ 1.1938e+00, -2.5251e-01, -7.8440e-01,  ..., -3.1268e-01,\n",
       "            -7.1159e-01, -3.3488e-01],\n",
       "           [ 1.6384e-01, -3.8742e-01,  5.4459e-02,  ..., -4.1467e-01,\n",
       "            -1.4038e+00, -4.8669e-01],\n",
       "           ...,\n",
       "           [ 8.3102e-01,  1.0422e+00,  2.0234e-01,  ...,  3.5604e-01,\n",
       "            -3.0512e-02, -4.4745e-01],\n",
       "           [-9.1554e-01,  7.9264e-01,  2.0657e-01,  ...,  1.6526e-01,\n",
       "            -1.0280e+00,  7.4771e-01],\n",
       "           [-2.3136e-01,  5.8082e-01,  8.0559e-01,  ..., -3.2194e-01,\n",
       "            -6.0865e-01, -5.2158e-01]],\n",
       " \n",
       "          [[-8.3200e-02, -1.7205e+00,  3.3049e-02,  ..., -5.8629e-01,\n",
       "             6.1703e-01,  9.0325e-01],\n",
       "           [ 1.3307e+00, -2.1495e+00, -8.8894e-01,  ..., -6.3446e-01,\n",
       "             1.3083e-01,  1.9441e+00],\n",
       "           [ 1.5709e+00, -5.9928e-01, -1.5989e+00,  ..., -3.6844e-01,\n",
       "             2.3018e-01, -7.9894e-01],\n",
       "           ...,\n",
       "           [ 1.6259e-01,  6.8637e-01, -7.4394e-01,  ..., -8.8264e-01,\n",
       "            -6.5763e-01,  5.3250e-01],\n",
       "           [ 1.7227e+00, -1.1294e+00, -5.6948e-01,  ..., -1.0554e+00,\n",
       "            -1.1312e+00, -1.3656e+00],\n",
       "           [ 1.5464e+00, -4.9963e-01, -6.9754e-01,  ..., -1.2923e+00,\n",
       "            -4.1493e-01, -9.5625e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 1.8875, -0.1167,  0.0813,  ...,  0.9613,  0.1123, -1.0091],\n",
       "           [ 2.8680, -0.3170, -0.9797,  ...,  1.8280, -0.4089, -1.0989],\n",
       "           [ 0.5660, -0.4868, -0.4981,  ...,  1.5514, -0.5650, -1.6988],\n",
       "           ...,\n",
       "           [ 0.8131, -0.5918, -0.1887,  ...,  3.1781,  0.7124, -0.1829],\n",
       "           [ 0.2941, -0.6995, -0.1779,  ...,  2.6301, -0.2207, -0.1630],\n",
       "           [ 0.2860, -0.6325, -0.2466,  ...,  2.5683,  0.3051,  0.2446]],\n",
       " \n",
       "          [[ 0.9261,  1.3754,  0.5311,  ..., -0.2433, -0.9000, -0.5798],\n",
       "           [ 0.0772,  1.2708,  0.9877,  ...,  1.0501, -0.4317, -0.6296],\n",
       "           [-0.4045, -0.4300, -0.1168,  ...,  0.4860, -1.4468, -0.8922],\n",
       "           ...,\n",
       "           [ 0.5307, -0.3604,  0.5790,  ..., -2.3126,  1.0383,  0.6808],\n",
       "           [-1.0745,  0.2536,  0.5679,  ..., -0.5609, -0.8464,  1.2141],\n",
       "           [-0.5978,  0.8782, -0.2456,  ..., -2.0975, -1.0922,  0.1754]],\n",
       " \n",
       "          [[-0.8345, -0.4405,  0.1962,  ..., -2.0615, -1.6732,  1.7202],\n",
       "           [-0.8206, -0.5529, -0.5213,  ..., -1.9022, -0.4227,  1.9228],\n",
       "           [-0.0353,  0.0600, -0.1903,  ..., -1.8207, -1.4383,  2.0432],\n",
       "           ...,\n",
       "           [-0.2671, -0.4005, -0.0098,  ..., -0.9676,  0.0806,  1.7959],\n",
       "           [-0.1339, -0.4132, -0.2823,  ..., -0.3605, -0.1582,  2.3464],\n",
       "           [-0.4273, -0.6043, -0.4026,  ..., -1.0890,  1.0934,  2.6735]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.7992, -0.4324,  0.7038,  ..., -1.0021, -0.1931, -0.7076],\n",
       "           [ 0.2342,  0.5729, -0.0073,  ..., -0.7616, -0.3460, -1.1005],\n",
       "           [ 0.9587,  1.2189,  0.7819,  ..., -1.3544,  0.4587, -1.4257],\n",
       "           ...,\n",
       "           [ 0.1178,  0.1008,  0.3665,  ..., -0.9286,  0.7121, -0.5239],\n",
       "           [ 0.0938, -0.1463,  0.7718,  ..., -1.5098,  0.5372, -0.6663],\n",
       "           [ 1.4139,  0.6345,  0.1512,  ..., -1.4699,  1.1277, -2.1724]],\n",
       " \n",
       "          [[ 2.4284, -0.5132,  0.5562,  ...,  1.6073, -0.6784, -1.3544],\n",
       "           [ 0.0916, -0.8308, -0.1240,  ...,  1.5630, -0.5468, -2.3174],\n",
       "           [-0.5022, -0.6827,  1.0597,  ...,  1.3833, -1.0003, -0.7040],\n",
       "           ...,\n",
       "           [-0.0763,  0.3034,  0.4178,  ...,  4.2198,  1.6078, -0.1198],\n",
       "           [-0.2347, -0.0315, -0.3193,  ...,  2.7329,  0.1389, -1.6694],\n",
       "           [-1.4220, -0.2649,  0.1271,  ...,  3.3518,  1.0434, -1.9862]],\n",
       " \n",
       "          [[-1.0188,  1.1840, -0.9034,  ..., -0.5661,  0.8596, -1.0085],\n",
       "           [-2.0808,  0.2755, -0.9356,  ...,  0.5633, -0.2284,  0.2993],\n",
       "           [-1.6495, -1.4939, -0.2738,  ...,  0.7689, -2.5170, -0.3371],\n",
       "           ...,\n",
       "           [-1.0496,  0.0520, -0.2360,  ...,  1.0690,  0.2895,  0.4200],\n",
       "           [-1.7381,  0.6384, -0.5882,  ...,  1.4162, -0.3072,  1.5518],\n",
       "           [-0.3850,  1.0314, -0.3082,  ...,  0.6575,  0.3019,  0.4930]]]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['past_key_value'].key_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 1.1425e-02, -1.3230e-02,  2.6524e-03,  ...,  1.1242e-02,\n",
       "            -4.3062e-03, -7.1702e-03],\n",
       "           [ 7.1862e-03, -7.9505e-04,  7.7968e-03,  ...,  8.3490e-04,\n",
       "             5.7170e-03, -1.1819e-02],\n",
       "           [ 3.0127e-03, -4.1507e-03, -6.8740e-03,  ...,  1.4032e-03,\n",
       "             2.2372e-03,  5.5290e-04],\n",
       "           ...,\n",
       "           [-3.5807e-03, -8.5695e-03,  9.7189e-03,  ...,  7.8387e-03,\n",
       "             3.2028e-03, -9.4971e-03],\n",
       "           [ 1.0221e-03, -3.2174e-03,  8.4431e-03,  ...,  3.1652e-03,\n",
       "            -3.3442e-03,  9.0078e-03],\n",
       "           [ 5.3439e-03,  2.7460e-03,  6.0733e-04,  ...,  1.2366e-02,\n",
       "            -3.0356e-03,  2.4451e-03]],\n",
       " \n",
       "          [[ 6.7095e-03,  2.1722e-03,  6.0971e-03,  ...,  4.5968e-03,\n",
       "             1.4262e-03, -2.3720e-03],\n",
       "           [ 8.7663e-03,  4.5856e-03, -4.4579e-03,  ...,  5.2764e-03,\n",
       "             4.9614e-03, -5.3013e-03],\n",
       "           [-1.5921e-04,  5.0845e-04, -5.8811e-04,  ...,  1.5965e-03,\n",
       "             3.6433e-04, -1.0041e-03],\n",
       "           ...,\n",
       "           [ 1.1487e-02,  1.2687e-03, -4.4479e-04,  ..., -5.1780e-03,\n",
       "             2.8358e-03,  5.1406e-03],\n",
       "           [ 1.3605e-04,  5.0781e-03,  2.6068e-03,  ...,  4.7192e-03,\n",
       "             1.7606e-03, -6.3886e-03],\n",
       "           [ 6.1797e-03,  2.8378e-03, -1.2735e-03,  ..., -5.3142e-03,\n",
       "            -1.4744e-03,  1.0353e-02]],\n",
       " \n",
       "          [[-9.0237e-03,  1.8819e-03,  2.8471e-03,  ...,  7.1291e-03,\n",
       "            -6.9838e-03,  5.4182e-03],\n",
       "           [-6.6913e-03, -7.9802e-03, -6.4098e-03,  ..., -5.0323e-03,\n",
       "            -8.1695e-03, -1.8114e-03],\n",
       "           [ 3.7951e-05, -5.1798e-03,  2.0797e-03,  ..., -9.3550e-04,\n",
       "             6.7073e-03,  7.3826e-03],\n",
       "           ...,\n",
       "           [ 9.1888e-04, -6.2716e-03, -9.1519e-03,  ...,  1.2047e-02,\n",
       "             1.5730e-03,  4.8610e-03],\n",
       "           [-1.1644e-03,  3.2291e-03,  1.3237e-02,  ..., -7.0399e-03,\n",
       "             6.8012e-03,  4.2908e-04],\n",
       "           [-1.9659e-03, -2.1878e-03, -4.2018e-03,  ..., -2.2993e-03,\n",
       "             2.6317e-03,  7.0723e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.7742e-03,  2.3113e-02, -7.8054e-02,  ..., -1.2009e-02,\n",
       "            -2.8092e-02, -4.0968e-03],\n",
       "           [-1.1506e-02, -7.7508e-02,  1.3601e-02,  ..., -5.9897e-02,\n",
       "             1.4009e-01,  9.2858e-03],\n",
       "           [ 1.0110e-02, -2.7304e-02, -3.3569e-02,  ...,  3.9598e-03,\n",
       "            -2.1768e-02, -5.9300e-03],\n",
       "           ...,\n",
       "           [ 3.0769e-02,  3.2000e-03,  1.4129e-02,  ..., -2.4851e-02,\n",
       "            -1.8625e-02,  2.6445e-02],\n",
       "           [ 2.7370e-03, -2.1939e-02, -4.3758e-03,  ..., -9.3453e-03,\n",
       "            -1.5226e-02,  1.4907e-02],\n",
       "           [-1.9749e-02, -4.1765e-02, -1.1452e-02,  ..., -3.2937e-02,\n",
       "             3.7235e-02, -1.3212e-02]],\n",
       " \n",
       "          [[ 9.3184e-03,  2.9283e-03,  5.3765e-03,  ...,  2.8492e-03,\n",
       "            -2.3230e-03, -9.9628e-03],\n",
       "           [-7.6340e-03, -2.1609e-03, -1.0945e-02,  ...,  2.1213e-03,\n",
       "            -5.7632e-03,  5.1402e-03],\n",
       "           [ 1.6900e-03, -1.1167e-03, -7.3581e-04,  ...,  2.6689e-03,\n",
       "             1.1091e-03,  1.7877e-03],\n",
       "           ...,\n",
       "           [ 8.5679e-03,  1.3053e-02, -7.3450e-03,  ...,  6.5435e-03,\n",
       "            -4.5005e-03,  8.2652e-03],\n",
       "           [ 5.3344e-04,  3.5909e-04,  4.4834e-03,  ..., -2.8594e-03,\n",
       "            -4.1441e-05, -1.8008e-03],\n",
       "           [ 1.6555e-03,  9.7970e-03, -7.4055e-03,  ..., -3.6841e-04,\n",
       "             1.0169e-02, -6.9725e-03]],\n",
       " \n",
       "          [[-1.5959e-03, -7.1478e-04,  1.2105e-02,  ...,  4.8964e-03,\n",
       "             9.6942e-04,  7.2388e-04],\n",
       "           [-7.2231e-03, -6.6034e-04,  7.9665e-03,  ...,  4.3247e-03,\n",
       "            -3.8259e-03,  8.5629e-03],\n",
       "           [-6.3740e-03, -1.4663e-03, -3.4029e-03,  ...,  3.3232e-04,\n",
       "             2.1422e-03, -6.1889e-04],\n",
       "           ...,\n",
       "           [ 7.1022e-04,  1.9271e-02, -1.5441e-03,  ...,  4.9427e-03,\n",
       "            -1.1184e-02,  2.9466e-03],\n",
       "           [-9.1303e-04,  6.4997e-05,  8.0753e-03,  ...,  2.1526e-03,\n",
       "            -1.9906e-04, -1.1882e-03],\n",
       "           [-7.1158e-04,  3.9439e-03, -8.2612e-04,  ...,  1.2909e-02,\n",
       "            -4.8564e-04,  5.0826e-03]]]], device='cuda:0'),\n",
       " tensor([[[[ 1.2607e-01, -6.1833e-02, -7.4508e-02,  ...,  4.6131e-02,\n",
       "             3.8533e-03, -3.0531e-02],\n",
       "           [-3.3102e-02,  2.1476e-02, -1.0064e-02,  ..., -3.4786e-02,\n",
       "             2.2577e-02,  9.4690e-03],\n",
       "           [-8.4859e-02, -5.3041e-02, -2.7538e-02,  ...,  1.3332e-02,\n",
       "            -5.0894e-03, -8.2910e-02],\n",
       "           ...,\n",
       "           [-1.1002e-01, -2.6179e-02,  2.9606e-02,  ..., -4.7604e-02,\n",
       "             3.2211e-02,  7.8339e-02],\n",
       "           [-1.8862e-02, -4.4158e-02, -2.2879e-02,  ...,  2.2310e-03,\n",
       "             2.2643e-02, -2.1210e-02],\n",
       "           [-1.6340e-01, -2.8063e-02,  3.8637e-02,  ...,  4.9550e-02,\n",
       "             3.4647e-02, -3.8175e-03]],\n",
       " \n",
       "          [[ 1.2847e-02,  7.4896e-03, -2.7745e-03,  ...,  7.9723e-03,\n",
       "             3.4609e-03, -4.5582e-03],\n",
       "           [ 2.7460e-03,  1.9588e-02,  1.1028e-02,  ..., -2.2025e-03,\n",
       "            -1.3637e-02,  5.6330e-03],\n",
       "           [ 1.0574e-02,  4.7071e-03, -4.8883e-03,  ..., -4.9253e-03,\n",
       "             2.9611e-03, -2.4469e-03],\n",
       "           ...,\n",
       "           [ 6.0088e-03, -9.3255e-03,  1.3849e-02,  ..., -3.9809e-03,\n",
       "             1.1593e-02, -1.2627e-02],\n",
       "           [-6.4042e-03, -9.7802e-03,  5.1324e-03,  ..., -1.9037e-02,\n",
       "             2.2273e-03, -1.0894e-02],\n",
       "           [-5.5810e-03,  2.3309e-02, -1.6290e-02,  ...,  3.4712e-03,\n",
       "             1.5037e-02,  3.6692e-04]],\n",
       " \n",
       "          [[-3.0987e-02,  3.3587e-03,  1.0149e-01,  ...,  1.2529e-01,\n",
       "             6.3482e-02, -1.1244e-02],\n",
       "           [ 7.7423e-02, -2.4520e-03, -1.1488e-01,  ...,  3.6546e-02,\n",
       "             4.9603e-02, -9.6924e-02],\n",
       "           [ 5.9861e-02, -4.4709e-03, -4.5450e-02,  ..., -3.2476e-02,\n",
       "             3.1522e-02, -4.3875e-03],\n",
       "           ...,\n",
       "           [-1.2945e-01,  2.1728e-03, -8.3724e-02,  ..., -1.4536e-01,\n",
       "             4.9969e-02,  2.4584e-01],\n",
       "           [ 1.1728e-02, -2.9480e-02,  1.0155e-01,  ..., -3.5938e-02,\n",
       "            -5.0983e-02,  1.0992e-01],\n",
       "           [-9.1293e-02,  1.3106e-02,  1.1389e-02,  ..., -5.9201e-02,\n",
       "             7.1496e-02,  2.3339e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.3578e-03, -1.3400e-02,  2.5045e-02,  ..., -5.3651e-03,\n",
       "            -1.1395e-02, -1.5584e-02],\n",
       "           [ 2.6443e-02, -2.6622e-02,  3.0015e-02,  ...,  8.9982e-03,\n",
       "             3.5569e-03,  1.5468e-02],\n",
       "           [ 3.7124e-03,  1.0669e-02,  1.4818e-02,  ..., -5.4764e-03,\n",
       "            -5.7201e-03,  2.3623e-03],\n",
       "           ...,\n",
       "           [-3.8141e-02, -9.0480e-03,  4.8447e-03,  ...,  3.2277e-02,\n",
       "            -4.9413e-02, -7.3340e-03],\n",
       "           [ 1.4948e-02,  3.3826e-02,  2.7240e-02,  ..., -6.5644e-03,\n",
       "            -1.0286e-02,  3.7562e-02],\n",
       "           [ 2.6357e-02,  1.4680e-02,  9.8269e-03,  ..., -4.7362e-03,\n",
       "            -4.1925e-02, -6.9523e-03]],\n",
       " \n",
       "          [[-1.5933e-02, -1.1276e-02,  3.9164e-02,  ..., -1.5209e-02,\n",
       "             1.2089e-02, -7.9188e-03],\n",
       "           [ 1.8563e-01, -5.8014e-02,  3.8079e-01,  ...,  1.0296e-02,\n",
       "            -3.1386e-02,  6.1951e-03],\n",
       "           [ 8.0715e-02,  2.5279e-02,  1.8474e-02,  ..., -4.8999e-03,\n",
       "             3.1806e-02, -7.5169e-03],\n",
       "           ...,\n",
       "           [-2.7060e-01,  1.2352e-02,  3.9479e-02,  ..., -3.3152e-02,\n",
       "             9.3444e-03,  1.4103e-02],\n",
       "           [-2.9407e-01,  2.1279e-03, -2.3154e-02,  ..., -8.9601e-03,\n",
       "            -2.4896e-02,  6.5088e-03],\n",
       "           [-2.3572e-01,  9.6004e-02, -3.7627e-03,  ..., -1.6096e-03,\n",
       "             5.6884e-02, -1.8590e-03]],\n",
       " \n",
       "          [[ 2.1766e-03,  7.8972e-03,  1.7245e-03,  ..., -5.4419e-03,\n",
       "            -2.9082e-03,  5.8618e-03],\n",
       "           [-3.6765e-04,  1.6586e-02, -2.7286e-02,  ..., -5.3738e-03,\n",
       "             5.5248e-03,  1.9041e-02],\n",
       "           [ 5.7380e-03, -1.8408e-03,  4.0465e-03,  ..., -6.1033e-03,\n",
       "             1.1502e-02, -2.8963e-03],\n",
       "           ...,\n",
       "           [ 6.8209e-03,  2.8195e-03, -1.1546e-05,  ...,  1.4225e-03,\n",
       "             1.7067e-02,  2.4473e-02],\n",
       "           [-8.7859e-03, -2.9242e-02, -9.4639e-03,  ..., -1.6693e-02,\n",
       "             1.3416e-02, -1.8678e-03],\n",
       "           [-9.4920e-03, -2.7010e-02,  1.4805e-03,  ...,  9.9401e-03,\n",
       "             5.7514e-03,  1.9462e-02]]]], device='cuda:0'),\n",
       " tensor([[[[ 8.2082e-02, -7.7026e-02,  5.5310e-02,  ..., -3.3213e-02,\n",
       "            -8.4107e-02, -1.5366e-01],\n",
       "           [-1.7432e-01,  2.0469e-01,  9.7080e-02,  ..., -3.1016e-02,\n",
       "            -3.1795e-01, -1.3036e-01],\n",
       "           [-1.2298e-01, -1.4468e-01,  2.1953e-02,  ...,  4.0548e-05,\n",
       "             1.0863e-01, -1.4186e-01],\n",
       "           ...,\n",
       "           [-1.1338e-01, -8.4750e-02, -2.4056e-01,  ..., -1.1043e-02,\n",
       "            -6.2156e-02,  2.8319e-01],\n",
       "           [-6.0215e-04, -3.7366e-02,  2.1136e-01,  ...,  2.1401e-01,\n",
       "            -3.6404e-03,  5.7949e-02],\n",
       "           [-1.8957e-01, -1.2565e-01,  1.3987e-02,  ...,  3.8583e-02,\n",
       "             1.5091e-01, -1.7433e-01]],\n",
       " \n",
       "          [[ 1.6077e-01, -1.4073e-01,  1.1957e-01,  ...,  7.3577e-02,\n",
       "             3.0237e-02,  1.1626e-01],\n",
       "           [-1.1735e+00,  2.0225e-01, -1.4578e-01,  ...,  3.3743e-03,\n",
       "             9.4704e-02,  6.5024e-02],\n",
       "           [-1.1951e-02,  1.1968e-01,  6.6748e-02,  ..., -1.1573e-01,\n",
       "             1.2357e-01,  7.8712e-03],\n",
       "           ...,\n",
       "           [ 4.1115e-01, -2.0656e-01, -4.8845e-01,  ..., -1.7689e-01,\n",
       "             1.5081e-01,  8.9098e-02],\n",
       "           [ 4.1371e-01, -5.7675e-02,  7.8229e-02,  ..., -2.5595e-01,\n",
       "             1.0108e-02, -9.6610e-02],\n",
       "           [ 6.9019e-02,  9.1996e-02,  3.2924e-02,  ...,  1.9336e-01,\n",
       "             1.5824e-02, -2.6290e-02]],\n",
       " \n",
       "          [[ 7.5227e-02,  5.1501e-02, -4.6367e-02,  ...,  1.4837e-02,\n",
       "             4.5225e-02,  7.3870e-03],\n",
       "           [-4.6578e-02, -1.8419e-01, -8.8785e-02,  ..., -1.7266e-01,\n",
       "            -5.0147e-03,  6.0030e-02],\n",
       "           [ 1.7914e-02,  5.7353e-02,  1.2773e-01,  ..., -3.3337e-02,\n",
       "             1.3567e-01,  1.0825e-02],\n",
       "           ...,\n",
       "           [-3.3123e-02, -1.0095e-01,  9.8326e-02,  ...,  2.2082e-02,\n",
       "             1.8210e-01,  7.3143e-03],\n",
       "           [-3.6223e-02,  8.1091e-03, -2.9829e-01,  ...,  1.1847e-01,\n",
       "            -1.2620e-01, -1.1397e-02],\n",
       "           [ 1.0705e-02,  1.9836e-02, -7.6558e-02,  ...,  1.1822e-02,\n",
       "            -1.7300e-01, -1.0494e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.1671e-02,  6.3345e-01,  2.4206e-02,  ..., -4.8024e-01,\n",
       "             1.3301e-01, -2.9043e-02],\n",
       "           [-1.6731e-02, -2.5305e-01,  1.4625e-02,  ..., -1.2392e-01,\n",
       "             2.4501e-01,  2.3508e-03],\n",
       "           [ 8.8397e-02, -4.8237e-01, -4.2517e-02,  ..., -1.0428e-02,\n",
       "            -1.1482e-01,  1.5059e-02],\n",
       "           ...,\n",
       "           [-2.3195e-02,  5.0105e-01,  1.0088e-01,  ...,  3.6806e-01,\n",
       "             1.7071e-01,  1.1151e-01],\n",
       "           [-6.4243e-02,  4.2440e-01,  9.9001e-02,  ..., -2.1376e-01,\n",
       "            -3.1537e-01,  1.7031e-01],\n",
       "           [ 1.1701e-02,  7.6969e-03,  2.6279e-01,  ...,  2.9704e-01,\n",
       "             5.9650e-01,  4.8193e-02]],\n",
       " \n",
       "          [[ 8.3204e-02, -3.3752e-01, -1.4352e-01,  ..., -3.8544e-02,\n",
       "            -4.6547e-02,  2.9849e-01],\n",
       "           [ 7.1650e-02,  5.4514e-01, -2.4478e-01,  ..., -3.1275e-01,\n",
       "            -1.1200e-01, -3.3534e-01],\n",
       "           [ 9.8520e-02,  3.9908e-03, -7.9879e-02,  ...,  2.2008e-02,\n",
       "             7.9692e-02,  8.3859e-02],\n",
       "           ...,\n",
       "           [-2.0845e-01, -4.9712e-01, -5.0400e-02,  ..., -2.6415e-01,\n",
       "            -8.6279e-03, -1.9307e-01],\n",
       "           [ 9.1830e-02, -1.9628e-01, -1.0609e-01,  ...,  8.3181e-03,\n",
       "             6.5771e-02, -3.1487e-02],\n",
       "           [-3.2609e-02,  1.0025e-01, -1.9716e-01,  ..., -6.1712e-02,\n",
       "             1.7098e-01,  1.0692e-01]],\n",
       " \n",
       "          [[-1.2960e-01, -1.5111e-03,  5.9747e-03,  ..., -8.7191e-01,\n",
       "            -4.3295e-01,  2.6118e-01],\n",
       "           [ 5.1967e-02,  6.9716e-02, -1.6603e-02,  ...,  5.9124e-02,\n",
       "             1.1093e-02,  6.7733e-02],\n",
       "           [ 1.2485e-01,  2.6168e-02,  6.2946e-02,  ...,  2.5848e-01,\n",
       "             4.8776e-02, -2.9794e-02],\n",
       "           ...,\n",
       "           [ 6.6013e-02,  6.9813e-02,  1.6831e-01,  ..., -6.2227e-02,\n",
       "            -1.6842e-01, -3.0578e-02],\n",
       "           [ 5.8509e-02,  1.2834e-02, -3.5533e-01,  ..., -8.5035e-02,\n",
       "             4.8150e-02,  2.0081e-01],\n",
       "           [-2.6948e-01,  1.5737e-01,  1.1396e-01,  ..., -2.1634e-01,\n",
       "             2.7801e-02,  3.1792e-01]]]], device='cuda:0'),\n",
       " tensor([[[[-0.0848,  0.2088,  0.2570,  ...,  0.4783, -0.0095, -0.1121],\n",
       "           [ 0.1130,  0.1740,  0.1873,  ...,  0.2168,  0.1394,  0.0059],\n",
       "           [-0.1255, -0.0514,  0.0520,  ..., -0.1336,  0.0730, -0.0166],\n",
       "           ...,\n",
       "           [ 0.2171,  0.0633,  0.0844,  ..., -0.3016,  0.0361, -0.1235],\n",
       "           [-0.2771,  0.1209,  0.4639,  ..., -0.3120,  0.2309, -0.2692],\n",
       "           [-0.2752,  0.1596,  0.1763,  ..., -0.3256,  0.0628, -0.1635]],\n",
       " \n",
       "          [[-0.0525, -0.2656, -0.3968,  ...,  0.0461,  0.2254,  0.2601],\n",
       "           [-0.5692, -0.0643, -0.3747,  ..., -0.0650,  0.1615, -0.1785],\n",
       "           [-0.1473,  0.0877, -0.2485,  ..., -0.0837,  0.0056,  0.0171],\n",
       "           ...,\n",
       "           [-0.2862,  0.0248, -0.1280,  ...,  0.3506, -0.0840, -0.2812],\n",
       "           [ 0.1530, -0.1358, -0.0886,  ..., -0.1371,  0.2030, -0.1886],\n",
       "           [ 0.0033, -0.1656,  0.1978,  ..., -0.3471,  0.2597, -0.1702]],\n",
       " \n",
       "          [[ 0.3294,  0.0773,  0.2678,  ..., -0.2186, -0.2790,  0.3000],\n",
       "           [-0.3109,  0.2143,  0.3189,  ..., -0.2444, -0.1220, -0.0049],\n",
       "           [-0.0904,  0.2170,  0.4605,  ...,  0.5252, -0.1945, -0.1612],\n",
       "           ...,\n",
       "           [ 0.1622, -0.2455,  0.4619,  ..., -0.0495, -0.0434,  0.3350],\n",
       "           [ 0.0500, -0.1253,  0.2003,  ...,  0.2430, -0.0834, -0.4924],\n",
       "           [ 0.0650,  0.0539,  0.0275,  ..., -0.2170, -0.1972,  0.1655]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0120, -0.5983, -0.3474,  ..., -0.1132, -0.1909, -0.1090],\n",
       "           [ 0.2763,  0.4310, -0.0851,  ..., -0.1603,  0.3983,  0.1855],\n",
       "           [ 0.1665, -0.1164,  0.1521,  ...,  0.1628, -0.2834,  0.0568],\n",
       "           ...,\n",
       "           [-0.1372,  0.3388,  0.1015,  ..., -0.5028,  0.9850,  0.4444],\n",
       "           [-0.4215, -0.3678,  0.1371,  ..., -0.2035,  0.4683, -0.1459],\n",
       "           [-0.2897, -0.3408,  0.0989,  ..., -0.1748,  0.3337,  0.5877]],\n",
       " \n",
       "          [[ 0.3000,  0.1382,  0.1299,  ...,  0.0208, -0.0469,  0.8827],\n",
       "           [ 0.0724,  0.3357, -0.3758,  ...,  0.2945, -0.0292,  0.1824],\n",
       "           [-0.2309,  0.7425,  0.1565,  ..., -0.0243, -0.3495,  0.3263],\n",
       "           ...,\n",
       "           [-0.2806,  0.1380, -0.4892,  ..., -0.0549, -0.0319, -0.0926],\n",
       "           [ 0.0078,  0.2359, -0.3117,  ..., -0.0444,  0.0670,  0.4520],\n",
       "           [ 0.1905, -0.0253, -0.4432,  ...,  0.0496,  0.0258, -0.0580]],\n",
       " \n",
       "          [[-0.0622,  0.2273, -0.3155,  ..., -0.1278, -0.0956, -0.1653],\n",
       "           [-0.1102,  0.2550, -0.2561,  ..., -0.3164,  0.0473, -0.1658],\n",
       "           [-0.1076,  0.4161,  0.1872,  ..., -0.2792, -0.0937, -0.1025],\n",
       "           ...,\n",
       "           [-0.0235, -0.0201, -0.0492,  ..., -0.0699,  0.1829, -0.0313],\n",
       "           [ 0.1882, -0.0631, -0.0608,  ...,  0.0270, -0.0355,  0.1619],\n",
       "           [ 0.0819, -0.0555, -0.0571,  ..., -0.1268,  0.0171,  0.1090]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 9.9513e-02, -7.2032e-02, -6.1988e-03,  ...,  4.7344e-01,\n",
       "             3.7108e-01,  3.1300e-01],\n",
       "           [ 1.0567e-01,  2.7306e-01, -8.6919e-02,  ...,  7.3074e-02,\n",
       "             4.2874e-01,  1.7674e-01],\n",
       "           [ 1.9883e-01,  9.2933e-03,  2.5318e-01,  ..., -1.8052e-02,\n",
       "            -1.6559e-01,  1.2334e-01],\n",
       "           ...,\n",
       "           [ 3.3627e-02, -2.6753e-01,  1.9342e-01,  ...,  3.3920e-01,\n",
       "            -2.7154e-02, -2.6875e-01],\n",
       "           [ 4.1570e-02,  1.4765e-01, -9.9356e-03,  ...,  1.2428e-01,\n",
       "             2.9824e-01,  2.0569e-01],\n",
       "           [ 3.0186e-01, -5.7765e-01, -1.0032e-01,  ...,  5.6302e-04,\n",
       "             1.3954e-01, -1.8584e-01]],\n",
       " \n",
       "          [[-1.6955e-01, -1.3830e-01,  1.4310e-01,  ..., -5.7484e-02,\n",
       "            -1.0795e-01, -1.8116e-01],\n",
       "           [-1.6848e-01,  2.3864e-01,  1.0884e-01,  ..., -3.7719e-03,\n",
       "            -2.3700e-01, -1.6875e-01],\n",
       "           [-3.0458e-01, -4.4682e-01, -3.9055e-01,  ...,  1.8703e-01,\n",
       "            -9.9401e-02, -1.9909e-01],\n",
       "           ...,\n",
       "           [-1.2835e-01,  2.5549e-02, -5.7717e-02,  ...,  1.5397e-01,\n",
       "            -2.5902e-02, -5.0794e-01],\n",
       "           [ 2.6080e-01, -2.0652e-01,  1.9437e-02,  ...,  1.6834e-01,\n",
       "            -2.5531e-01, -3.4186e-01],\n",
       "           [ 2.2999e-01,  1.7899e-01,  1.3552e-01,  ...,  3.8556e-01,\n",
       "            -9.2048e-02,  9.5170e-02]],\n",
       " \n",
       "          [[ 2.9760e-01,  2.7913e-01,  4.2579e-01,  ...,  1.1988e-01,\n",
       "             2.8142e-02,  9.8650e-03],\n",
       "           [-2.0438e-01, -5.2353e-01, -2.6303e-01,  ...,  4.9139e-01,\n",
       "             3.0365e-01, -6.4054e-01],\n",
       "           [ 3.0416e-01, -3.8443e-01,  9.7071e-02,  ...,  5.4581e-01,\n",
       "             2.8555e-01, -2.2390e-01],\n",
       "           ...,\n",
       "           [ 2.1150e-01,  8.7925e-02,  2.9289e-01,  ...,  4.0189e-02,\n",
       "             4.6120e-02,  1.7375e-01],\n",
       "           [-3.2130e-01, -1.6295e-01, -8.6657e-02,  ..., -2.5037e-01,\n",
       "             1.3854e-01,  3.4959e-01],\n",
       "           [-3.5871e-04, -9.1356e-02,  4.2999e-01,  ..., -9.2487e-02,\n",
       "             3.2439e-01,  8.1907e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.6145e-02, -1.2659e-01, -1.5309e-01,  ...,  7.8549e-02,\n",
       "             9.6494e-02, -1.1073e-01],\n",
       "           [-1.2642e-01,  3.2634e-01,  1.7380e-01,  ...,  9.9804e-02,\n",
       "            -7.7689e-02,  2.0297e-01],\n",
       "           [ 4.7035e-02, -1.5251e-01, -2.6896e-01,  ...,  2.7208e-01,\n",
       "            -1.1290e-01, -1.0027e-01],\n",
       "           ...,\n",
       "           [-1.4164e-01, -4.2051e-02, -2.5573e-01,  ...,  1.7612e-01,\n",
       "            -1.3672e-01,  2.2115e-01],\n",
       "           [ 8.0300e-02, -3.6899e-01, -2.8563e-01,  ...,  7.4682e-02,\n",
       "            -1.7588e-01, -1.0265e-01],\n",
       "           [ 3.8971e-02, -2.6429e-01, -2.3028e-01,  ...,  4.0651e-01,\n",
       "            -2.6447e-01,  5.5855e-02]],\n",
       " \n",
       "          [[-2.6646e-02, -8.9804e-02, -1.6055e-02,  ..., -4.6492e-02,\n",
       "            -4.0443e-03, -1.0312e-01],\n",
       "           [-8.1686e-03, -5.1339e-02, -8.6897e-02,  ...,  6.1817e-01,\n",
       "            -3.5592e-01,  2.0793e-02],\n",
       "           [ 5.7255e-02,  3.6608e-02,  1.9849e-01,  ...,  1.0588e-01,\n",
       "            -9.3621e-02,  1.0781e-01],\n",
       "           ...,\n",
       "           [-1.0477e-01, -9.7509e-02, -1.2474e-01,  ...,  4.3081e-02,\n",
       "            -3.9005e-02, -3.9338e-01],\n",
       "           [-1.2447e-01, -2.7580e-01, -7.9441e-02,  ..., -1.2678e-01,\n",
       "            -1.4677e-02, -6.9347e-02],\n",
       "           [ 7.9256e-03, -2.0126e-01,  7.7455e-03,  ..., -1.3842e-01,\n",
       "             9.7258e-02, -3.7586e-01]],\n",
       " \n",
       "          [[-1.2253e-01, -4.9852e-02,  3.1371e-02,  ...,  1.2031e-01,\n",
       "            -1.3980e-01, -9.5927e-02],\n",
       "           [ 8.9985e-03, -3.1158e-02, -3.8923e-01,  ..., -7.3194e-02,\n",
       "             1.9506e-01, -5.2391e-02],\n",
       "           [-4.1681e-01, -2.2858e-01, -6.9709e-02,  ..., -1.7514e-01,\n",
       "            -1.9715e-01, -1.6928e-01],\n",
       "           ...,\n",
       "           [ 1.6789e-01, -9.4577e-03, -1.3166e-01,  ..., -1.2296e-01,\n",
       "            -1.4521e-01,  1.8902e-01],\n",
       "           [-1.5625e-01, -3.8832e-01,  1.7414e-01,  ..., -1.2288e-01,\n",
       "            -1.5728e-01,  1.3990e-01],\n",
       "           [-6.3299e-02, -2.1212e-01,  7.9321e-02,  ...,  2.5319e-01,\n",
       "             1.6123e-01,  1.5428e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 0.0925,  0.2443,  0.8946,  ..., -0.0246, -0.2347,  0.6308],\n",
       "           [ 0.1006,  0.3450,  0.0417,  ..., -0.1597,  0.2949,  0.7180],\n",
       "           [ 0.1822,  0.8182, -0.0821,  ...,  0.0883,  0.2403,  0.2615],\n",
       "           ...,\n",
       "           [ 0.4787,  0.1180,  0.2792,  ..., -0.0506,  0.0567, -0.1446],\n",
       "           [ 0.8839, -0.2056,  0.2768,  ..., -0.3841,  0.0315,  0.0287],\n",
       "           [ 0.8162,  0.2752,  0.6767,  ..., -0.5462,  0.4494,  0.1664]],\n",
       " \n",
       "          [[ 0.1789, -0.0849,  0.1192,  ..., -0.4742, -0.4921, -0.4052],\n",
       "           [-0.1324, -0.4091, -0.0107,  ...,  0.8072, -0.1632,  1.0853],\n",
       "           [ 0.0050, -0.2113, -0.2231,  ..., -0.0125, -0.4992, -0.0516],\n",
       "           ...,\n",
       "           [ 0.3925,  0.2217, -0.8389,  ...,  0.4634,  1.1153, -0.0617],\n",
       "           [ 0.1552, -0.1416, -0.2383,  ...,  0.4050, -0.0304,  0.0341],\n",
       "           [-0.1552,  0.0928, -0.1714,  ...,  0.1736,  0.1087,  0.3842]],\n",
       " \n",
       "          [[-0.2701,  0.0269,  0.0033,  ...,  0.1955, -0.4619, -0.3733],\n",
       "           [ 0.0551,  0.1189,  0.2241,  ...,  0.0457,  0.3283, -0.0916],\n",
       "           [-0.1057, -0.1342,  0.2019,  ...,  0.0654, -0.2502, -0.3375],\n",
       "           ...,\n",
       "           [ 0.2766,  0.0019,  0.4868,  ..., -0.2510, -0.1431, -0.3031],\n",
       "           [ 0.1233, -0.1398, -0.1244,  ...,  0.2270, -0.4150, -0.8282],\n",
       "           [ 0.0844,  0.0103,  0.1235,  ..., -0.3041, -0.3581, -0.1989]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.1398,  0.0762,  0.0468,  ...,  0.3574, -0.5591,  0.0660],\n",
       "           [ 0.1720, -0.0828, -0.1219,  ..., -0.0165,  0.0701, -0.1318],\n",
       "           [ 0.3535, -0.1226, -0.0344,  ...,  0.3980, -0.2113,  0.1903],\n",
       "           ...,\n",
       "           [-0.0011, -0.3516,  0.0927,  ...,  0.2864,  0.6028, -0.2529],\n",
       "           [ 0.1529,  0.0741, -0.2222,  ..., -0.0153,  0.3087,  0.2396],\n",
       "           [ 0.0406,  0.0690, -0.3925,  ...,  0.1015, -0.0495,  0.2149]],\n",
       " \n",
       "          [[ 0.3395,  0.1008,  0.2329,  ...,  0.0137, -0.0905,  0.1220],\n",
       "           [ 0.0967, -0.3155, -0.3692,  ...,  0.0463, -0.0309, -0.1372],\n",
       "           [ 0.3068,  0.2462, -0.2402,  ...,  0.1258,  0.1597, -0.2250],\n",
       "           ...,\n",
       "           [-0.0776, -0.1490,  0.3645,  ...,  0.2652,  0.3233, -0.1215],\n",
       "           [ 0.0674, -0.2114, -0.0021,  ...,  0.0578, -0.2939, -0.0100],\n",
       "           [ 0.0128, -0.0420,  0.0891,  ...,  0.6246, -0.3525, -0.1488]],\n",
       " \n",
       "          [[-0.0974,  0.1395, -0.1570,  ...,  0.1767,  0.0452,  0.1750],\n",
       "           [ 0.3656,  0.1007, -0.3232,  ...,  0.7593, -0.1243,  0.0213],\n",
       "           [ 0.1975,  0.6894, -0.2469,  ...,  0.3896, -0.3926, -0.0783],\n",
       "           ...,\n",
       "           [ 0.1493,  0.1912, -0.4435,  ...,  0.1894,  0.1552,  0.0538],\n",
       "           [ 0.2323,  0.3707, -0.1173,  ...,  0.2041, -0.0975,  0.1268],\n",
       "           [-0.2204,  0.0763,  0.1878,  ..., -0.0898, -0.2975,  0.0814]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 0.3991,  0.3316, -0.1625,  ..., -0.5417, -0.1245,  0.4902],\n",
       "           [ 0.1220, -0.2661,  0.6562,  ..., -0.1775,  0.0958,  0.2839],\n",
       "           [ 0.4469,  0.1400, -0.1024,  ..., -0.2704, -1.0357,  0.8428],\n",
       "           ...,\n",
       "           [-0.3285,  0.0592, -0.1281,  ...,  0.5154,  0.1596,  0.2848],\n",
       "           [ 0.1572,  0.5513, -0.1758,  ..., -0.3056,  0.0827,  0.4480],\n",
       "           [-0.2966,  0.0252, -0.1063,  ...,  0.0286,  0.0473,  0.1893]],\n",
       " \n",
       "          [[ 0.1082, -0.0615, -0.1580,  ...,  0.0570, -0.1913, -0.4024],\n",
       "           [ 0.0756, -0.0220, -0.1508,  ..., -0.0696, -0.1970, -0.5699],\n",
       "           [ 0.3393,  0.3342, -0.1007,  ..., -0.5166, -0.1856, -0.0393],\n",
       "           ...,\n",
       "           [ 0.2446,  0.3226,  0.1790,  ...,  0.1566,  0.2197,  0.3254],\n",
       "           [-0.2012,  0.2164,  0.1503,  ..., -0.0825,  0.2070, -0.2531],\n",
       "           [-0.2109,  0.3300, -0.0562,  ...,  0.0215, -0.1227,  0.1150]],\n",
       " \n",
       "          [[-0.3683, -0.4771, -0.4877,  ..., -1.5529, -0.1576, -0.1040],\n",
       "           [ 0.1561,  0.4520, -0.1086,  ..., -0.4500, -0.0140,  0.2711],\n",
       "           [-0.3092, -0.5226, -0.4187,  ..., -0.8100, -0.4945,  0.0202],\n",
       "           ...,\n",
       "           [ 0.3113,  0.3548,  0.7818,  ...,  0.0755,  0.2009, -0.4459],\n",
       "           [ 0.9820,  0.6462, -0.2069,  ..., -0.2338,  0.4474, -0.4268],\n",
       "           [ 0.4484,  0.3119,  0.0842,  ..., -0.2335,  0.2854, -0.2288]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.1520,  0.3985, -0.1951,  ...,  0.0480, -0.2311,  0.0340],\n",
       "           [-0.0374, -0.0863,  0.0341,  ...,  0.0752,  0.1394, -0.0657],\n",
       "           [-0.0304, -0.0658, -0.5102,  ..., -0.3062,  0.1413,  0.4158],\n",
       "           ...,\n",
       "           [-0.0395, -0.0683,  0.1260,  ...,  0.2395,  0.0243, -0.4620],\n",
       "           [ 0.1758,  0.0749, -0.1115,  ..., -0.1667,  0.0329,  0.0851],\n",
       "           [ 0.1206, -0.0407,  0.2608,  ...,  0.4232, -0.2875,  0.0823]],\n",
       " \n",
       "          [[ 0.3632,  0.0446, -0.1604,  ...,  0.3027,  0.3512,  0.2572],\n",
       "           [ 0.3599, -0.0680, -0.0824,  ..., -0.0755,  0.1310, -0.0558],\n",
       "           [ 0.2954,  0.0280,  0.0959,  ...,  0.0960,  0.5135,  0.2410],\n",
       "           ...,\n",
       "           [ 0.6551, -0.3406, -0.2649,  ..., -0.0726, -0.2102, -0.2474],\n",
       "           [ 0.0587,  0.1208, -0.2966,  ..., -0.0251, -0.6484, -0.4338],\n",
       "           [ 0.4992, -0.2543, -0.2876,  ..., -0.5178, -0.1263, -0.0345]],\n",
       " \n",
       "          [[-0.2600, -0.0069,  0.2615,  ..., -0.2231, -0.2828,  0.3307],\n",
       "           [ 0.3070, -0.4610, -0.3183,  ...,  0.1847,  0.0596, -0.0429],\n",
       "           [ 0.2940,  0.0061,  0.5265,  ..., -0.4088, -0.2589,  0.3750],\n",
       "           ...,\n",
       "           [ 0.0099,  0.0348, -0.3540,  ..., -0.1785,  0.2088,  0.0791],\n",
       "           [ 0.0114,  0.1507, -0.1125,  ...,  0.0066, -0.0684, -0.0786],\n",
       "           [ 0.2460,  0.0470, -0.2393,  ...,  0.2074,  0.0771, -0.0497]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-0.3071, -0.1357, -0.0905,  ..., -0.3270,  0.0053, -0.0912],\n",
       "           [ 0.1538, -0.1410, -0.4063,  ..., -0.5892,  0.1532, -0.4075],\n",
       "           [-0.1483, -0.3109, -0.2265,  ..., -0.1751, -0.6597, -0.2257],\n",
       "           ...,\n",
       "           [ 0.2764, -0.3576, -0.4051,  ..., -0.2121, -0.0683, -0.3131],\n",
       "           [ 0.1658, -0.2547,  0.0045,  ..., -0.0392,  0.2069,  0.0577],\n",
       "           [ 0.4098, -0.1824, -0.1742,  ..., -0.1989,  0.2633,  0.1611]],\n",
       " \n",
       "          [[ 0.0183, -0.4639,  0.3823,  ...,  0.1606, -0.3520, -0.4839],\n",
       "           [-0.1883, -0.2969, -0.0179,  ..., -0.1750,  0.4979,  0.0367],\n",
       "           [-0.0318, -0.8251,  0.2946,  ..., -0.0818,  0.1853, -0.1275],\n",
       "           ...,\n",
       "           [-0.8481,  0.4082,  0.0862,  ..., -0.9004, -0.1918,  0.0040],\n",
       "           [-0.3975,  0.2648,  0.0690,  ...,  0.4513,  0.2548, -0.2791],\n",
       "           [-0.1956, -0.1103,  0.0340,  ..., -0.0314,  0.1893,  0.2741]],\n",
       " \n",
       "          [[-0.3219,  0.2112, -0.4968,  ..., -0.4498,  0.6346,  0.6931],\n",
       "           [-0.1601,  0.2619, -0.3036,  ..., -0.1796,  0.1209, -0.1320],\n",
       "           [-0.2362, -0.2770, -0.3250,  ..., -0.1258,  1.0208, -0.0721],\n",
       "           ...,\n",
       "           [-0.2837,  0.5789, -0.1371,  ..., -0.3197, -0.0115,  0.0666],\n",
       "           [-0.4113,  0.1523, -0.4658,  ...,  0.1661,  0.3646,  0.3041],\n",
       "           [-0.1466, -0.4489,  0.2605,  ..., -0.0144, -0.1230,  0.3430]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.3605, -0.2061,  0.0994,  ..., -0.3283,  0.0583,  0.4439],\n",
       "           [-0.1976,  0.0923,  0.0991,  ..., -0.2841,  0.0212, -0.0640],\n",
       "           [-0.0933, -0.3049, -0.3079,  ...,  0.2324,  0.5654,  0.2266],\n",
       "           ...,\n",
       "           [-0.8366, -0.5121, -0.3027,  ...,  0.0043,  0.2352,  0.0578],\n",
       "           [-0.0509,  0.1912, -0.6762,  ..., -0.0041,  0.7264,  0.1499],\n",
       "           [-0.2793, -0.1740, -0.0575,  ...,  0.0565,  0.0761, -0.1812]],\n",
       " \n",
       "          [[-0.1302,  0.4027, -0.1128,  ..., -0.3244, -0.0050, -0.0756],\n",
       "           [ 0.6347, -0.0438,  0.6178,  ...,  0.3713, -0.0819,  0.2754],\n",
       "           [ 0.1674,  0.2153, -0.2013,  ..., -0.0473, -0.0820,  1.0088],\n",
       "           ...,\n",
       "           [-0.1399, -0.2673, -0.1671,  ...,  0.5277,  0.5454,  0.0378],\n",
       "           [-0.3178,  0.2767,  0.0311,  ...,  0.1386,  0.0032,  0.3537],\n",
       "           [ 0.2686, -0.2523, -0.0775,  ...,  0.5449,  0.2897, -0.1931]],\n",
       " \n",
       "          [[-0.1522,  0.6125,  0.5383,  ..., -0.1540,  0.2295, -0.2694],\n",
       "           [-0.2366,  0.4956,  0.5850,  ...,  0.2303,  0.1011,  0.0713],\n",
       "           [-0.8193,  0.2843,  0.2163,  ..., -0.4485, -0.2050, -0.2215],\n",
       "           ...,\n",
       "           [-0.0366, -0.3042,  0.0228,  ...,  0.1624,  0.0515, -0.5012],\n",
       "           [-0.0497, -0.0145,  0.6479,  ...,  0.3587, -0.2445, -0.0014],\n",
       "           [-0.1587,  0.0651,  0.3143,  ...,  0.0129, -0.2435,  0.0237]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 1.6217e-01, -2.4893e-01, -2.6470e-01,  ...,  6.6452e-01,\n",
       "            -1.0858e-01,  9.2449e-01],\n",
       "           [ 2.5158e-01,  2.4005e-01,  3.0574e-01,  ..., -4.8012e-04,\n",
       "            -6.1345e-01, -1.7897e-02],\n",
       "           [-3.2363e-01,  4.8671e-01, -6.0454e-02,  ...,  4.0900e-01,\n",
       "            -4.8043e-01,  7.0827e-02],\n",
       "           ...,\n",
       "           [-4.1808e-01,  7.7991e-02, -3.9518e-01,  ...,  2.8526e-01,\n",
       "             2.2596e-01, -4.1344e-02],\n",
       "           [-1.5021e-01, -2.0025e-02,  2.9417e-01,  ...,  5.4206e-01,\n",
       "             4.6636e-01, -1.1274e-01],\n",
       "           [ 7.4146e-02, -6.6102e-01,  6.1897e-02,  ...,  2.1682e-01,\n",
       "             7.9832e-02, -1.1124e-01]],\n",
       " \n",
       "          [[-2.4550e-01, -2.9533e-03, -4.0657e-01,  ..., -4.8397e-02,\n",
       "             1.7932e-01,  6.9446e-02],\n",
       "           [-6.0675e-02, -2.0538e-01, -2.3993e-01,  ...,  1.6217e-02,\n",
       "             1.8664e-01,  7.4257e-02],\n",
       "           [-3.6623e-01, -9.1340e-02, -4.6169e-01,  ...,  8.9346e-01,\n",
       "            -2.4445e-01, -2.5145e-01],\n",
       "           ...,\n",
       "           [-4.0299e-01, -1.2473e-01, -5.1487e-01,  ..., -1.7164e-01,\n",
       "            -6.1238e-01,  2.3541e-01],\n",
       "           [-1.6732e-01,  5.9866e-03,  1.5120e-01,  ..., -4.9590e-02,\n",
       "             1.2612e-01,  2.4649e-01],\n",
       "           [-1.9252e-01, -3.0947e-01, -2.4554e-02,  ...,  5.0708e-02,\n",
       "             2.4100e-01, -2.3521e-01]],\n",
       " \n",
       "          [[ 2.4751e-01,  2.6980e-01, -2.9057e-02,  ..., -5.2639e-02,\n",
       "            -4.0085e-01,  1.9435e-01],\n",
       "           [ 1.5436e-01, -4.2340e-01,  1.3778e-01,  ...,  5.8965e-02,\n",
       "            -1.1362e-01, -1.6575e-01],\n",
       "           [ 7.1570e-02, -5.1663e-01, -4.0177e-02,  ...,  1.1028e-01,\n",
       "             9.5511e-02, -1.5431e-01],\n",
       "           ...,\n",
       "           [ 3.8886e-01,  3.1015e-01,  2.5839e-01,  ...,  1.2339e-01,\n",
       "             4.6218e-02,  1.4814e-01],\n",
       "           [ 2.3883e-01,  4.1118e-01,  2.7579e-01,  ..., -2.9287e-01,\n",
       "            -7.7429e-02,  4.3929e-01],\n",
       "           [ 1.8038e-02, -1.3908e-01,  8.9713e-02,  ..., -3.1160e-01,\n",
       "            -1.3576e-01, -1.2322e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.3163e-02, -3.1263e-01, -4.6113e-01,  ...,  1.1026e-01,\n",
       "            -8.3054e-01,  2.0036e-01],\n",
       "           [-1.6065e-01,  1.2253e-01,  1.9905e-01,  ..., -3.4230e-01,\n",
       "            -2.5654e-01,  3.5856e-02],\n",
       "           [-8.9487e-02,  5.8597e-03,  1.1351e-01,  ...,  2.8452e-02,\n",
       "            -1.9793e-01, -2.3700e-01],\n",
       "           ...,\n",
       "           [-2.0897e-01, -3.4704e-01, -1.7977e-01,  ..., -8.0298e-03,\n",
       "            -2.2118e-01,  4.8122e-02],\n",
       "           [-3.3482e-01, -5.5421e-01, -3.5376e-01,  ..., -2.3908e-01,\n",
       "             9.0947e-03, -1.4443e-01],\n",
       "           [-1.1585e-01, -5.9628e-02, -5.8697e-01,  ..., -8.9589e-02,\n",
       "             2.9213e-01, -4.8843e-02]],\n",
       " \n",
       "          [[ 1.9953e-01,  2.1036e-01, -7.2438e-03,  ..., -6.5917e-02,\n",
       "             3.1297e-01,  3.9125e-02],\n",
       "           [-6.7111e-01,  2.7831e-01,  2.2966e-01,  ..., -1.8471e-01,\n",
       "             1.2157e-01, -1.2293e-01],\n",
       "           [ 2.1169e-01, -6.6474e-02,  1.7078e-01,  ..., -4.9556e-01,\n",
       "             3.9425e-01, -7.6258e-01],\n",
       "           ...,\n",
       "           [-1.3119e-01, -1.6025e-01, -1.2868e-01,  ...,  6.8695e-02,\n",
       "             2.0475e-01, -1.5106e-01],\n",
       "           [ 2.6002e-01,  1.0279e-01,  1.1080e-01,  ..., -3.7486e-01,\n",
       "            -1.1550e-01, -2.4228e-01],\n",
       "           [ 2.8585e-01,  2.5957e-01,  1.2396e-02,  ..., -4.9408e-01,\n",
       "             3.2526e-01, -5.7419e-01]],\n",
       " \n",
       "          [[ 1.4043e-01,  5.9970e-01,  3.7013e-01,  ..., -7.4990e-02,\n",
       "             2.8085e-01, -2.5552e-01],\n",
       "           [ 2.3767e-01, -4.5368e-02, -4.9477e-02,  ...,  7.1031e-02,\n",
       "             5.8438e-01,  8.0557e-02],\n",
       "           [-2.4805e-01,  7.5602e-02,  2.8428e-01,  ..., -3.5012e-01,\n",
       "            -1.2830e-01, -2.9355e-01],\n",
       "           ...,\n",
       "           [ 3.2824e-01,  3.0460e-01,  3.1306e-01,  ...,  5.3265e-02,\n",
       "             4.6065e-01,  1.2911e-01],\n",
       "           [ 4.8028e-01, -1.9661e-01, -6.0446e-02,  ...,  4.4970e-02,\n",
       "             2.3341e-01,  1.2825e-01],\n",
       "           [ 1.1126e-01, -4.1917e-01,  3.2836e-01,  ..., -2.8617e-01,\n",
       "            -3.5609e-02,  4.2716e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 0.1271, -0.0974, -0.2914,  ...,  0.4568, -0.4735,  0.1564],\n",
       "           [ 0.1427,  0.3692, -0.3044,  ..., -0.1026, -0.3184,  0.0756],\n",
       "           [ 0.0124,  0.2951, -0.2369,  ...,  0.2631, -0.1444, -0.3914],\n",
       "           ...,\n",
       "           [-0.0064,  0.2146,  0.0279,  ..., -0.2554, -0.1667,  0.2578],\n",
       "           [ 0.0908,  0.1139,  0.1194,  ...,  0.4228, -0.0591,  0.0186],\n",
       "           [ 0.3304,  0.1961,  0.2471,  ...,  0.2720, -0.0478, -0.1416]],\n",
       " \n",
       "          [[-0.0688, -0.2765, -0.0181,  ..., -0.6672,  0.3820,  0.8057],\n",
       "           [-0.5075, -0.0865, -0.0276,  ..., -0.5564,  0.2011, -0.0215],\n",
       "           [-0.3118, -0.8001, -0.2586,  ..., -0.5877,  0.3222,  0.3447],\n",
       "           ...,\n",
       "           [ 0.0701, -0.4049,  0.4622,  ..., -0.8639,  0.1411, -0.2858],\n",
       "           [-0.1309, -0.1046,  0.3103,  ..., -0.1584,  0.2080, -0.3800],\n",
       "           [-0.1163, -0.0392,  0.4900,  ..., -0.3286, -0.0662, -0.5415]],\n",
       " \n",
       "          [[ 0.1805, -0.2385,  0.2905,  ...,  0.1426,  0.0357, -0.0907],\n",
       "           [-0.3666,  0.1847, -0.2183,  ..., -0.2461,  0.0850, -0.0695],\n",
       "           [ 0.0123,  0.0218, -0.1420,  ..., -0.1364, -0.3857,  0.0644],\n",
       "           ...,\n",
       "           [-0.1857, -0.1258,  0.0060,  ..., -0.1964, -0.2210, -0.0751],\n",
       "           [-0.2038, -0.1770, -0.0693,  ..., -0.2325, -0.2913,  0.2184],\n",
       "           [-0.1623,  0.0353, -0.0887,  ...,  0.0315, -0.0899,  0.0459]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0148,  0.1994, -0.3865,  ...,  0.2145, -0.2569, -0.0945],\n",
       "           [ 0.1447,  0.0565,  0.4922,  ...,  0.2585,  0.4475, -0.2609],\n",
       "           [-0.2464,  0.0976, -0.2689,  ...,  0.2466,  0.0260,  0.5909],\n",
       "           ...,\n",
       "           [-0.4300, -0.7216, -0.1963,  ...,  0.3779,  0.6394, -0.0010],\n",
       "           [ 0.3046, -0.2988, -0.0644,  ...,  0.2156,  0.6143,  0.2981],\n",
       "           [ 0.0088, -0.7304, -0.6082,  ...,  0.1865, -0.3295,  0.7184]],\n",
       " \n",
       "          [[ 0.0291,  0.1613, -0.2096,  ...,  0.0466, -0.0302,  0.5690],\n",
       "           [ 0.1686,  0.1489,  0.0382,  ..., -0.0494, -0.3099,  0.1979],\n",
       "           [-0.1032,  0.1351, -0.1251,  ..., -0.1015,  0.0116,  0.4105],\n",
       "           ...,\n",
       "           [-0.0223, -0.0430,  0.1164,  ..., -0.0542, -0.0818, -0.1328],\n",
       "           [ 0.2704, -0.0399,  0.0263,  ..., -0.0819, -0.0979,  0.0476],\n",
       "           [ 0.0443,  0.0655, -0.0554,  ..., -0.3116, -0.3170, -0.0314]],\n",
       " \n",
       "          [[ 0.2227, -0.0496, -0.1382,  ...,  0.0679, -0.0460, -0.0194],\n",
       "           [ 0.0342,  0.1813,  0.0127,  ...,  0.0238,  0.0710, -0.5378],\n",
       "           [ 0.3499, -0.4340,  0.0631,  ...,  0.0079, -0.1775,  0.0835],\n",
       "           ...,\n",
       "           [ 0.5192, -0.3784, -0.1545,  ...,  0.1238,  0.1369, -0.2513],\n",
       "           [ 0.2384, -0.1789, -0.2565,  ...,  0.1480,  0.2862, -0.3754],\n",
       "           [ 0.1054,  0.0805,  0.0552,  ...,  0.4532,  0.2860, -0.0656]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 0.2202, -0.0635,  0.1176,  ...,  0.2105,  0.1674,  0.2294],\n",
       "           [ 0.1736, -0.0942, -0.1002,  ..., -0.3290,  0.0738, -0.3114],\n",
       "           [ 0.0985, -0.2161,  0.1121,  ..., -0.4533, -0.4161,  0.4178],\n",
       "           ...,\n",
       "           [ 0.1674,  0.1490,  0.3048,  ..., -0.4188,  0.1326,  0.1027],\n",
       "           [ 0.1358,  0.0070,  0.1088,  ..., -0.0777,  0.6315, -0.0327],\n",
       "           [-0.6407, -0.3830,  0.5775,  ...,  0.6918,  0.8415, -0.1153]],\n",
       " \n",
       "          [[ 0.2481, -0.2337,  0.2456,  ...,  0.0721,  0.0362, -0.2586],\n",
       "           [ 0.3123,  0.0587, -0.0527,  ..., -0.4317, -0.1565,  0.2852],\n",
       "           [ 0.3260, -0.0119,  0.0119,  ...,  0.0949, -0.3907, -0.2218],\n",
       "           ...,\n",
       "           [ 0.3180,  0.3131, -0.1916,  ..., -0.3409,  0.5362, -0.2578],\n",
       "           [-0.2880,  0.3939, -0.1127,  ...,  0.0038,  0.3375, -0.4635],\n",
       "           [-0.2626,  0.3255, -0.5206,  ...,  0.1554,  0.1546, -0.0464]],\n",
       " \n",
       "          [[-0.0101, -0.4131,  0.5020,  ..., -0.1703,  0.5729,  0.5831],\n",
       "           [ 0.1629, -0.3996,  0.4398,  ..., -0.4751,  0.0608, -0.4168],\n",
       "           [ 0.5459,  0.4138,  0.3161,  ..., -0.3166, -0.2931,  0.3751],\n",
       "           ...,\n",
       "           [-0.2797,  0.3640,  0.0819,  ...,  0.3745,  0.4122,  0.1645],\n",
       "           [-0.2977,  0.7743,  0.3726,  ...,  0.3224,  0.1340,  0.2576],\n",
       "           [ 0.1822,  0.1227,  0.4681,  ...,  0.0237, -0.3454,  0.8501]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0432, -0.5583, -0.0376,  ..., -0.2999, -0.1231, -0.4858],\n",
       "           [-0.0631,  0.0812, -0.1907,  ...,  0.1869, -0.1520,  0.1562],\n",
       "           [-0.1650,  0.0204,  0.0216,  ..., -0.0493, -0.2676, -0.1641],\n",
       "           ...,\n",
       "           [-0.0241, -0.4159,  0.2379,  ..., -0.3810, -0.1778, -0.4383],\n",
       "           [-0.1800, -0.4748, -0.1346,  ..., -0.3364, -0.1700, -0.6912],\n",
       "           [-0.3823, -0.6120,  0.1840,  ..., -0.1356, -0.1800, -0.6050]],\n",
       " \n",
       "          [[ 0.3359,  0.3927, -0.0954,  ..., -0.3352, -0.1300,  0.2980],\n",
       "           [ 0.3639,  0.3836,  0.0554,  ...,  0.3004, -0.0644,  0.1765],\n",
       "           [-0.0383,  0.1238,  0.0606,  ..., -0.2932,  0.0482,  0.3057],\n",
       "           ...,\n",
       "           [ 0.0133, -0.3903, -0.1572,  ...,  0.0513, -0.0820, -0.1845],\n",
       "           [ 0.0955, -0.1889, -0.3078,  ..., -0.1658,  0.1361,  0.0752],\n",
       "           [ 0.4093, -0.0347,  0.1019,  ..., -0.0063, -0.3698, -0.1044]],\n",
       " \n",
       "          [[-0.0829, -0.4977,  0.5600,  ..., -0.1077,  0.0056, -0.2843],\n",
       "           [ 0.1045, -0.0357,  0.2494,  ..., -0.2210,  0.1056,  0.3012],\n",
       "           [-0.0231, -0.0713,  0.4623,  ...,  0.1956, -0.0128,  0.1148],\n",
       "           ...,\n",
       "           [-0.1928, -0.1654,  0.3721,  ...,  0.2056,  0.1616, -0.1902],\n",
       "           [-0.2651,  0.1828, -0.3981,  ...,  0.1828,  0.1900, -0.0682],\n",
       "           [ 0.1833, -0.4557,  0.2425,  ...,  0.0121, -0.0050,  0.0296]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 0.1893,  0.0607,  0.5088,  ...,  0.1103, -0.3226, -0.1570],\n",
       "           [ 0.2603,  0.1041, -0.4671,  ..., -0.0153,  0.0390,  0.1008],\n",
       "           [-0.1208, -0.0339,  0.1562,  ..., -0.2053, -0.0999,  0.0705],\n",
       "           ...,\n",
       "           [ 0.5077, -0.1441, -0.0746,  ...,  0.0313, -0.6657, -0.2369],\n",
       "           [ 0.1757, -0.1865, -0.0314,  ...,  0.1434, -0.4834, -0.0528],\n",
       "           [ 0.3627, -0.4973,  0.1580,  ...,  0.0770, -0.0985, -0.0609]],\n",
       " \n",
       "          [[ 0.5004, -0.0849, -0.3951,  ..., -0.0346, -0.0335,  0.2496],\n",
       "           [ 0.3125, -0.1909, -0.6776,  ..., -0.4379,  0.5516,  0.4542],\n",
       "           [ 0.1882, -0.0860, -0.2119,  ..., -0.0630,  0.3316,  0.3432],\n",
       "           ...,\n",
       "           [ 0.1057,  0.3010,  0.2831,  ...,  0.1324,  0.3653, -0.1075],\n",
       "           [-0.0727,  0.2123, -0.0615,  ...,  0.0480,  0.3836,  0.0254],\n",
       "           [-0.2972,  0.1185,  0.0153,  ...,  0.0593,  0.2928, -0.3128]],\n",
       " \n",
       "          [[-0.4129,  0.8611,  0.4078,  ...,  0.1148,  0.6667, -0.3458],\n",
       "           [-0.1118,  0.4273,  0.6257,  ...,  0.6074,  0.8687, -0.0463],\n",
       "           [ 0.1014,  0.1668, -0.5122,  ...,  0.7168,  0.0528, -0.2332],\n",
       "           ...,\n",
       "           [ 0.0531,  0.2229,  0.2785,  ...,  0.0054, -0.6685,  0.1791],\n",
       "           [ 0.2768, -0.0883,  0.0349,  ..., -0.6651, -0.6599,  0.2584],\n",
       "           [ 0.7151,  0.1305,  0.1221,  ..., -0.2899, -0.3220,  0.2208]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.4024,  0.2208,  0.1519,  ...,  0.2304, -0.4460,  0.0119],\n",
       "           [ 0.1708,  0.6353, -0.0129,  ...,  0.4292,  0.1953,  1.3016],\n",
       "           [-0.3342,  0.0065,  0.2921,  ...,  0.0768,  0.3015,  0.1451],\n",
       "           ...,\n",
       "           [-0.4329,  0.0987,  0.0250,  ...,  0.2999, -0.2295,  0.6665],\n",
       "           [-0.2000,  0.1645, -0.4200,  ...,  0.1325, -0.0360,  0.8474],\n",
       "           [-0.2660, -0.0398,  0.2310,  ...,  0.6474, -0.4567,  0.5242]],\n",
       " \n",
       "          [[ 0.0352,  0.3154,  0.1485,  ..., -0.3079, -0.7099, -0.2263],\n",
       "           [-0.3373,  0.1238,  0.2052,  ...,  0.5614, -0.0636, -0.1562],\n",
       "           [ 0.0357,  0.2042, -0.0137,  ..., -0.0610, -0.6573, -0.2584],\n",
       "           ...,\n",
       "           [-0.0220, -0.4219, -0.3691,  ..., -0.1224,  0.1386, -0.2558],\n",
       "           [-0.1277, -0.1618, -0.1059,  ...,  0.0484,  0.2820, -0.2368],\n",
       "           [ 0.2578,  0.2252,  0.0214,  ..., -0.0787,  0.2640, -0.0742]],\n",
       " \n",
       "          [[ 0.1905,  0.0231, -0.2561,  ...,  0.2859, -0.0722,  0.4241],\n",
       "           [ 0.2285,  0.4266, -0.2575,  ..., -0.1435,  0.0689,  0.2147],\n",
       "           [-0.0025, -0.5215,  0.1815,  ...,  0.2063,  0.3594,  0.1268],\n",
       "           ...,\n",
       "           [ 0.0923, -0.0756, -0.3925,  ...,  0.2347,  0.2352,  0.1913],\n",
       "           [ 0.4360,  0.0462,  0.0049,  ..., -0.0708,  0.1045,  0.0598],\n",
       "           [ 0.3031, -0.0559,  0.4377,  ...,  0.1092,  0.5529,  0.1530]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 1.6304e-01, -4.9796e-01,  2.5344e-01,  ...,  1.1882e-02,\n",
       "            -7.1327e-02, -5.2437e-01],\n",
       "           [ 2.4800e-01, -6.0027e-01,  4.6301e-01,  ..., -3.6215e-01,\n",
       "            -4.8315e-01, -5.6617e-01],\n",
       "           [ 3.9655e-01, -2.7740e-01, -6.4075e-01,  ...,  4.3334e-01,\n",
       "            -5.6260e-01, -5.7009e-01],\n",
       "           ...,\n",
       "           [ 2.0538e-02, -7.6347e-02, -2.8125e-01,  ..., -1.8343e-01,\n",
       "             8.0128e-02, -3.5461e-01],\n",
       "           [ 2.6174e-01, -2.3259e-02, -1.1659e-01,  ...,  1.6715e-01,\n",
       "             2.0888e-01, -1.8158e-01],\n",
       "           [-3.9176e-01,  6.7048e-01, -4.0373e-01,  ...,  9.6483e-02,\n",
       "             5.2160e-02,  1.2113e-01]],\n",
       " \n",
       "          [[-6.1989e-02, -2.1742e-01,  3.3023e-02,  ...,  3.2207e-01,\n",
       "            -3.8247e-02,  4.0300e-02],\n",
       "           [ 1.8377e-01, -4.2279e-01, -3.7083e-01,  ...,  5.7106e-01,\n",
       "            -1.0706e-03, -2.3636e-01],\n",
       "           [ 1.4209e-01, -1.3627e-02,  1.4331e-01,  ...,  5.0152e-02,\n",
       "             4.5382e-01,  1.5814e-01],\n",
       "           ...,\n",
       "           [-1.3809e-01, -1.8135e-02,  1.8761e-01,  ..., -1.3258e-01,\n",
       "             2.8162e-01, -2.2724e-01],\n",
       "           [-6.7029e-02,  7.1580e-02,  1.0288e-01,  ..., -2.2750e-01,\n",
       "             9.2620e-02,  6.1501e-02],\n",
       "           [ 2.0779e-01,  2.6726e-02,  2.3873e-01,  ..., -1.6969e-01,\n",
       "            -4.8477e-01,  1.3742e-01]],\n",
       " \n",
       "          [[ 1.8172e-01,  4.1734e-01,  1.0611e+00,  ..., -4.5523e-01,\n",
       "            -2.6944e-01,  3.2092e-01],\n",
       "           [-7.6781e-01,  1.0784e+00, -9.0435e-01,  ..., -2.0730e-01,\n",
       "             1.2432e-01, -3.3571e-01],\n",
       "           [ 1.0648e+00,  2.2546e-01,  8.0950e-01,  ...,  2.5982e-01,\n",
       "             4.9827e-01, -4.9153e-01],\n",
       "           ...,\n",
       "           [-6.2389e-01,  4.9978e-01, -3.2444e-01,  ...,  3.5221e-01,\n",
       "             3.1330e-01, -6.9899e-01],\n",
       "           [-8.1816e-01,  7.7346e-02, -4.9633e-02,  ...,  1.9933e-01,\n",
       "            -4.9953e-02, -1.2354e-01],\n",
       "           [ 1.4587e-01, -8.2669e-01, -3.7645e-01,  ..., -1.2426e-01,\n",
       "             5.5164e-01,  8.5013e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.1735e-01,  1.7509e-02,  9.4432e-02,  ..., -5.7299e-02,\n",
       "            -3.3267e-01,  2.8139e-01],\n",
       "           [ 1.2601e-01, -6.4267e-01,  3.5154e-01,  ..., -1.3543e-01,\n",
       "            -1.8057e-01,  3.8362e-01],\n",
       "           [-1.5895e-01, -6.7679e-01, -3.6505e-01,  ...,  2.1487e-01,\n",
       "            -1.5817e-01,  2.7596e-01],\n",
       "           ...,\n",
       "           [-2.5344e-01, -1.0044e-03, -5.1537e-01,  ..., -3.6223e-02,\n",
       "            -4.7121e-01, -2.5851e-01],\n",
       "           [-7.9609e-01, -3.5271e-01, -1.0796e-01,  ...,  7.3170e-02,\n",
       "            -7.5880e-02,  4.5907e-02],\n",
       "           [-1.9335e-02, -4.9206e-02,  2.4052e-01,  ...,  1.6833e-01,\n",
       "             3.3974e-02,  1.5597e-01]],\n",
       " \n",
       "          [[ 1.6798e-01, -2.9743e-01, -4.2044e-01,  ...,  1.8004e-01,\n",
       "             4.2098e-02,  8.9604e-02],\n",
       "           [ 6.0013e-03,  2.6185e-01, -4.5219e-01,  ...,  5.4935e-01,\n",
       "             5.7505e-03, -2.5426e-01],\n",
       "           [ 2.3031e-01,  5.3404e-03, -3.5536e-01,  ...,  8.0196e-02,\n",
       "             6.0095e-02,  1.8107e-01],\n",
       "           ...,\n",
       "           [ 5.8480e-02,  3.2503e-01,  5.8320e-01,  ..., -3.3502e-01,\n",
       "             2.1659e-01,  5.0768e-02],\n",
       "           [ 1.7979e-01,  5.0670e-01,  9.8633e-02,  ..., -2.4780e-01,\n",
       "             2.6627e-01,  2.0831e-01],\n",
       "           [ 1.1380e-02,  5.4805e-01,  1.6144e-01,  ...,  6.6539e-02,\n",
       "             5.3516e-03,  2.4340e-01]],\n",
       " \n",
       "          [[ 5.4680e-01,  4.3935e-01,  6.5119e-02,  ..., -1.1736e-01,\n",
       "            -9.7049e-02,  5.6202e-01],\n",
       "           [ 1.3338e-01,  1.8838e-01, -7.5209e-02,  ...,  1.2116e-01,\n",
       "            -1.0390e-02, -1.3685e-01],\n",
       "           [-2.2307e-02,  4.7816e-02,  1.4614e-01,  ..., -2.7525e-01,\n",
       "            -7.0918e-02, -1.0635e-01],\n",
       "           ...,\n",
       "           [ 1.5097e-01, -9.4696e-02, -3.4273e-01,  ..., -7.6547e-02,\n",
       "            -2.0672e-01, -8.7028e-02],\n",
       "           [ 2.0524e-02, -2.6455e-02, -3.7700e-01,  ..., -9.6841e-02,\n",
       "            -1.3779e-01, -2.5866e-01],\n",
       "           [-2.9104e-01,  2.1284e-01, -2.4357e-01,  ..., -1.3337e-01,\n",
       "            -2.5332e-01, -4.0273e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 1.3755e-01, -3.9396e-01, -4.3091e-01,  ..., -3.5522e-01,\n",
       "            -4.1681e-01,  7.3811e-01],\n",
       "           [ 2.9856e-01, -2.3794e-02, -1.8578e-01,  ..., -3.4427e-01,\n",
       "            -3.1957e-01,  4.5638e-01],\n",
       "           [ 9.0586e-01, -4.8659e-02, -9.6561e-04,  ..., -6.6156e-01,\n",
       "            -4.6925e-01,  1.6550e-01],\n",
       "           ...,\n",
       "           [-3.3823e-01, -5.9753e-01, -6.6123e-01,  ...,  2.1476e-01,\n",
       "             9.8954e-01, -2.9892e-01],\n",
       "           [-1.7434e-01, -5.3601e-01, -4.2851e-01,  ..., -2.9534e-01,\n",
       "             3.7026e-01, -2.7754e-01],\n",
       "           [-2.3921e-01,  1.7256e-01, -2.8414e-01,  ..., -4.2897e-02,\n",
       "             3.3865e-01, -7.5771e-02]],\n",
       " \n",
       "          [[ 1.9158e-01,  1.8154e-01, -1.1461e-02,  ...,  3.7883e-01,\n",
       "            -1.2903e-01,  4.1881e-02],\n",
       "           [ 2.6856e-01, -4.1919e-01, -1.7046e-01,  ...,  4.4493e-01,\n",
       "             9.4998e-02,  1.6371e-02],\n",
       "           [ 1.7943e-01, -3.3255e-01, -3.1888e-01,  ...,  1.2205e-01,\n",
       "            -7.6247e-01,  2.9140e-02],\n",
       "           ...,\n",
       "           [-1.5832e-01, -7.0618e-02, -1.9535e-01,  ..., -2.2228e-02,\n",
       "            -1.1020e-01,  1.7594e-01],\n",
       "           [ 9.4821e-02, -3.3694e-01, -3.2773e-01,  ..., -2.4069e-01,\n",
       "            -3.6120e-02,  3.0453e-01],\n",
       "           [-4.1378e-03, -4.6640e-01,  1.4442e-01,  ..., -2.3507e-01,\n",
       "            -4.3218e-01, -2.6161e-01]],\n",
       " \n",
       "          [[-1.0774e-01,  2.4982e-01, -1.5393e-01,  ..., -6.3811e-02,\n",
       "            -2.6005e-01,  6.6368e-02],\n",
       "           [ 2.1302e-01,  1.3529e-01,  1.7999e-01,  ...,  2.3543e-01,\n",
       "            -2.3085e-01, -2.7610e-01],\n",
       "           [ 4.1724e-01,  1.0811e-01, -4.0834e-01,  ...,  1.8765e-01,\n",
       "            -2.6482e-01, -1.5815e-01],\n",
       "           ...,\n",
       "           [ 3.4262e-02,  4.3406e-02,  1.4356e-01,  ..., -1.2858e-01,\n",
       "            -1.3280e-01, -1.2288e-01],\n",
       "           [ 1.1855e-01,  1.8565e-01, -1.3239e-03,  ...,  8.0267e-02,\n",
       "            -3.2121e-02,  6.5723e-02],\n",
       "           [ 2.2438e-01,  1.9101e-01,  2.3946e-02,  ...,  2.2489e-01,\n",
       "            -2.0781e-01, -4.4678e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.3194e-01, -5.3417e-01,  3.7902e-02,  ...,  5.5018e-01,\n",
       "            -6.4793e-01,  3.6128e-01],\n",
       "           [-2.0201e-03, -3.7366e-01, -2.7424e-01,  ...,  3.0975e-01,\n",
       "            -3.3444e-01, -3.8292e-01],\n",
       "           [-1.1032e-02, -4.9023e-01,  6.3381e-01,  ...,  4.6512e-01,\n",
       "            -7.7925e-01, -1.9044e-01],\n",
       "           ...,\n",
       "           [ 2.5793e-01, -7.8535e-01, -1.6234e-01,  ..., -3.4429e-01,\n",
       "            -2.4418e-01,  1.5418e-01],\n",
       "           [ 3.4415e-01, -4.6545e-01, -4.9158e-02,  ...,  2.4240e-01,\n",
       "            -4.3245e-02,  3.1025e-02],\n",
       "           [ 4.7489e-01, -4.0482e-01,  2.8843e-02,  ..., -8.3818e-03,\n",
       "            -4.3404e-03, -2.9208e-02]],\n",
       " \n",
       "          [[ 3.9604e-01,  1.0803e-01, -1.2018e-01,  ..., -2.2058e-01,\n",
       "            -4.1471e-01,  2.0503e-02],\n",
       "           [-1.8329e-01, -4.1930e-02,  3.2930e-01,  ...,  5.2028e-02,\n",
       "            -5.6355e-01, -2.6825e-03],\n",
       "           [-7.7544e-02, -6.2778e-01,  1.1230e-01,  ...,  3.0025e-01,\n",
       "            -1.4679e-02,  1.6087e-01],\n",
       "           ...,\n",
       "           [-2.1001e-01,  4.4618e-01, -7.0941e-01,  ...,  2.3709e-01,\n",
       "            -4.3575e-01, -1.9089e-01],\n",
       "           [-4.5859e-01,  1.2251e-01, -3.9462e-01,  ..., -1.0834e-01,\n",
       "            -2.2608e-01,  5.3128e-02],\n",
       "           [-1.5993e-01,  1.7787e-01, -8.9866e-02,  ...,  2.2377e-01,\n",
       "             1.8875e-02,  8.7214e-02]],\n",
       " \n",
       "          [[ 4.0570e-01,  1.4804e-01,  1.9422e-02,  ..., -4.8927e-01,\n",
       "            -7.6876e-03,  1.2250e-01],\n",
       "           [ 1.5105e-01, -3.0096e-03, -1.7840e-01,  ...,  1.3130e-01,\n",
       "            -1.9319e-01, -2.8407e-01],\n",
       "           [-3.0683e-02, -2.7703e-01,  2.7726e-01,  ...,  9.2084e-02,\n",
       "            -5.3936e-01, -5.0926e-01],\n",
       "           ...,\n",
       "           [ 9.5853e-02,  1.3447e-01,  2.7715e-01,  ..., -7.6524e-02,\n",
       "             2.4601e-01,  1.0214e-01],\n",
       "           [-7.3806e-02,  1.8643e-01,  1.5168e-01,  ..., -1.4757e-01,\n",
       "            -7.3261e-02,  1.8402e-01],\n",
       "           [ 2.7045e-01, -2.3614e-01,  3.4845e-02,  ...,  3.8603e-01,\n",
       "            -2.3739e-01, -1.4141e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 4.3449e-01, -1.5099e-01,  1.3802e-01,  ..., -8.0853e-01,\n",
       "             4.5345e-01, -1.3317e-01],\n",
       "           [-3.6260e-01, -1.6082e-01,  2.2741e-01,  ..., -2.0118e-01,\n",
       "             4.1667e-01,  2.7582e-01],\n",
       "           [-1.3140e-01, -3.3883e-01, -1.9188e-02,  ..., -8.7212e-01,\n",
       "             3.8618e-01,  6.0440e-01],\n",
       "           ...,\n",
       "           [ 6.3565e-01,  3.0221e-01,  3.1416e-01,  ..., -4.7870e-01,\n",
       "             4.1481e-01, -4.4888e-01],\n",
       "           [ 2.0518e-01,  6.4684e-02,  5.7678e-01,  ..., -3.8065e-01,\n",
       "             9.3653e-01,  1.3796e-01],\n",
       "           [ 3.9776e-02,  1.7278e-01,  5.7947e-02,  ...,  5.2036e-02,\n",
       "             7.8528e-01,  9.4667e-03]],\n",
       " \n",
       "          [[-9.5697e-01,  1.7781e-02,  1.1407e-01,  ..., -5.4034e-02,\n",
       "            -6.1907e-01, -3.8920e-01],\n",
       "           [-1.7876e-01,  2.1096e-01, -1.6296e-01,  ...,  6.8814e-01,\n",
       "             6.7921e-02, -3.5550e-01],\n",
       "           [-2.5559e-01,  3.5985e-01,  2.4166e-01,  ..., -7.8746e-01,\n",
       "             1.1368e-01, -2.3086e-01],\n",
       "           ...,\n",
       "           [-5.3840e-01, -4.3528e-02,  4.9825e-01,  ..., -9.0551e-01,\n",
       "             5.2564e-02, -2.1672e-01],\n",
       "           [-2.2129e-01, -1.4121e-01,  1.6348e-01,  ..., -6.3080e-01,\n",
       "            -3.2844e-01, -5.4209e-01],\n",
       "           [-3.1043e-02,  1.7302e-02,  2.4327e-01,  ..., -1.0569e+00,\n",
       "             2.9192e-01, -1.1857e+00]],\n",
       " \n",
       "          [[-7.9446e-02, -1.6444e-01, -1.2562e-01,  ...,  1.4049e-01,\n",
       "            -3.4490e-02, -6.7887e-01],\n",
       "           [ 8.3015e-01, -1.3249e-01, -3.0037e-01,  ...,  2.8814e-01,\n",
       "            -1.2915e-01, -5.0208e-01],\n",
       "           [ 1.4027e-01, -1.3949e-01, -4.2007e-01,  ..., -1.9260e-02,\n",
       "             6.5957e-02, -6.6856e-01],\n",
       "           ...,\n",
       "           [ 1.3793e-01, -1.7687e-01,  6.8334e-02,  ..., -1.3272e-01,\n",
       "             3.8337e-01, -2.4178e-01],\n",
       "           [ 2.3754e-01, -5.8108e-02, -1.1399e-02,  ..., -1.6373e-02,\n",
       "             3.3322e-01,  1.2554e-01],\n",
       "           [ 1.3929e-01, -2.6788e-01, -3.0633e-01,  ..., -4.7656e-01,\n",
       "             1.1772e-01, -1.6205e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.7782e-02, -1.0421e-01,  1.2461e-01,  ...,  2.5270e-01,\n",
       "             3.5142e-02,  8.6956e-02],\n",
       "           [-1.7550e-01, -7.5015e-02,  3.7960e-01,  ...,  3.3750e-02,\n",
       "            -1.0032e-04, -1.1140e-01],\n",
       "           [ 1.5700e-01, -1.6748e-01,  1.6540e-01,  ...,  5.0593e-02,\n",
       "            -1.7258e-01,  7.2280e-03],\n",
       "           ...,\n",
       "           [ 1.5189e-02,  3.1768e-01,  1.0834e-01,  ...,  8.0760e-02,\n",
       "            -2.8970e-01,  2.0972e-01],\n",
       "           [ 4.0219e-01,  7.1005e-02,  1.6942e-02,  ...,  2.1953e-01,\n",
       "            -5.3150e-01,  3.4063e-01],\n",
       "           [ 2.2652e-01,  4.0714e-02, -1.9317e-01,  ...,  6.6311e-01,\n",
       "            -2.3604e-02, -1.5552e-01]],\n",
       " \n",
       "          [[ 9.7819e-01, -2.9695e-01, -4.6280e-01,  ..., -6.6909e-01,\n",
       "            -3.2181e-01, -6.9684e-01],\n",
       "           [ 9.2437e-02,  3.3477e-01, -5.8598e-01,  ..., -3.4210e-01,\n",
       "            -4.0418e-01, -4.7274e-01],\n",
       "           [ 1.4289e-01, -1.4046e-01, -3.7374e-01,  ..., -7.6166e-01,\n",
       "            -1.7458e-01, -3.4249e-01],\n",
       "           ...,\n",
       "           [-6.3156e-01,  3.3580e-01, -2.5178e-01,  ...,  4.5003e-02,\n",
       "            -1.6442e-02, -2.6994e-01],\n",
       "           [-3.3321e-01, -3.2330e-01, -2.5507e-01,  ..., -1.0912e-01,\n",
       "             7.0814e-01, -6.1940e-01],\n",
       "           [-4.1971e-01, -6.7986e-01, -2.5738e-01,  ...,  3.7347e-01,\n",
       "             2.9046e-01, -1.0357e+00]],\n",
       " \n",
       "          [[ 4.0000e-01, -9.4877e-01,  1.5569e-02,  ...,  2.6297e-01,\n",
       "            -6.8513e-02, -4.2844e-01],\n",
       "           [ 6.0132e-02, -4.9800e-01,  1.3847e-01,  ..., -2.7624e-01,\n",
       "             2.3292e-02, -3.9622e-01],\n",
       "           [ 3.0546e-01, -3.3771e-01, -1.3524e-01,  ..., -3.6002e-01,\n",
       "            -6.5651e-02, -8.1839e-01],\n",
       "           ...,\n",
       "           [-5.3743e-02, -1.0258e+00, -3.0548e-01,  ...,  8.9411e-01,\n",
       "             1.4757e-01, -9.0738e-02],\n",
       "           [ 2.1897e-01,  3.9918e-01,  3.1945e-01,  ...,  6.7341e-01,\n",
       "            -1.0741e-01, -2.9349e-01],\n",
       "           [ 6.5458e-02, -3.8630e-01, -1.6684e-01,  ...,  2.4476e-01,\n",
       "             7.7737e-02, -9.3022e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 0.5088, -0.0958, -0.1217,  ..., -0.4016, -0.1562,  0.2764],\n",
       "           [ 0.0746,  0.2906, -0.1428,  ..., -0.2927, -0.0651, -0.4801],\n",
       "           [-0.1548, -0.1461, -0.2570,  ..., -0.1522,  0.0325,  0.1468],\n",
       "           ...,\n",
       "           [ 0.1265, -0.1557,  0.1101,  ..., -0.1872, -0.2462,  0.0274],\n",
       "           [ 0.0505,  0.2736, -0.0026,  ..., -0.7305, -0.1934, -0.0151],\n",
       "           [ 0.2074,  0.0039, -0.0937,  ..., -0.2546,  0.0393, -0.1198]],\n",
       " \n",
       "          [[ 0.3454,  0.1071,  0.6615,  ...,  0.0627, -0.4614,  0.2385],\n",
       "           [ 0.6196, -0.1311,  0.8200,  ..., -0.5849, -0.9809,  0.7672],\n",
       "           [ 0.1347,  0.4961,  0.5928,  ..., -0.1179, -0.4923,  0.0329],\n",
       "           ...,\n",
       "           [-0.0365,  0.0610, -0.1104,  ..., -0.1296,  0.3315,  0.5401],\n",
       "           [ 0.1122,  0.0399, -0.0618,  ..., -0.4813,  0.0777, -0.0190],\n",
       "           [-0.4038,  0.2124, -0.3299,  ..., -0.3034,  0.2327,  0.2749]],\n",
       " \n",
       "          [[ 0.7743,  0.3044,  0.0904,  ...,  0.0101,  0.2010,  0.0637],\n",
       "           [-0.2125,  0.0276, -0.2323,  ..., -0.5023, -0.0863, -0.0373],\n",
       "           [ 0.1293,  0.5092, -0.0418,  ..., -0.0506,  0.0290,  0.2289],\n",
       "           ...,\n",
       "           [ 0.1729,  0.2107,  0.1340,  ..., -0.1690, -0.9038, -0.3902],\n",
       "           [-0.0999,  0.2037, -0.1228,  ...,  0.2669, -0.0508, -0.1962],\n",
       "           [-1.1805, -0.0189,  0.0655,  ...,  0.2832, -0.5866, -0.1565]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.3887, -0.8086, -0.0990,  ...,  0.6522,  0.0455, -0.1970],\n",
       "           [ 0.3585, -0.0089, -0.3308,  ..., -0.8175, -0.0134, -0.2490],\n",
       "           [-0.7853, -0.2867, -0.3349,  ..., -0.2788, -0.3045, -0.5691],\n",
       "           ...,\n",
       "           [-0.0780,  0.0913,  0.8136,  ...,  0.5893,  0.4440,  0.2776],\n",
       "           [-0.0722,  0.3238,  0.4997,  ...,  0.5046,  0.1837, -0.0167],\n",
       "           [-0.2133,  0.3122,  0.5597,  ...,  0.6573,  0.2480,  0.2436]],\n",
       " \n",
       "          [[-0.3320,  0.2796, -0.4782,  ..., -0.7807,  0.1346, -0.6867],\n",
       "           [-0.4543,  0.2392, -0.2282,  ..., -0.2981,  0.3631, -0.3204],\n",
       "           [ 0.1828,  0.4548, -0.3808,  ..., -0.1237,  0.2769,  0.2340],\n",
       "           ...,\n",
       "           [-0.1682,  0.1036, -0.1656,  ...,  0.7523, -0.4532, -0.1923],\n",
       "           [ 0.2604,  0.2515, -0.3499,  ...,  0.4920,  0.0036, -0.3654],\n",
       "           [-0.4336,  0.4354,  0.0168,  ...,  0.1882,  0.1293, -0.1583]],\n",
       " \n",
       "          [[-0.0784, -0.1344, -0.0773,  ..., -0.5776,  0.1607, -0.0131],\n",
       "           [-0.0808,  0.0490, -0.3709,  ..., -0.3644,  0.4629,  0.0637],\n",
       "           [ 0.1840, -0.1378, -0.2050,  ..., -0.0267,  0.4064,  0.0687],\n",
       "           ...,\n",
       "           [-0.0686,  0.0090, -0.0268,  ...,  0.0158, -0.0928,  0.2419],\n",
       "           [-0.1212,  0.1952,  0.0764,  ..., -0.0965, -0.2032,  0.4843],\n",
       "           [ 0.0591,  0.2048,  0.3932,  ..., -0.3087, -0.2915,  0.3581]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 8.1681e-02, -6.4760e-01,  7.1604e-02,  ..., -2.0833e-01,\n",
       "            -3.5735e-01,  8.6533e-02],\n",
       "           [ 1.2279e-01, -1.1603e-01,  2.9707e-01,  ..., -7.3034e-01,\n",
       "             5.4045e-01,  1.4898e-01],\n",
       "           [ 2.4568e-01, -7.6104e-01,  1.5784e-01,  ...,  3.1366e-01,\n",
       "            -1.8959e-03, -8.3513e-02],\n",
       "           ...,\n",
       "           [-1.1000e-01,  1.6075e-01,  7.5646e-02,  ...,  1.1260e+00,\n",
       "            -2.3374e-01, -6.0709e-02],\n",
       "           [ 1.1635e-01, -4.4763e-01, -1.4710e-02,  ...,  5.4452e-01,\n",
       "            -1.1773e-01, -1.9652e-02],\n",
       "           [ 6.7817e-02,  1.1827e-02, -2.6252e-01,  ..., -4.4554e-01,\n",
       "             1.7348e-01,  1.3762e-01]],\n",
       " \n",
       "          [[-1.1195e+00, -1.2980e+00, -8.4534e-01,  ...,  3.3568e-01,\n",
       "            -5.4582e-01,  7.9942e-01],\n",
       "           [ 2.5341e-01, -1.7436e-01, -1.4570e-01,  ..., -2.3455e-01,\n",
       "             1.2239e-01,  2.0476e-01],\n",
       "           [ 1.9382e+00,  2.5383e-01,  1.5174e+00,  ...,  1.2813e+00,\n",
       "             1.2225e-01,  3.6761e-01],\n",
       "           ...,\n",
       "           [ 8.7499e-01,  4.9108e-01,  6.1637e-01,  ...,  3.3533e-02,\n",
       "            -4.4616e-02, -4.4587e-01],\n",
       "           [ 1.6955e+00, -3.1561e-01,  2.6134e-01,  ..., -2.8396e-01,\n",
       "             5.1560e-01, -1.2091e-01],\n",
       "           [ 6.4879e-01, -8.0481e-01,  9.3418e-01,  ..., -6.9146e-01,\n",
       "            -2.2941e-01, -7.7162e-01]],\n",
       " \n",
       "          [[ 4.3486e-01, -2.1493e-01, -2.6418e-01,  ...,  3.1570e-01,\n",
       "            -2.1734e-01, -6.9810e-01],\n",
       "           [ 3.4056e-01, -2.2828e-01,  1.1816e-02,  ..., -3.2133e-01,\n",
       "            -3.3940e-01, -4.5122e-02],\n",
       "           [ 2.6281e-01,  2.1178e-01, -2.7990e-01,  ...,  3.4235e-01,\n",
       "            -5.4500e-01, -1.4694e-01],\n",
       "           ...,\n",
       "           [-1.3077e-01, -2.7815e-01,  3.1025e-01,  ...,  2.3174e-01,\n",
       "             4.8543e-02, -4.7635e-02],\n",
       "           [ 1.8636e-01, -3.5199e-01,  2.2498e-01,  ...,  3.1782e-01,\n",
       "            -3.5800e-01,  1.3588e-01],\n",
       "           [ 9.5898e-02,  2.5339e-01, -3.4681e-01,  ...,  1.6773e-01,\n",
       "             5.1817e-01,  5.6140e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.0039e-01,  2.7636e-01, -2.3911e-02,  ...,  4.5443e-02,\n",
       "             3.2130e-01, -3.4653e-01],\n",
       "           [ 1.1529e+00,  6.8351e-01, -7.2446e-01,  ...,  4.2729e-01,\n",
       "             4.1849e-01,  8.3889e-01],\n",
       "           [ 3.7004e-01,  2.7137e-01, -7.7489e-02,  ...,  3.7150e-01,\n",
       "             5.5438e-01,  1.0744e-01],\n",
       "           ...,\n",
       "           [ 5.1011e-02,  2.1976e-01,  1.3602e-01,  ...,  4.9056e-02,\n",
       "            -3.4502e-01,  2.1311e-01],\n",
       "           [-4.7352e-01,  3.5742e-01, -2.4971e-02,  ...,  1.5597e-01,\n",
       "            -3.1323e-01, -1.5305e-01],\n",
       "           [-7.5568e-01, -4.9536e-01, -4.4531e-01,  ...,  2.6543e-01,\n",
       "            -1.6888e-01, -1.5165e-01]],\n",
       " \n",
       "          [[-7.4473e-01,  3.9177e-01,  3.3274e-01,  ...,  4.8470e-01,\n",
       "            -6.0989e-01,  4.3858e-01],\n",
       "           [-5.5689e-01,  1.0869e+00,  6.9071e-01,  ...,  2.4768e-01,\n",
       "            -4.3822e-01,  5.8791e-02],\n",
       "           [-3.9252e-01,  7.6695e-01,  1.3455e+00,  ...,  5.6527e-01,\n",
       "            -1.0581e-01,  9.7493e-01],\n",
       "           ...,\n",
       "           [-7.0876e-02, -4.0787e-01,  3.7534e-01,  ...,  2.3417e-01,\n",
       "             6.5354e-01,  8.8915e-02],\n",
       "           [-2.0774e-01, -9.9736e-01,  2.0599e-01,  ...,  2.6787e-01,\n",
       "             5.6120e-02,  6.2328e-01],\n",
       "           [-4.1736e-01, -6.7723e-01,  3.5861e-01,  ...,  1.7267e-01,\n",
       "            -3.0587e-01, -1.2383e-02]],\n",
       " \n",
       "          [[-5.4971e-01,  4.9274e-01,  1.7888e-01,  ...,  2.8986e-01,\n",
       "             9.2192e-02, -3.9473e-01],\n",
       "           [ 4.8419e-02, -1.0997e-01, -1.3640e-01,  ...,  1.5640e+00,\n",
       "             1.5919e-02,  3.9910e-01],\n",
       "           [ 1.8250e-01,  1.2816e-01,  4.5532e-01,  ...,  1.7791e-01,\n",
       "             4.4165e-01, -1.8455e-01],\n",
       "           ...,\n",
       "           [-5.6704e-01,  4.0013e-01, -2.2386e-01,  ...,  1.2839e-01,\n",
       "             3.3836e-02, -1.8036e-01],\n",
       "           [-5.4814e-01,  6.5765e-01,  9.5981e-02,  ..., -3.2789e-01,\n",
       "             5.4252e-01, -1.0591e-01],\n",
       "           [-8.6737e-02,  2.4794e-01,  6.5003e-01,  ...,  1.1316e-01,\n",
       "             3.9964e-01,  6.1285e-02]]]], device='cuda:0'),\n",
       " tensor([[[[-7.8635e-01, -8.4927e-01,  9.0336e-02,  ...,  2.8521e-02,\n",
       "             2.0878e-01,  3.9486e-01],\n",
       "           [-6.1741e-01, -2.7699e-01, -5.7435e-01,  ...,  6.9879e-01,\n",
       "             1.1618e+00, -1.4554e-01],\n",
       "           [-5.1061e-01,  4.0546e-01, -8.5333e-01,  ..., -2.4409e-01,\n",
       "            -7.2133e-02, -5.4321e-01],\n",
       "           ...,\n",
       "           [-4.0264e-01, -6.6352e-01, -5.0955e-01,  ..., -6.8143e-01,\n",
       "             8.5411e-01, -3.2272e-01],\n",
       "           [-1.4414e+00, -5.7798e-01,  1.1670e-01,  ..., -7.2327e-01,\n",
       "             2.2550e-01, -3.3472e-01],\n",
       "           [-1.2167e+00, -9.9110e-01, -2.2795e-01,  ..., -1.6423e-01,\n",
       "            -2.8228e-01, -2.5374e-01]],\n",
       " \n",
       "          [[-1.4984e-01, -7.8312e-01,  3.0739e-01,  ...,  2.7479e-01,\n",
       "            -6.5244e-01,  9.5417e-01],\n",
       "           [-2.2790e-01, -9.6728e-01,  3.2710e-01,  ...,  5.5433e-01,\n",
       "            -4.7463e-01,  9.5429e-01],\n",
       "           [-8.0291e-01, -3.3369e-01,  3.8806e-01,  ..., -5.9994e-03,\n",
       "            -5.4761e-02,  8.4788e-01],\n",
       "           ...,\n",
       "           [-2.3650e-01,  2.8237e-01,  3.4930e-01,  ..., -2.2734e-01,\n",
       "             1.5031e-01,  2.3198e-01],\n",
       "           [-2.3721e-01, -1.0501e-02,  3.2470e-01,  ..., -3.3791e-01,\n",
       "             3.3409e-01,  1.7316e-01],\n",
       "           [-9.7056e-02, -3.0388e-01,  5.8546e-01,  ..., -1.5777e-01,\n",
       "            -4.1851e-02,  6.6523e-01]],\n",
       " \n",
       "          [[-6.5588e-01, -4.6756e-01, -2.2574e-01,  ...,  1.5423e-01,\n",
       "             1.3482e-01,  1.7387e-01],\n",
       "           [-2.4266e-01, -6.7429e-01,  1.8074e-01,  ..., -5.1088e-01,\n",
       "            -3.4044e-01,  3.7011e-01],\n",
       "           [-1.2648e-02, -1.5069e-01, -5.9121e-02,  ..., -5.9676e-02,\n",
       "            -2.3066e-01, -1.2846e-01],\n",
       "           ...,\n",
       "           [ 1.4800e-01, -4.3911e-02, -6.4025e-01,  ...,  1.8614e-01,\n",
       "            -4.4147e-02,  2.5693e-01],\n",
       "           [-8.3330e-02, -1.1135e-02, -1.5335e-03,  ..., -1.2065e-01,\n",
       "             5.8313e-03,  6.6805e-02],\n",
       "           [ 7.5333e-03, -3.4878e-01, -2.5226e-01,  ...,  3.0751e-01,\n",
       "             2.1010e-01, -1.7851e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.6973e-01,  5.1835e-01,  8.8470e-02,  ...,  2.9983e-01,\n",
       "            -2.2085e-01,  2.2346e-01],\n",
       "           [ 2.0769e-01,  3.3966e-01, -6.2015e-01,  ...,  4.5341e-01,\n",
       "            -3.6956e-01,  3.8837e-01],\n",
       "           [-1.0263e-01,  4.4953e-01,  7.8832e-02,  ...,  6.9948e-02,\n",
       "            -5.8043e-01,  6.6624e-01],\n",
       "           ...,\n",
       "           [ 4.3867e-01,  3.4072e-01, -1.0057e-02,  ...,  7.1347e-01,\n",
       "             1.9872e-01,  8.3689e-01],\n",
       "           [ 4.8315e-01, -3.0531e-01, -9.7630e-02,  ...,  2.1990e-01,\n",
       "             1.1246e-01,  6.3401e-01],\n",
       "           [ 2.8978e-01, -1.5703e-01,  8.5464e-01,  ...,  2.6873e-02,\n",
       "            -6.5670e-02,  2.5569e-01]],\n",
       " \n",
       "          [[ 2.2579e-01,  6.4147e-02,  1.9020e-01,  ..., -1.3061e-01,\n",
       "             3.3143e-01, -3.0294e-01],\n",
       "           [ 4.1756e-01,  3.3270e-01, -1.6782e-01,  ..., -4.1735e-01,\n",
       "             1.6519e+00,  1.3086e-01],\n",
       "           [ 1.0388e-01, -3.3304e-01,  1.4249e-01,  ...,  2.1392e-01,\n",
       "             8.5039e-01,  7.1303e-01],\n",
       "           ...,\n",
       "           [-5.9588e-01,  1.6073e-01, -6.9457e-04,  ..., -1.7211e-01,\n",
       "            -3.9220e-02, -1.7594e-01],\n",
       "           [-5.2211e-02, -8.7955e-02, -6.7224e-02,  ..., -1.3502e-01,\n",
       "            -1.0612e-01, -1.2669e-02],\n",
       "           [-2.2948e-01, -1.0797e-01,  5.5353e-02,  ..., -2.1508e-01,\n",
       "            -2.4219e-02, -1.9765e-01]],\n",
       " \n",
       "          [[-5.1965e-03,  1.2804e-01, -2.0370e-01,  ..., -3.3374e-01,\n",
       "            -4.9310e-01,  2.1234e-01],\n",
       "           [ 9.0952e-01,  1.0981e-01, -2.9245e-01,  ...,  6.8400e-01,\n",
       "            -2.0442e-01, -4.2543e-02],\n",
       "           [ 4.9945e-01,  5.0343e-01, -5.1838e-02,  ..., -9.6399e-02,\n",
       "            -3.4939e-01, -3.9336e-04],\n",
       "           ...,\n",
       "           [-3.5943e-01,  5.5769e-01,  6.4332e-01,  ...,  1.9581e-01,\n",
       "            -9.7265e-02, -2.3474e-01],\n",
       "           [-8.7896e-02,  2.7685e-01,  1.1520e-01,  ...,  5.7693e-01,\n",
       "            -3.2119e-01, -3.2988e-01],\n",
       "           [ 5.9566e-02, -3.1393e-01,  2.7903e-01,  ...,  3.2915e-01,\n",
       "            -3.4575e-01,  1.6573e-01]]]], device='cuda:0'),\n",
       " tensor([[[[-0.3006,  0.5576,  0.4765,  ...,  0.2177,  0.6052,  0.4199],\n",
       "           [-0.0262,  0.0085,  0.8016,  ..., -0.2978, -0.0416, -0.0784],\n",
       "           [-0.4394,  0.7351,  0.1631,  ...,  0.0484,  0.0928, -0.0858],\n",
       "           ...,\n",
       "           [ 0.3482, -0.0238,  0.6476,  ...,  0.2494,  0.7453, -0.8530],\n",
       "           [ 0.3102,  0.0188,  0.2569,  ...,  0.8240, -0.2272, -0.3945],\n",
       "           [ 0.2901,  0.6466,  0.1408,  ...,  0.2245, -0.6183,  0.0032]],\n",
       " \n",
       "          [[-0.0683, -0.1779,  0.3172,  ...,  0.1046, -0.2341, -0.5669],\n",
       "           [-0.5735, -0.2222,  0.4672,  ..., -0.5999,  0.6247,  0.2447],\n",
       "           [-0.3165, -0.1925,  0.3245,  ..., -0.8800,  0.5267, -0.5968],\n",
       "           ...,\n",
       "           [ 0.9162, -0.8339,  0.4692,  ..., -0.2104, -0.0248, -1.0918],\n",
       "           [ 0.9961, -0.3111,  0.8519,  ...,  0.8165, -0.0305, -0.3867],\n",
       "           [ 0.3818, -0.4116, -0.0918,  ...,  0.0680,  0.4377, -0.1714]],\n",
       " \n",
       "          [[ 0.7540,  0.3095, -0.2053,  ..., -0.6098,  0.3349, -0.1555],\n",
       "           [ 0.3616,  0.0359, -0.3819,  ...,  0.1017, -0.3723, -0.4427],\n",
       "           [-0.4536, -0.8786, -0.0656,  ...,  0.0817, -0.4543, -0.5647],\n",
       "           ...,\n",
       "           [-0.0609, -0.0233,  0.4025,  ..., -0.2871, -0.5297,  0.3718],\n",
       "           [-0.4972,  0.2884,  0.4103,  ..., -0.8196,  0.1186,  1.2576],\n",
       "           [-0.4747, -0.8799, -0.6152,  ...,  0.4471, -0.0995,  0.8163]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0375, -0.7242, -0.1688,  ..., -0.4156,  0.2970,  0.2999],\n",
       "           [-0.1269, -1.1092,  0.2687,  ..., -0.1326, -0.1749,  0.1205],\n",
       "           [ 0.3458, -1.0881,  0.2917,  ...,  0.3898, -0.8645,  0.6133],\n",
       "           ...,\n",
       "           [ 0.6424,  0.2758, -0.1523,  ..., -0.2822,  0.4693,  0.4463],\n",
       "           [ 0.0885, -0.3707, -0.2980,  ..., -0.0991,  0.2517, -0.2104],\n",
       "           [-0.0251,  0.0098, -0.2744,  ..., -0.4436, -0.1398,  0.1591]],\n",
       " \n",
       "          [[-0.0952, -0.1105, -0.0750,  ..., -0.2273, -0.0296, -0.1627],\n",
       "           [-0.9133, -0.5115,  0.3089,  ...,  0.7036,  0.7559,  0.2061],\n",
       "           [-0.4558, -0.7122,  0.0861,  ...,  0.1744, -0.1583,  0.1329],\n",
       "           ...,\n",
       "           [-0.1334,  0.1729,  0.3825,  ...,  0.1842,  0.0992,  0.2409],\n",
       "           [ 1.0089,  0.3499,  0.0348,  ..., -0.9363,  0.0492, -0.2329],\n",
       "           [ 1.5541,  1.0241,  0.0293,  ...,  0.5753, -0.0091,  0.2508]],\n",
       " \n",
       "          [[ 0.3188, -0.0610, -0.2361,  ...,  0.1782, -0.2822,  0.3114],\n",
       "           [-0.3258, -0.3804, -0.0573,  ...,  0.3933, -0.0225, -0.3043],\n",
       "           [-0.1124, -0.2109, -0.0313,  ..., -0.0016, -0.0108, -0.3402],\n",
       "           ...,\n",
       "           [-0.1697, -0.4577,  0.0223,  ..., -0.0944, -0.1005, -0.6269],\n",
       "           [-0.1707, -0.4396,  0.0382,  ...,  0.6299, -0.2980, -0.6238],\n",
       "           [-0.3017, -0.3267,  0.0473,  ...,  0.5413,  0.4169,  0.3969]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-3.0881e-01, -2.9699e-01, -7.8417e-02,  ...,  3.5386e-01,\n",
       "            -3.2324e-01, -1.7421e-01],\n",
       "           [-1.3766e-01, -2.3722e-01, -3.5054e-01,  ...,  1.9769e-01,\n",
       "            -5.6245e-01, -1.5035e-01],\n",
       "           [-2.4507e-01,  1.9131e-01,  1.8701e-01,  ...,  3.1802e-01,\n",
       "             3.7697e-02, -3.4139e-01],\n",
       "           ...,\n",
       "           [-4.8160e-01, -7.1210e-01,  4.6177e-01,  ...,  9.7810e-02,\n",
       "            -8.0987e-02,  3.3334e-01],\n",
       "           [-3.3210e-01,  1.0014e-04,  2.7263e-01,  ...,  1.7534e-03,\n",
       "            -5.8269e-02,  3.0851e-01],\n",
       "           [-3.6228e-01, -2.3288e-01,  4.6658e-01,  ..., -5.7406e-01,\n",
       "            -5.6280e-01,  3.0550e-01]],\n",
       " \n",
       "          [[-5.1438e-01, -6.1021e-01, -3.6713e-01,  ..., -7.3016e-01,\n",
       "            -1.7415e-01, -3.9580e-01],\n",
       "           [-9.3076e-01,  1.7176e-01,  9.9070e-02,  ..., -3.8598e-01,\n",
       "            -7.8016e-02, -3.0255e-01],\n",
       "           [-5.4407e-01, -9.6995e-01,  2.1705e-01,  ..., -1.3384e-01,\n",
       "            -2.5832e-02, -6.2181e-01],\n",
       "           ...,\n",
       "           [ 2.0496e-01,  3.6339e-02, -3.3373e-01,  ..., -2.5600e-02,\n",
       "             3.7647e-03, -1.1663e-01],\n",
       "           [-1.3143e-01,  2.9449e-01,  2.0451e-02,  ..., -1.2699e-01,\n",
       "            -4.5967e-02,  2.0487e-01],\n",
       "           [ 5.3335e-01,  7.8857e-01,  4.2172e-01,  ..., -1.0500e-01,\n",
       "             8.2514e-01, -1.8847e-02]],\n",
       " \n",
       "          [[-1.9583e-02, -5.4796e-02,  4.9127e-01,  ...,  5.7397e-01,\n",
       "            -3.2551e-02, -8.4472e-01],\n",
       "           [ 4.0453e-01, -3.7459e-01, -1.9076e-01,  ...,  3.6069e-01,\n",
       "            -6.5597e-01, -3.4864e-01],\n",
       "           [-1.8406e-01,  2.0002e-01, -2.4483e-01,  ...,  1.3430e-01,\n",
       "            -5.0497e-01, -3.4913e-01],\n",
       "           ...,\n",
       "           [ 3.9429e-01, -4.7363e-02,  2.0242e-01,  ...,  1.4531e-01,\n",
       "             3.5930e-01,  2.3276e-01],\n",
       "           [ 8.9056e-01,  3.4385e-01,  9.7183e-01,  ...,  4.9522e-01,\n",
       "             3.9262e-01,  3.1858e-01],\n",
       "           [ 1.6624e-01, -1.8872e-01,  3.5110e-01,  ...,  4.8959e-01,\n",
       "             2.7815e-01, -5.2328e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.9506e-01,  5.7479e-01, -4.3179e-01,  ..., -7.6809e-02,\n",
       "             6.1600e-01,  3.6328e-01],\n",
       "           [-3.2174e-01, -5.3386e-02,  2.9566e-02,  ...,  2.5774e-01,\n",
       "             7.3386e-02,  3.9451e-01],\n",
       "           [ 6.4784e-02, -2.4131e-01, -1.0044e+00,  ..., -1.5648e-01,\n",
       "             1.4640e-01,  9.1533e-01],\n",
       "           ...,\n",
       "           [-7.5280e-02,  1.8804e-01, -2.5356e-01,  ...,  6.8562e-02,\n",
       "             4.8014e-01,  3.0226e-01],\n",
       "           [ 6.9241e-01, -3.0504e-01, -2.7469e-01,  ..., -4.6567e-01,\n",
       "             8.4299e-01,  1.0018e+00],\n",
       "           [ 3.3893e-01,  2.0609e-01,  4.8996e-01,  ...,  1.0480e-01,\n",
       "            -9.1536e-02, -3.8405e-02]],\n",
       " \n",
       "          [[ 8.0686e-01,  6.7416e-01, -7.4002e-02,  ..., -1.3672e+00,\n",
       "             9.0286e-02,  2.4083e-01],\n",
       "           [ 4.4448e-01,  5.2288e-01, -7.3937e-02,  ...,  3.8387e-01,\n",
       "             2.7515e-01,  1.5855e-01],\n",
       "           [ 3.4697e-01, -7.0840e-03,  2.2670e-01,  ..., -9.4047e-01,\n",
       "            -1.0945e-01,  3.0156e-01],\n",
       "           ...,\n",
       "           [ 3.9155e-01,  6.1089e-01, -1.5033e-01,  ...,  3.1401e-01,\n",
       "            -1.1886e-01,  7.0772e-01],\n",
       "           [ 2.5232e-01,  3.6159e-01,  3.9093e-02,  ...,  1.3432e-01,\n",
       "            -1.6877e-01,  6.2499e-01],\n",
       "           [ 8.0383e-01,  6.4824e-01,  8.7637e-01,  ...,  3.8858e-01,\n",
       "            -6.1855e-01, -3.6519e-02]],\n",
       " \n",
       "          [[ 3.5918e-01, -2.9720e-01,  4.1964e-01,  ..., -4.0700e-01,\n",
       "             4.3383e-01,  1.4336e+00],\n",
       "           [ 5.0924e-01, -4.9320e-01, -5.6188e-02,  ..., -4.9822e-01,\n",
       "             3.1691e-01,  7.1223e-01],\n",
       "           [ 1.9084e-01, -4.2838e-01,  5.2226e-01,  ...,  1.7605e-01,\n",
       "            -3.6850e-03,  7.9975e-01],\n",
       "           ...,\n",
       "           [-4.2232e-02, -1.1354e-01,  6.2288e-01,  ..., -6.8419e-01,\n",
       "            -3.1303e-01,  7.8161e-01],\n",
       "           [-1.0508e-01, -2.7379e-02,  5.4643e-01,  ..., -2.1952e-01,\n",
       "             1.6620e-01,  5.5618e-01],\n",
       "           [-2.2332e-01, -3.9468e-01,  4.7456e-01,  ...,  7.6779e-02,\n",
       "            -3.4788e-01,  2.3037e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 0.0938, -0.1519, -0.2141,  ..., -0.0111,  0.6203,  0.0402],\n",
       "           [-0.3266, -0.0940,  0.5687,  ..., -0.3606,  0.4120, -0.3709],\n",
       "           [-0.1225,  0.2544, -0.2147,  ..., -0.0476, -0.3284, -0.2518],\n",
       "           ...,\n",
       "           [ 0.0949, -0.1416,  0.5172,  ..., -0.2533, -0.0376,  0.0112],\n",
       "           [-0.1685, -0.1771,  0.6391,  ..., -0.4256, -0.1006, -0.2296],\n",
       "           [-0.5922,  0.2982,  0.7101,  ..., -0.0170,  0.3587,  0.4720]],\n",
       " \n",
       "          [[ 0.5383,  0.2549, -0.1343,  ..., -0.1529, -0.0587,  0.2052],\n",
       "           [-0.2809, -0.4412,  0.4797,  ...,  0.0349, -0.0477,  0.4407],\n",
       "           [ 0.5485,  0.1295, -0.3782,  ...,  0.1061, -0.2888,  0.4896],\n",
       "           ...,\n",
       "           [ 0.6039, -0.2569,  0.3447,  ...,  0.5892,  0.4743,  0.5661],\n",
       "           [ 0.4319, -0.4233,  0.8413,  ..., -0.2436,  0.3037,  0.1787],\n",
       "           [-0.1458, -0.2953,  0.6276,  ..., -0.5187,  0.4033, -0.0353]],\n",
       " \n",
       "          [[ 0.7781,  0.0776, -0.1800,  ...,  0.1562, -0.2657,  0.5381],\n",
       "           [ 0.0450, -0.4384,  0.2286,  ...,  0.0712,  0.2234, -0.2877],\n",
       "           [ 0.2779,  0.0891,  0.3488,  ..., -0.4009, -0.0245, -0.2574],\n",
       "           ...,\n",
       "           [ 0.0583,  0.1490,  0.0977,  ...,  0.2737, -0.2009, -0.3458],\n",
       "           [-0.5150,  0.0567, -0.2650,  ...,  0.4525, -0.2625, -0.1722],\n",
       "           [-0.6661, -0.5054,  0.3864,  ..., -0.1082, -0.1428, -0.8510]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.2089, -0.6883,  0.5069,  ..., -0.2141, -0.3228, -0.1549],\n",
       "           [-0.3552,  0.0460, -0.0272,  ...,  0.5635, -0.1145,  0.0422],\n",
       "           [-0.7577, -0.2976, -0.8961,  ...,  0.3329,  0.2117,  0.5618],\n",
       "           ...,\n",
       "           [-0.3632,  0.5258, -0.0225,  ..., -0.3428,  0.0628, -0.2104],\n",
       "           [-0.7073, -0.8060,  0.2305,  ...,  0.1619,  0.5225,  0.9329],\n",
       "           [-0.7339,  0.0457,  0.0701,  ...,  0.2734, -0.2015,  0.3302]],\n",
       " \n",
       "          [[ 0.0674,  0.0639, -0.6419,  ...,  0.0605,  0.2664,  0.0202],\n",
       "           [-0.1295, -0.1558, -0.0525,  ..., -0.3718, -0.0337, -0.4326],\n",
       "           [ 0.1133, -0.4296, -1.0169,  ..., -0.0641, -0.7345,  0.3042],\n",
       "           ...,\n",
       "           [ 0.3635,  0.2819,  0.3463,  ..., -0.0713, -0.0795, -0.2139],\n",
       "           [-0.1246,  0.3194,  0.5815,  ..., -0.0627,  0.0747,  0.5071],\n",
       "           [ 0.4417,  0.1791,  0.5644,  ..., -0.1388,  0.5585, -0.2043]],\n",
       " \n",
       "          [[-0.2229, -0.4545,  0.5004,  ...,  0.1802, -0.0317,  0.2433],\n",
       "           [-0.2593, -0.4613,  0.5219,  ..., -0.3146, -0.2769,  0.2676],\n",
       "           [ 0.1439, -0.4289,  0.8425,  ..., -0.0642, -0.0178,  0.6824],\n",
       "           ...,\n",
       "           [-0.1954, -0.1071, -0.0074,  ..., -0.3143,  0.0406,  0.3649],\n",
       "           [ 0.0726, -0.3449,  0.2634,  ..., -0.1110, -0.0917,  0.0207],\n",
       "           [ 0.2549, -0.2640,  0.2835,  ..., -0.1875,  0.4191, -0.0875]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 2.6185e-01,  1.5613e-01,  1.2342e-01,  ...,  2.3153e-01,\n",
       "             9.3608e-02, -4.7991e-01],\n",
       "           [-2.6989e-01, -4.2844e-01,  8.2528e-01,  ...,  4.3665e-02,\n",
       "            -8.2563e-01, -7.3370e-02],\n",
       "           [ 2.3968e-01,  1.7508e-01,  1.6577e-01,  ...,  2.3463e-01,\n",
       "             1.6564e-03,  9.1005e-02],\n",
       "           ...,\n",
       "           [-8.3604e-02,  6.7191e-01,  6.8725e-01,  ..., -2.0720e-01,\n",
       "             2.3029e-01,  4.6795e-01],\n",
       "           [ 1.0594e-01,  6.4562e-01, -3.3774e-01,  ..., -1.6912e-01,\n",
       "             5.5666e-01,  2.3372e-01],\n",
       "           [-4.5625e-01,  4.7039e-01, -3.0236e-02,  ..., -2.8386e-01,\n",
       "             1.1013e-01,  3.7260e-01]],\n",
       " \n",
       "          [[ 6.5639e-02,  1.8674e-02, -1.6607e-03,  ...,  4.0659e-03,\n",
       "            -5.6844e-01,  4.4450e-01],\n",
       "           [ 3.5682e-01,  4.9542e-01,  5.1629e-01,  ...,  5.4769e-02,\n",
       "            -2.4368e-01,  1.5823e-01],\n",
       "           [-5.5601e-02, -6.8517e-01,  1.5479e-01,  ...,  1.1582e-01,\n",
       "             9.0444e-02,  1.0048e-01],\n",
       "           ...,\n",
       "           [ 2.6809e-02,  3.6787e-01, -6.9395e-01,  ...,  3.0392e-02,\n",
       "            -6.2539e-02,  7.9915e-01],\n",
       "           [ 3.4231e-01, -2.8946e-01,  2.0495e-01,  ...,  2.6937e-01,\n",
       "            -2.5712e-01, -2.2803e-01],\n",
       "           [-5.1669e-01,  6.0049e-01, -2.9685e-02,  ..., -3.7174e-01,\n",
       "             4.4999e-01, -1.2920e+00]],\n",
       " \n",
       "          [[ 2.3280e-01,  1.1118e+00,  4.0900e-01,  ...,  4.7335e-01,\n",
       "            -6.3748e-01,  2.0785e-01],\n",
       "           [-5.1177e-01,  8.6018e-01, -2.6754e-01,  ...,  4.3569e-01,\n",
       "            -1.6804e-01, -2.8483e-02],\n",
       "           [-2.7915e-01,  1.1355e+00,  6.1731e-02,  ..., -3.9974e-01,\n",
       "            -5.3113e-01, -7.0731e-02],\n",
       "           ...,\n",
       "           [-5.0812e-01,  8.3450e-01,  7.1324e-01,  ..., -2.3315e-01,\n",
       "            -2.0358e-01,  5.2116e-01],\n",
       "           [-4.8067e-01,  8.3909e-02,  6.0926e-01,  ..., -4.3694e-01,\n",
       "             4.5069e-01,  4.9512e-01],\n",
       "           [ 7.5005e-02,  1.3596e+00,  3.5002e-01,  ..., -5.1005e-01,\n",
       "             4.1021e-01, -3.2048e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.2851e-01,  4.3536e-01,  3.4291e-01,  ..., -1.0660e-01,\n",
       "             2.9565e-01,  4.4976e-01],\n",
       "           [-1.3132e-01,  7.9695e-01, -7.7645e-01,  ...,  8.5212e-01,\n",
       "             3.6659e-01,  4.1736e-01],\n",
       "           [ 6.1132e-01,  3.8926e-01, -2.4447e-01,  ..., -1.3029e-01,\n",
       "             2.0999e-01,  4.8822e-01],\n",
       "           ...,\n",
       "           [ 5.6402e-02,  4.0707e-01,  2.0026e-01,  ...,  1.6437e-01,\n",
       "            -1.0805e-01,  1.1961e-01],\n",
       "           [-1.3948e-01,  4.5426e-01,  6.0364e-02,  ...,  4.9650e-01,\n",
       "            -4.3597e-02, -2.5122e-01],\n",
       "           [ 2.3320e-01,  2.4960e-01, -6.8044e-01,  ...,  9.5415e-02,\n",
       "             5.5030e-01,  8.6945e-01]],\n",
       " \n",
       "          [[ 1.6380e-02,  7.9124e-01,  4.2552e-01,  ..., -5.8853e-01,\n",
       "            -8.8928e-01,  2.2303e-01],\n",
       "           [-6.7906e-01,  6.9327e-01,  3.3185e-01,  ...,  4.8234e-01,\n",
       "            -3.6963e-01, -5.3963e-01],\n",
       "           [-3.5722e-01,  1.3034e+00,  5.4440e-01,  ...,  9.8332e-02,\n",
       "            -2.3184e-01,  6.4440e-01],\n",
       "           ...,\n",
       "           [ 6.2183e-01,  1.4289e-01,  1.5053e-01,  ...,  6.3810e-01,\n",
       "            -1.3529e-02,  6.1140e-01],\n",
       "           [-3.3180e-03, -1.3326e+00,  3.9545e-01,  ..., -4.3014e-01,\n",
       "             4.0483e-01,  1.4021e+00],\n",
       "           [ 1.7341e-01,  8.7831e-02, -3.2626e-01,  ...,  9.4593e-01,\n",
       "             7.6335e-01,  1.2799e+00]],\n",
       " \n",
       "          [[-1.9665e-01, -2.0321e-01,  4.3317e-01,  ...,  1.6687e-01,\n",
       "            -3.7671e-01,  1.2725e-01],\n",
       "           [-7.8629e-01, -2.1684e+00,  5.9493e-01,  ...,  3.8748e-01,\n",
       "            -3.0763e-01,  3.6001e-01],\n",
       "           [ 7.8867e-02, -3.6303e-01,  6.5034e-01,  ..., -1.0111e-01,\n",
       "            -1.1786e-02,  6.9722e-01],\n",
       "           ...,\n",
       "           [-3.2828e-01, -1.4464e-01,  4.4048e-01,  ..., -1.1265e-01,\n",
       "            -4.9991e-01, -5.5209e-01],\n",
       "           [-1.2737e-01,  8.1205e-02,  4.7518e-01,  ...,  4.9506e-01,\n",
       "            -4.2813e-01, -1.9696e-01],\n",
       "           [ 1.2543e-01, -1.9485e-02,  3.1641e-01,  ..., -5.9886e-01,\n",
       "            -2.5442e-01, -6.5999e-02]]]], device='cuda:0'),\n",
       " tensor([[[[-0.0982,  0.6299, -0.3042,  ...,  0.8701,  0.3829, -0.3602],\n",
       "           [-0.3966, -0.5129, -0.6457,  ...,  0.0123, -0.5880, -1.4422],\n",
       "           [-0.1557,  0.3864, -0.4886,  ..., -0.4104,  0.1623, -1.0620],\n",
       "           ...,\n",
       "           [ 0.0949, -0.1190, -0.2799,  ...,  0.2251,  0.1880, -0.0490],\n",
       "           [-0.1459, -0.1178, -0.6999,  ..., -0.1445, -0.0962,  0.0120],\n",
       "           [ 0.3197, -0.0308, -0.2201,  ..., -0.0837, -0.4658,  0.4474]],\n",
       " \n",
       "          [[ 0.4704,  1.0122,  0.2078,  ..., -0.0217,  0.6696, -1.7160],\n",
       "           [ 0.4517,  0.2491, -0.7069,  ..., -0.3440,  1.0192, -0.1351],\n",
       "           [ 1.0894,  0.0767, -0.4854,  ..., -0.8071,  0.6501, -0.5853],\n",
       "           ...,\n",
       "           [-0.5722, -0.4526,  0.0389,  ...,  0.5217, -0.0287, -0.7155],\n",
       "           [-0.7032, -0.3550, -0.0462,  ...,  0.5634,  0.2641, -0.7229],\n",
       "           [-0.2323, -0.4503,  0.0630,  ..., -0.5052, -0.0500, -0.7341]],\n",
       " \n",
       "          [[ 0.2778,  0.7277, -0.5682,  ..., -0.4145, -0.2949,  0.1221],\n",
       "           [-0.0392,  0.5761, -0.0034,  ...,  0.0202,  0.4143, -0.1699],\n",
       "           [-0.2384,  0.1211, -0.1030,  ..., -0.0679,  0.2377,  0.2847],\n",
       "           ...,\n",
       "           [-0.1065,  0.3900, -0.9483,  ...,  0.9398,  0.8249, -0.5030],\n",
       "           [-0.2452,  0.1158, -0.0073,  ...,  0.8245,  0.0230, -1.0056],\n",
       "           [-0.3596, -0.4361, -0.4829,  ...,  0.6312,  0.0491, -0.5547]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.2761, -0.9013, -0.9279,  ..., -0.6458, -0.1672, -0.3554],\n",
       "           [-0.0305, -0.3144, -0.7534,  ...,  1.0442, -0.1515, -0.1214],\n",
       "           [-0.3900, -0.1093, -0.1169,  ..., -1.2540,  0.2668, -0.2876],\n",
       "           ...,\n",
       "           [ 0.3760, -0.7198, -0.1429,  ..., -0.0872, -0.4228, -0.2352],\n",
       "           [-0.1506,  0.0372,  0.5780,  ..., -0.7058,  0.2472, -0.3660],\n",
       "           [ 0.0601, -0.4462,  0.0875,  ..., -0.3123, -0.3322,  0.3522]],\n",
       " \n",
       "          [[ 0.2627,  0.3492,  0.5835,  ...,  0.1187, -0.0639, -0.2445],\n",
       "           [-0.0896,  0.5054,  0.0552,  ...,  0.7456,  0.0353,  1.0129],\n",
       "           [-0.2728,  0.3370,  0.1707,  ...,  0.1621, -0.3191,  0.2691],\n",
       "           ...,\n",
       "           [-0.1125, -0.2632, -0.3868,  ...,  0.0733, -0.1972, -0.9404],\n",
       "           [-0.2182, -0.1513, -0.7009,  ...,  0.3535, -0.7058, -0.0892],\n",
       "           [-0.5751, -0.3806, -0.6064,  ...,  0.0532, -0.1768, -0.2751]],\n",
       " \n",
       "          [[-0.6325, -0.5717,  0.1488,  ..., -0.0131,  0.5252, -0.3650],\n",
       "           [ 0.0619, -0.5687, -0.1224,  ..., -0.0669,  0.0417,  0.2949],\n",
       "           [-0.0950, -0.0585, -0.3620,  ..., -0.3781,  0.7936,  0.0488],\n",
       "           ...,\n",
       "           [ 0.2617, -0.2045, -0.5565,  ..., -0.5609, -0.3072, -0.5310],\n",
       "           [-0.0162, -0.3004,  0.4041,  ..., -1.5526, -0.6767,  0.0069],\n",
       "           [-0.4205, -0.3796,  0.3419,  ..., -0.6682, -0.6332,  0.3700]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 1.3590e-01,  2.6484e-02, -5.4882e-01,  ..., -4.2998e-01,\n",
       "             5.9733e-01,  5.5146e-01],\n",
       "           [-9.9922e-02, -6.2632e-01, -2.2590e-01,  ...,  7.5546e-02,\n",
       "            -6.7436e-02, -9.3402e-02],\n",
       "           [ 1.3119e-02,  5.2942e-01, -5.0782e-01,  ...,  5.8595e-01,\n",
       "             2.9199e-01, -1.2983e-01],\n",
       "           ...,\n",
       "           [-2.2581e-01,  5.3019e-01,  7.0531e-01,  ..., -1.2313e-02,\n",
       "             5.0534e-01, -1.2105e-01],\n",
       "           [-5.8428e-01, -2.4185e-02,  2.2402e-01,  ..., -5.1476e-02,\n",
       "             3.7110e-01, -2.9634e-01],\n",
       "           [ 1.2935e-01,  3.9944e-01,  1.9171e-01,  ...,  6.1316e-02,\n",
       "            -8.0970e-01, -1.8409e-01]],\n",
       " \n",
       "          [[ 2.5847e-02, -1.7272e+00, -2.9802e-01,  ..., -4.0779e-01,\n",
       "            -3.1577e-01, -1.5657e-01],\n",
       "           [-2.8553e-01, -5.0940e-01, -3.8076e-02,  ..., -5.5022e-01,\n",
       "            -8.8463e-01,  2.2433e-01],\n",
       "           [ 5.1454e-01, -5.5695e-01,  4.9095e-01,  ..., -9.6038e-01,\n",
       "            -2.4395e-02, -3.4758e-01],\n",
       "           ...,\n",
       "           [ 3.7057e-01, -2.0872e-01, -5.3655e-01,  ...,  1.8612e-02,\n",
       "            -1.8839e-02, -4.3969e-01],\n",
       "           [-2.1671e-02,  1.6095e-01, -6.8719e-01,  ..., -1.8137e-01,\n",
       "             1.3621e-02,  3.6813e-02],\n",
       "           [ 1.2671e-01,  2.8544e-01, -4.4411e-01,  ..., -7.5598e-01,\n",
       "             2.8993e-01, -2.7267e-01]],\n",
       " \n",
       "          [[-2.4834e-01,  8.5401e-02,  5.3975e-01,  ...,  2.8202e-01,\n",
       "            -1.6113e-01,  3.0717e-01],\n",
       "           [-1.2702e+00,  4.9510e-01, -2.3402e-01,  ...,  1.0610e+00,\n",
       "            -3.2708e-01, -4.2588e-01],\n",
       "           [-6.3818e-01, -1.1594e-01,  8.2689e-01,  ..., -1.4586e-01,\n",
       "             7.2404e-01,  2.1909e-01],\n",
       "           ...,\n",
       "           [ 1.5078e-02, -8.2689e-01, -1.8753e-02,  ...,  1.0342e-01,\n",
       "             3.9253e-01,  2.7993e-01],\n",
       "           [ 1.3380e-01, -7.3451e-01,  1.8141e-01,  ..., -1.4347e-01,\n",
       "            -4.2771e-01, -3.2865e-01],\n",
       "           [ 2.3723e-01, -1.1387e-01, -7.5631e-01,  ...,  4.5009e-01,\n",
       "            -5.8659e-02,  1.3034e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.6633e-01, -2.6868e-01,  5.8313e-01,  ...,  5.7840e-01,\n",
       "            -2.9144e-01, -2.0404e-01],\n",
       "           [-3.3473e-01,  5.4997e-01,  2.7136e-01,  ...,  8.6840e-01,\n",
       "             5.3419e-01, -8.7656e-01],\n",
       "           [ 4.4912e-01,  5.0052e-01, -1.8110e-01,  ...,  1.4946e-01,\n",
       "             1.3252e+00, -9.1438e-01],\n",
       "           ...,\n",
       "           [ 3.8584e-01,  2.6166e-01,  3.2349e-01,  ..., -2.2128e-01,\n",
       "             3.7348e-03, -1.2767e+00],\n",
       "           [ 6.3350e-02, -9.7541e-02,  3.9960e-01,  ..., -3.3192e-01,\n",
       "             6.2953e-01, -7.6905e-01],\n",
       "           [ 2.8550e-01,  9.1312e-01,  2.8828e-01,  ..., -1.6115e-01,\n",
       "            -1.1940e+00,  9.2117e-02]],\n",
       " \n",
       "          [[ 7.5566e-02, -2.3047e-01,  6.0092e-02,  ...,  3.7928e-01,\n",
       "             1.6624e-01,  2.3401e-01],\n",
       "           [ 3.5427e-01, -3.5346e-01, -8.2333e-02,  ...,  7.4344e-01,\n",
       "             1.5881e-01,  2.8059e-05],\n",
       "           [ 7.7920e-01,  3.9165e-01,  6.7322e-01,  ..., -3.2793e-01,\n",
       "             1.6191e-01,  2.7849e-01],\n",
       "           ...,\n",
       "           [ 8.0508e-01, -3.4797e-01,  3.1586e-02,  ..., -3.0187e-01,\n",
       "             2.0058e-03,  2.2052e-01],\n",
       "           [ 6.4539e-01,  2.7308e-01,  4.0903e-01,  ..., -4.0107e-01,\n",
       "             7.3496e-01, -3.2296e-02],\n",
       "           [ 9.8156e-01,  9.5276e-02,  2.9619e-01,  ..., -1.7896e-01,\n",
       "             1.0119e-01, -4.6937e-01]],\n",
       " \n",
       "          [[-5.9070e-02, -4.8765e-02, -1.2285e-01,  ..., -5.6157e-02,\n",
       "             7.0908e-01, -6.9191e-02],\n",
       "           [-6.4355e-01,  4.2608e-01,  6.1159e-01,  ...,  7.3661e-01,\n",
       "             2.0782e-01, -5.1783e-01],\n",
       "           [ 3.0848e-01,  1.4050e-01,  2.2408e-01,  ...,  4.5751e-01,\n",
       "             5.9716e-01, -1.6101e-01],\n",
       "           ...,\n",
       "           [ 2.0756e-02, -3.6042e-01, -4.3790e-01,  ..., -4.0636e-01,\n",
       "             3.2691e-01,  2.7527e-01],\n",
       "           [ 1.3203e+00, -3.3211e-02, -3.4054e-01,  ..., -9.7752e-02,\n",
       "             5.5122e-01, -3.1159e-01],\n",
       "           [ 1.2091e-02,  3.1718e-01, -5.8962e-01,  ..., -5.5105e-01,\n",
       "             2.9305e-01,  3.1039e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 0.2543, -0.2484, -0.5924,  ...,  0.6836,  0.8544, -0.3964],\n",
       "           [ 0.2169,  0.7366, -0.2432,  ...,  0.1156,  0.6328, -0.4006],\n",
       "           [ 0.6160,  0.1886,  0.2364,  ...,  0.4559,  0.0418,  0.0053],\n",
       "           ...,\n",
       "           [ 0.6175, -0.9066, -0.1970,  ...,  0.1968,  0.2781, -0.5235],\n",
       "           [ 0.4749, -0.7775, -0.0703,  ...,  0.1131,  0.6236, -0.3113],\n",
       "           [ 0.4815, -0.1958, -0.2740,  ...,  0.2095,  0.1053, -0.5008]],\n",
       " \n",
       "          [[ 0.1485,  0.1682,  0.0486,  ..., -0.5547,  0.8454, -0.3152],\n",
       "           [ 0.6182, -0.7702, -0.0737,  ...,  0.1588,  0.5669, -0.5626],\n",
       "           [-0.6487,  0.0491, -0.3160,  ..., -1.5302, -0.1470, -0.0397],\n",
       "           ...,\n",
       "           [-0.6476, -0.2175,  0.2118,  ..., -0.1719,  0.8869, -0.0329],\n",
       "           [ 0.1756, -0.1994, -0.0111,  ...,  0.2515,  0.9107, -0.0474],\n",
       "           [-0.2306, -1.2118, -0.3189,  ..., -0.5268,  0.5553,  0.1899]],\n",
       " \n",
       "          [[-0.0822,  0.1884,  0.0700,  ..., -0.5231,  1.2550, -0.1964],\n",
       "           [ 0.1298, -0.9116, -0.7465,  ..., -0.2267,  0.3063, -0.3338],\n",
       "           [ 0.1420, -0.2149,  0.7020,  ..., -0.6090,  0.8911,  0.2041],\n",
       "           ...,\n",
       "           [-0.5219, -0.7561, -0.0028,  ...,  0.4483,  0.2527,  0.0308],\n",
       "           [-0.6411, -0.7721,  0.5026,  ..., -0.3955,  0.0301, -0.3583],\n",
       "           [-0.6012, -0.8770, -0.2382,  ..., -0.4173, -0.1737,  0.0344]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.2400,  0.5462,  0.6604,  ..., -0.0860, -0.4547, -0.8371],\n",
       "           [ 0.7538,  0.1111,  0.2970,  ...,  0.1354, -0.0274,  0.7500],\n",
       "           [ 0.8816,  0.2903,  0.0304,  ..., -1.0143, -0.2806,  0.5495],\n",
       "           ...,\n",
       "           [ 0.2903,  0.4347,  0.5768,  ...,  0.1581,  0.6255,  0.8263],\n",
       "           [-0.7149,  0.3686,  0.4418,  ...,  0.4614,  1.9471, -0.6686],\n",
       "           [ 0.6926, -0.2241,  0.1683,  ...,  0.0411,  0.4887,  0.5887]],\n",
       " \n",
       "          [[ 0.1777, -0.2258, -0.3885,  ..., -0.2446,  0.1228,  0.1318],\n",
       "           [-1.1563, -0.2747,  0.0564,  ..., -0.7737, -0.0070,  0.8161],\n",
       "           [ 0.4885, -0.5950,  0.1221,  ..., -0.0425, -0.1451, -0.0989],\n",
       "           ...,\n",
       "           [ 0.1215, -0.0987, -0.0698,  ..., -0.6230, -0.1685,  0.0020],\n",
       "           [-0.2146, -0.8398,  0.7491,  ..., -1.1640, -0.2533, -0.0685],\n",
       "           [ 0.1969,  0.3309, -0.2045,  ..., -1.2199,  0.1052,  0.3576]],\n",
       " \n",
       "          [[-0.0516,  0.8220, -0.0716,  ...,  0.0617, -0.3259,  0.3941],\n",
       "           [-0.5830,  0.6697,  1.3265,  ...,  0.1508, -0.0080,  0.3089],\n",
       "           [-0.0561,  0.8512, -0.3008,  ...,  0.3621, -0.6291,  0.4649],\n",
       "           ...,\n",
       "           [ 0.3792,  0.6964,  0.7921,  ...,  0.2395,  0.4184,  0.2471],\n",
       "           [ 0.1936, -0.0897,  0.3415,  ...,  0.2989,  0.0476, -0.8891],\n",
       "           [ 0.4584,  0.7587, -0.2144,  ...,  0.0557,  0.7533,  0.0313]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 4.5096e-01,  1.3296e-01,  7.5372e-01,  ...,  6.6764e-01,\n",
       "             7.9000e-01, -7.0111e-02],\n",
       "           [ 9.6624e-02,  1.1808e-01,  7.9365e-01,  ..., -5.9868e-01,\n",
       "             1.1980e+00,  3.5931e-04],\n",
       "           [-1.9940e-01, -4.1978e-01,  4.9482e-01,  ...,  5.4025e-01,\n",
       "             1.6992e-01,  3.3669e-01],\n",
       "           ...,\n",
       "           [-3.3787e-01,  2.5302e-01, -1.5696e-01,  ..., -1.5049e-01,\n",
       "            -1.6155e-01, -4.4070e-01],\n",
       "           [ 2.8690e-01, -1.2012e+00, -3.3685e-01,  ...,  1.3092e-01,\n",
       "            -4.5071e-02, -4.1153e-01],\n",
       "           [ 3.6292e-01, -7.9964e-01, -7.2332e-01,  ..., -2.0609e-01,\n",
       "            -2.0232e-01,  2.8082e-01]],\n",
       " \n",
       "          [[ 2.2347e-01,  5.9135e-01, -7.2010e-02,  ..., -5.5492e-01,\n",
       "            -4.3656e-01, -2.8242e-01],\n",
       "           [-1.1202e-01,  1.3674e+00,  9.5979e-02,  ..., -7.4683e-01,\n",
       "            -9.5946e-01, -4.5333e-01],\n",
       "           [-8.7260e-01, -5.4160e-02, -1.7863e-01,  ..., -5.4127e-01,\n",
       "            -6.9934e-01, -3.6855e-01],\n",
       "           ...,\n",
       "           [ 2.9571e-01,  8.0508e-01,  2.8720e-01,  ...,  3.0738e-01,\n",
       "            -5.9509e-02,  1.9508e-01],\n",
       "           [ 3.4685e-01, -4.5855e-02, -3.1867e-01,  ..., -6.2852e-01,\n",
       "            -3.5883e-01,  5.3996e-01],\n",
       "           [ 2.9093e-01,  8.9836e-01,  4.0230e-01,  ..., -3.9110e-02,\n",
       "             3.1516e-01, -2.8674e-01]],\n",
       " \n",
       "          [[-2.9697e-01, -2.9517e-01,  1.4000e-01,  ...,  5.2926e-02,\n",
       "            -4.9801e-01,  6.4856e-01],\n",
       "           [-8.1410e-01, -9.4139e-02,  1.0378e+00,  ...,  2.6332e-01,\n",
       "             1.4617e-01,  7.1341e-01],\n",
       "           [-3.4178e-01, -6.7072e-01,  1.5015e+00,  ..., -4.1170e-01,\n",
       "             4.9808e-01,  4.7917e-02],\n",
       "           ...,\n",
       "           [-3.4584e-01, -5.0864e-02, -2.1611e-01,  ...,  3.2408e-01,\n",
       "             3.3721e-01, -3.6555e-01],\n",
       "           [-2.5116e-01, -7.0134e-01,  9.6900e-01,  ..., -4.8442e-01,\n",
       "             5.8281e-01,  6.5452e-01],\n",
       "           [ 1.8805e-02, -6.5176e-01, -1.8621e-01,  ...,  6.1944e-01,\n",
       "             5.5704e-01, -2.8930e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.9750e-01,  1.7567e-01,  4.6255e-01,  ..., -6.2713e-01,\n",
       "            -4.0004e-01,  6.7547e-01],\n",
       "           [-8.7652e-01, -6.4689e-01, -1.4071e+00,  ..., -3.3757e-01,\n",
       "            -7.2277e-01, -3.4659e-01],\n",
       "           [-1.0233e-01,  4.5411e-01, -5.0960e-01,  ..., -1.9305e-02,\n",
       "            -5.0266e-01,  1.0130e+00],\n",
       "           ...,\n",
       "           [-2.4561e-01, -1.6352e-01, -6.8339e-01,  ..., -6.6555e-01,\n",
       "             9.0803e-01,  1.3582e-01],\n",
       "           [ 8.1629e-02, -8.5697e-02,  2.8844e-01,  ..., -1.1070e+00,\n",
       "            -1.7389e-01,  4.1609e-01],\n",
       "           [-1.8489e-01, -5.3623e-02, -2.1880e-02,  ..., -4.2875e-01,\n",
       "             3.0328e-01,  1.2897e-01]],\n",
       " \n",
       "          [[-2.8075e-02,  4.6899e-02,  1.3891e-01,  ...,  1.6760e-01,\n",
       "             9.7843e-01,  6.8313e-01],\n",
       "           [-5.3916e-01,  4.2994e-01, -3.5438e-02,  ..., -1.5986e-01,\n",
       "             1.1281e-01, -1.4674e-01],\n",
       "           [-4.8972e-02, -2.7028e-01,  8.0720e-02,  ...,  3.0695e-01,\n",
       "             3.5392e-01, -8.0389e-01],\n",
       "           ...,\n",
       "           [ 9.0223e-02, -1.7021e-01,  4.7785e-01,  ...,  4.5379e-01,\n",
       "             7.9277e-01, -1.0347e-01],\n",
       "           [-3.4131e-01,  2.5172e-02, -5.3144e-01,  ..., -3.4417e-01,\n",
       "             1.9153e-01,  6.8945e-02],\n",
       "           [ 4.7387e-02, -5.4914e-01, -3.7756e-02,  ...,  5.1396e-01,\n",
       "             6.0108e-01, -2.4774e-01]],\n",
       " \n",
       "          [[ 1.5053e-01,  6.5125e-01, -4.3058e-01,  ...,  1.0495e-02,\n",
       "            -1.4018e-01,  9.7671e-01],\n",
       "           [ 2.1801e-02,  9.0309e-02, -7.8631e-01,  ...,  3.7936e-01,\n",
       "             3.3848e-01,  1.6992e-01],\n",
       "           [ 3.9369e-01,  2.6391e-01, -7.8294e-01,  ...,  2.6521e-01,\n",
       "             3.9667e-01,  1.0707e+00],\n",
       "           ...,\n",
       "           [-3.0004e-01, -1.2399e-01,  4.7753e-01,  ..., -5.5929e-01,\n",
       "             3.3347e-02,  2.1986e-01],\n",
       "           [-2.8631e-02,  1.0569e-01,  4.8159e-01,  ...,  7.0008e-02,\n",
       "             1.1731e-01,  1.6799e-01],\n",
       "           [-4.5178e-01,  1.0785e+00,  1.1542e-01,  ..., -8.1303e-01,\n",
       "             6.9299e-01, -1.8279e-01]]]], device='cuda:0'),\n",
       " tensor([[[[-5.1338e-01,  4.2361e-01, -4.2098e-01,  ...,  2.9640e-01,\n",
       "            -2.0363e-01,  7.1696e-02],\n",
       "           [ 5.2555e-02,  2.5150e-01, -5.0902e-01,  ...,  1.3761e+00,\n",
       "            -3.1169e-01,  3.1914e-01],\n",
       "           [ 1.2319e-01,  4.2203e-02, -8.6350e-01,  ...,  1.1162e+00,\n",
       "             6.9198e-01,  2.9527e-01],\n",
       "           ...,\n",
       "           [ 2.8524e-01,  7.4941e-01, -1.1017e-01,  ...,  5.5352e-01,\n",
       "            -2.6458e-02,  6.3929e-01],\n",
       "           [-3.0159e-01,  8.7793e-01, -3.9755e-01,  ...,  4.6299e-01,\n",
       "             5.0593e-01, -3.1202e-01],\n",
       "           [ 3.6861e-01,  8.1215e-01,  2.5500e-01,  ...,  7.5009e-01,\n",
       "            -2.3162e-01, -5.2063e-02]],\n",
       " \n",
       "          [[-6.1395e-01, -3.5058e-01,  6.2160e-01,  ..., -6.9471e-01,\n",
       "             1.4707e-01,  4.7199e-01],\n",
       "           [-9.0483e-01, -8.9627e-01,  1.0279e+00,  ...,  3.3312e-01,\n",
       "            -5.8251e-01, -7.4763e-01],\n",
       "           [-3.3788e-01,  2.7241e-01,  7.5454e-01,  ..., -3.0126e-01,\n",
       "             8.3507e-02,  2.3211e-01],\n",
       "           ...,\n",
       "           [-1.5477e-01,  3.1305e-01,  6.9791e-01,  ...,  6.8668e-01,\n",
       "             6.0440e-01, -1.4853e-01],\n",
       "           [-4.1134e-01, -1.0392e+00,  5.6254e-01,  ..., -5.3989e-02,\n",
       "            -7.4816e-01, -3.5912e-01],\n",
       "           [ 5.1087e-01, -3.4794e-01, -1.2600e-01,  ...,  5.9259e-01,\n",
       "            -5.6450e-02, -1.0185e-01]],\n",
       " \n",
       "          [[ 3.6801e-01,  1.9440e-03,  6.6041e-01,  ..., -1.0873e+00,\n",
       "            -1.6827e-01, -7.2811e-01],\n",
       "           [-2.4239e-03, -1.4515e-01,  5.2696e-01,  ..., -3.1119e-02,\n",
       "             3.6240e-01, -2.3478e-01],\n",
       "           [-1.4223e+00, -5.8661e-01, -4.5009e-01,  ...,  4.5977e-02,\n",
       "             5.4551e-02,  7.7029e-02],\n",
       "           ...,\n",
       "           [-6.2149e-02,  3.8784e-01,  1.3649e-01,  ...,  9.6314e-02,\n",
       "            -9.2479e-01, -8.3049e-01],\n",
       "           [ 4.2046e-01, -6.6827e-01, -3.5924e-03,  ...,  2.7869e-01,\n",
       "            -3.5816e-01, -4.8568e-01],\n",
       "           [ 5.0007e-01,  1.8017e-01,  4.1000e-01,  ...,  8.7537e-01,\n",
       "            -2.1370e-01, -6.1481e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0917e-01,  1.4321e+00, -3.3821e-01,  ..., -5.1676e-01,\n",
       "             8.9730e-01,  4.1483e-01],\n",
       "           [-2.8643e-02,  5.9926e-01, -2.1411e-01,  ..., -6.4900e-01,\n",
       "             6.3247e-01,  2.3838e-01],\n",
       "           [-1.7557e-01,  8.9586e-02, -6.5968e-01,  ..., -1.1026e+00,\n",
       "             1.0117e+00,  3.6460e-01],\n",
       "           ...,\n",
       "           [-5.9250e-01,  6.7869e-01,  3.8991e-01,  ..., -3.5701e-01,\n",
       "             1.2073e+00,  1.0774e-01],\n",
       "           [ 2.3067e-01,  3.6326e-02, -1.6282e-01,  ...,  3.1049e-01,\n",
       "             6.2170e-02, -4.2901e-02],\n",
       "           [-2.9205e-01,  2.8598e-01, -1.8679e-01,  ..., -3.5685e-01,\n",
       "             6.1958e-01, -3.0066e-01]],\n",
       " \n",
       "          [[ 4.2450e-01,  2.9558e-01, -6.1367e-01,  ..., -2.4651e-01,\n",
       "            -4.7719e-01, -6.4498e-01],\n",
       "           [-3.4970e-01, -2.6401e-01, -4.4401e-01,  ..., -4.1864e-01,\n",
       "            -2.9440e-01, -9.4323e-01],\n",
       "           [-5.5783e-03,  3.9089e-01, -2.9255e-01,  ...,  1.6082e-01,\n",
       "             1.5336e-01, -1.2121e+00],\n",
       "           ...,\n",
       "           [ 3.1054e-01, -1.0962e+00, -1.6489e+00,  ..., -4.6582e-01,\n",
       "             2.8282e-01, -5.2759e-01],\n",
       "           [-5.9112e-02,  1.0231e+00, -7.8261e-01,  ..., -2.7400e-01,\n",
       "             3.0717e-01, -6.4423e-01],\n",
       "           [ 5.0128e-01, -1.9558e+00, -1.5878e+00,  ..., -3.5941e-02,\n",
       "            -4.0052e-02, -1.0760e+00]],\n",
       " \n",
       "          [[ 6.5775e-01,  4.4501e-01, -4.3467e-01,  ..., -2.7290e-02,\n",
       "            -6.4478e-01,  1.9407e-02],\n",
       "           [ 1.3329e-01, -8.6650e-01, -9.3152e-01,  ..., -7.3917e-01,\n",
       "            -1.4616e-01, -1.3306e+00],\n",
       "           [ 1.2172e-01,  3.2492e-01, -3.3566e-01,  ...,  5.9295e-03,\n",
       "            -4.4996e-01, -1.7074e-01],\n",
       "           ...,\n",
       "           [-1.6686e-01, -3.8285e-01,  2.1663e-02,  ..., -1.6321e-01,\n",
       "            -4.7029e-03,  1.0686e-01],\n",
       "           [-1.6459e-01, -1.8368e-01, -1.1425e-01,  ...,  2.0679e-01,\n",
       "             7.9615e-01,  5.6804e-02],\n",
       "           [ 4.7927e-01, -4.4486e-01, -3.8338e-01,  ...,  2.4487e-01,\n",
       "            -4.6065e-01, -7.5244e-03]]]], device='cuda:0'),\n",
       " tensor([[[[ 0.1473, -0.1227,  0.4702,  ...,  0.2756, -0.6746, -0.3860],\n",
       "           [-0.4112, -0.6029,  0.5338,  ..., -0.3268, -0.5748, -0.3294],\n",
       "           [ 0.0378, -0.1466,  0.3462,  ..., -0.3229, -1.0914, -0.4841],\n",
       "           ...,\n",
       "           [ 0.0790, -0.4273, -0.0639,  ..., -0.8567, -0.6883, -0.3240],\n",
       "           [ 0.5882, -0.5891,  0.1061,  ...,  0.4713, -0.1547,  0.0435],\n",
       "           [ 0.1212, -0.3951,  0.2984,  ...,  0.2172, -0.3439, -0.1653]],\n",
       " \n",
       "          [[ 0.9822, -0.6156,  0.1430,  ..., -0.1220,  0.8927,  0.4579],\n",
       "           [ 0.4466, -1.0392, -0.0419,  ...,  1.0424,  0.8101,  0.3411],\n",
       "           [ 0.7797, -1.7214, -0.4387,  ...,  0.0905,  0.6640,  0.1515],\n",
       "           ...,\n",
       "           [ 0.6289,  0.2510,  0.5594,  ..., -0.6975, -0.3557, -0.4075],\n",
       "           [ 0.4580, -0.3307,  0.4874,  ..., -0.7801, -0.6292, -0.2073],\n",
       "           [ 0.3450, -0.0489,  0.9815,  ...,  0.2807, -0.7748, -0.3966]],\n",
       " \n",
       "          [[-0.4588, -0.0781,  0.2050,  ...,  0.3750, -0.0022,  0.0491],\n",
       "           [-0.4284, -1.0234,  0.5289,  ...,  0.2072,  0.0374, -0.1368],\n",
       "           [-0.1549,  0.0777,  0.2421,  ...,  0.7612, -0.5022,  0.0495],\n",
       "           ...,\n",
       "           [-0.0388, -0.9531, -0.1643,  ..., -0.3253, -0.2422,  0.0359],\n",
       "           [ 0.5704, -0.3005, -0.1567,  ...,  0.2799, -0.7033, -0.7927],\n",
       "           [ 0.3238, -0.4033, -0.5561,  ...,  0.1860, -0.4047, -0.1849]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.2401, -0.2955,  0.3943,  ...,  0.0721,  0.1899,  0.1296],\n",
       "           [-0.5008,  1.1409,  0.6946,  ..., -0.2515,  0.5253, -0.4985],\n",
       "           [ 0.1235, -0.1363,  0.2196,  ..., -0.2173,  0.6277, -0.5404],\n",
       "           ...,\n",
       "           [-0.4085,  0.2833,  0.7888,  ...,  1.1574,  0.1088, -0.1656],\n",
       "           [ 0.6322, -0.7339,  0.3866,  ...,  0.6106, -0.9329, -0.3510],\n",
       "           [ 0.1748,  0.1241,  0.6697,  ...,  1.1138,  0.3592, -0.0513]],\n",
       " \n",
       "          [[-0.4445, -0.2000, -0.4812,  ..., -0.5777, -0.6702, -0.1280],\n",
       "           [-0.3330, -0.5181,  0.1995,  ..., -0.1160, -0.2465,  0.3834],\n",
       "           [ 0.0944, -0.4302,  0.4186,  ...,  0.1200,  0.4108, -1.0220],\n",
       "           ...,\n",
       "           [-0.4984, -0.3745, -0.4530,  ...,  1.1814, -0.4358, -0.4680],\n",
       "           [-0.0909, -0.3441,  0.9859,  ...,  0.4849, -1.0261, -0.8655],\n",
       "           [ 0.0128, -0.6401, -0.0990,  ...,  0.9755, -1.2628, -0.2935]],\n",
       " \n",
       "          [[ 0.1070, -0.8283, -1.2910,  ..., -0.4200, -0.1418, -0.1879],\n",
       "           [ 1.3177, -0.4594, -0.6866,  ..., -0.6427,  1.1365, -0.0594],\n",
       "           [ 0.3641, -0.9955, -0.7518,  ...,  0.0603,  0.0300,  0.1077],\n",
       "           ...,\n",
       "           [ 0.9118, -0.8513,  0.6954,  ..., -0.9578,  0.1605,  0.5781],\n",
       "           [-0.1416,  0.2724, -0.2007,  ..., -0.4927, -0.2136,  0.3821],\n",
       "           [ 1.1190, -1.0469,  0.6853,  ..., -0.6631,  0.6338,  0.6443]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[ 1.0286e+00,  7.6410e-03,  1.3821e+00,  ...,  1.7576e-01,\n",
       "            -1.5250e-02,  2.6779e-01],\n",
       "           [ 6.1685e-01, -2.4543e-01,  1.0876e+00,  ...,  6.9362e-02,\n",
       "             1.4650e-01, -1.3127e-01],\n",
       "           [ 3.6889e-01,  7.5349e-02,  2.3775e-01,  ...,  1.0236e-01,\n",
       "            -4.2261e-01,  4.9235e-02],\n",
       "           ...,\n",
       "           [ 5.5496e-01, -1.7638e-01, -1.1632e+00,  ..., -2.7205e-01,\n",
       "             1.8063e-01,  1.2118e+00],\n",
       "           [ 3.1223e-01, -2.2018e-01,  1.0118e-01,  ...,  4.2671e-01,\n",
       "            -1.2299e-01,  3.8481e-01],\n",
       "           [ 1.9222e+00, -9.1642e-01,  4.6262e-01,  ...,  8.2176e-01,\n",
       "            -7.1312e-01,  7.0071e-01]],\n",
       " \n",
       "          [[-2.2980e-02,  8.6661e-01,  5.6601e-01,  ...,  3.5639e-01,\n",
       "             2.0366e-01,  1.3282e-01],\n",
       "           [ 2.7147e-01,  5.7235e-01,  1.5402e+00,  ...,  3.6010e-01,\n",
       "             4.8722e-01, -4.2968e-01],\n",
       "           [-2.9254e-02,  5.6831e-01,  7.2582e-01,  ..., -4.1110e-01,\n",
       "             4.2097e-01, -1.0304e+00],\n",
       "           ...,\n",
       "           [ 3.7833e-01,  3.9459e-02,  6.3917e-02,  ...,  9.5160e-02,\n",
       "             4.0674e-01,  8.5324e-01],\n",
       "           [ 8.1803e-01,  6.1178e-01, -5.8007e-02,  ..., -1.4420e-01,\n",
       "             5.6267e-01, -1.2135e-01],\n",
       "           [-1.1734e-01,  4.3491e-01,  1.9881e-01,  ...,  1.0667e-01,\n",
       "             5.9281e-01,  9.7123e-01]],\n",
       " \n",
       "          [[-4.1307e-01,  1.3906e+00, -4.3637e-01,  ...,  7.0786e-01,\n",
       "            -8.7211e-01, -4.6787e-03],\n",
       "           [-1.0643e+00,  1.1227e+00, -9.0525e-01,  ...,  3.9795e-01,\n",
       "             1.5199e-01, -8.8451e-01],\n",
       "           [ 2.8928e-01, -3.3605e-01, -5.0511e-01,  ..., -1.6988e-02,\n",
       "            -7.5878e-01, -1.6647e+00],\n",
       "           ...,\n",
       "           [ 1.4079e-02,  1.4194e+00, -2.4991e-01,  ..., -2.6153e-01,\n",
       "            -1.4158e-01, -9.4086e-02],\n",
       "           [ 3.5602e-01,  9.0502e-01, -2.7377e-01,  ..., -9.1010e-02,\n",
       "            -4.6306e-01, -1.0359e+00],\n",
       "           [-9.8753e-01,  1.5449e+00,  6.5276e-01,  ..., -3.5628e-01,\n",
       "             1.0876e+00,  2.4444e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 9.7201e-02, -1.3789e-01,  6.1929e-02,  ...,  6.4294e-02,\n",
       "            -5.6346e-01,  2.7691e-01],\n",
       "           [ 2.8566e-01,  2.0281e-01,  7.2950e-02,  ..., -2.5967e-01,\n",
       "             3.1622e-01,  9.3380e-01],\n",
       "           [ 5.1687e-01,  2.0139e-01,  6.8071e-01,  ...,  5.8078e-01,\n",
       "             2.4586e-01,  2.3023e-01],\n",
       "           ...,\n",
       "           [-2.2010e-01,  2.5960e-01,  6.1643e-01,  ..., -1.4011e-01,\n",
       "             8.2740e-01, -4.0800e-01],\n",
       "           [ 2.8556e-01, -1.1444e+00,  5.0320e-01,  ..., -3.0016e-02,\n",
       "             2.4139e-01,  6.9686e-02],\n",
       "           [-2.2626e-01,  3.8485e-01,  1.0303e-01,  ..., -1.0037e-01,\n",
       "             5.8232e-01, -7.7414e-01]],\n",
       " \n",
       "          [[-4.2180e-01, -3.7212e-01,  1.0796e-01,  ..., -6.3765e-01,\n",
       "             2.7649e-01,  6.6332e-01],\n",
       "           [ 2.6699e-01, -6.7963e-01, -2.7844e-01,  ..., -7.4595e-01,\n",
       "            -5.4031e-01,  3.7470e-01],\n",
       "           [-1.5103e-01, -1.7474e-01, -4.7057e-02,  ...,  2.9205e-01,\n",
       "             6.5875e-03,  6.5789e-01],\n",
       "           ...,\n",
       "           [ 5.6617e-05,  3.0012e-01,  2.7189e-01,  ...,  9.3960e-02,\n",
       "            -5.6342e-01, -4.4940e-01],\n",
       "           [ 1.4440e+00,  3.9273e-01, -1.2307e-01,  ..., -1.0282e+00,\n",
       "            -7.0770e-01, -4.2301e-01],\n",
       "           [ 7.9103e-01, -5.6545e-01,  4.3304e-01,  ...,  2.5283e-01,\n",
       "            -3.1762e-01, -1.2840e+00]],\n",
       " \n",
       "          [[-2.4767e-01, -2.7103e-01,  8.6054e-01,  ...,  6.8052e-01,\n",
       "            -4.5263e-01, -8.9736e-01],\n",
       "           [-7.2852e-01, -1.6465e-02,  1.8860e-01,  ...,  5.8286e-02,\n",
       "             9.0426e-01,  4.0765e-02],\n",
       "           [-4.9126e-01, -8.8638e-01,  9.3636e-01,  ...,  3.8714e-01,\n",
       "             9.2749e-01, -7.8513e-01],\n",
       "           ...,\n",
       "           [-4.3612e-01, -4.2941e-01,  4.4416e-01,  ..., -8.7904e-02,\n",
       "            -2.7267e-03, -3.5731e-01],\n",
       "           [-7.5569e-02,  2.0889e-01,  5.8244e-01,  ...,  3.0222e-01,\n",
       "             6.7417e-01,  3.1433e-02],\n",
       "           [-3.9510e-01, -3.1635e-01,  8.1891e-01,  ...,  6.4423e-01,\n",
       "            -5.9465e-01, -2.1217e-01]]]], device='cuda:0'),\n",
       " tensor([[[[ 7.2397e-02, -3.6565e-01, -1.0096e-01,  ..., -7.0489e-01,\n",
       "            -4.6440e-01,  4.5201e-01],\n",
       "           [-5.7452e-01, -5.2142e-01, -9.5654e-02,  ...,  3.5969e-01,\n",
       "            -1.4540e-01,  9.1160e-01],\n",
       "           [ 5.7446e-01,  1.7868e-01, -2.5724e-01,  ..., -1.4846e-02,\n",
       "            -5.7416e-01,  6.3339e-01],\n",
       "           ...,\n",
       "           [ 7.8843e-01, -6.0084e-01,  5.8417e-01,  ..., -5.4886e-01,\n",
       "            -7.1003e-02,  1.0394e-01],\n",
       "           [ 2.8610e-01,  3.0477e-01,  3.4351e-01,  ...,  2.8987e-01,\n",
       "             5.9622e-02, -4.8147e-01],\n",
       "           [ 1.0171e+00, -3.9011e-01,  6.3799e-02,  ..., -4.3180e-01,\n",
       "             8.8367e-01,  2.7053e-01]],\n",
       " \n",
       "          [[-5.7152e-03, -5.4329e-01,  4.1638e-01,  ...,  8.7757e-01,\n",
       "            -3.4830e-02,  6.7802e-02],\n",
       "           [ 1.0113e+00, -4.2883e-01, -2.1401e-01,  ...,  4.8879e-01,\n",
       "             6.5327e-01,  1.3006e-01],\n",
       "           [-6.0285e-01, -8.9664e-01,  3.7867e-01,  ...,  1.2935e+00,\n",
       "             2.8648e-01, -2.7169e-01],\n",
       "           ...,\n",
       "           [-5.7060e-01, -2.4767e-01,  7.5474e-03,  ..., -5.9082e-02,\n",
       "             1.1347e-01,  8.3565e-01],\n",
       "           [-2.6807e-01,  7.8885e-02,  1.3302e-01,  ...,  1.1467e+00,\n",
       "             6.2712e-01, -3.3681e-01],\n",
       "           [-2.3319e-01,  1.4506e-01, -9.4633e-01,  ..., -4.0830e-01,\n",
       "             3.5300e-01,  2.7850e-01]],\n",
       " \n",
       "          [[ 6.4613e-01, -1.8191e-01,  6.4744e-01,  ...,  4.9162e-01,\n",
       "            -2.4839e-01, -5.0391e-01],\n",
       "           [-1.7320e-01, -1.4141e-01,  7.9949e-01,  ..., -1.0071e+00,\n",
       "            -3.5282e-01, -7.3540e-01],\n",
       "           [ 6.2474e-01,  7.3472e-01,  2.6679e-01,  ...,  7.3347e-02,\n",
       "            -5.6136e-01, -5.1475e-01],\n",
       "           ...,\n",
       "           [ 1.2365e-01,  1.0303e+00, -4.6340e-01,  ...,  1.7197e-01,\n",
       "            -6.3748e-01, -2.6956e-01],\n",
       "           [ 7.3590e-01,  5.1100e-01, -8.0074e-02,  ...,  2.7351e-01,\n",
       "            -2.1938e-01,  1.4487e-01],\n",
       "           [ 1.7809e-01, -3.3303e-01, -8.2112e-01,  ...,  3.6076e-01,\n",
       "            -4.0810e-01, -2.2060e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.5977e-01, -7.0269e-01, -4.5257e-03,  ...,  2.8105e-01,\n",
       "             1.9028e-01,  5.3697e-01],\n",
       "           [ 5.8496e-01, -1.1490e+00,  4.1571e-01,  ..., -6.6161e-01,\n",
       "            -3.4438e-01,  1.3118e+00],\n",
       "           [ 2.8125e-01,  1.3767e-02, -5.3172e-02,  ..., -6.0058e-01,\n",
       "            -8.1008e-02,  1.0659e+00],\n",
       "           ...,\n",
       "           [ 5.4966e-01,  1.9344e-01,  2.3021e-01,  ..., -1.5860e-02,\n",
       "            -1.4954e-01,  8.3924e-01],\n",
       "           [ 3.5580e-01,  7.6606e-01, -1.6723e-02,  ...,  8.3188e-01,\n",
       "             4.3652e-01,  5.2700e-01],\n",
       "           [ 2.6221e-01, -1.9671e-01, -2.5055e-01,  ...,  2.0140e-01,\n",
       "             7.2376e-01,  2.4440e-01]],\n",
       " \n",
       "          [[ 2.2593e-01,  4.6785e-01, -1.6850e+00,  ..., -6.1631e-02,\n",
       "            -1.7714e-01,  1.2172e+00],\n",
       "           [-4.5961e-01,  3.6778e-01, -7.5532e-01,  ...,  7.2722e-01,\n",
       "            -1.9846e-01,  3.3384e-01],\n",
       "           [ 1.4913e-01,  4.5325e-03, -2.4135e+00,  ..., -2.8216e-02,\n",
       "            -1.2793e+00,  6.0502e-01],\n",
       "           ...,\n",
       "           [ 7.0290e-02, -8.4337e-02,  4.6817e-01,  ...,  1.3842e-01,\n",
       "             3.1110e-02, -4.9924e-01],\n",
       "           [-4.0142e-01,  7.1787e-01,  1.1356e+00,  ..., -2.7712e-01,\n",
       "             1.1941e-01,  7.3497e-01],\n",
       "           [ 5.1958e-01, -1.8810e-01,  5.2089e-02,  ...,  1.9103e-01,\n",
       "             2.4414e-01,  2.4548e-01]],\n",
       " \n",
       "          [[ 9.4174e-01,  4.0020e-01,  8.8127e-02,  ..., -4.4897e-01,\n",
       "             3.1019e-01,  5.1358e-02],\n",
       "           [-4.0192e-01,  8.7308e-02,  2.4950e-01,  ...,  8.5998e-01,\n",
       "             3.4415e-01, -2.3568e-03],\n",
       "           [ 1.1138e+00,  1.4126e-01, -2.2436e-01,  ..., -2.0764e-01,\n",
       "            -4.0127e-01, -3.3921e-03],\n",
       "           ...,\n",
       "           [ 2.3681e-01, -1.5346e-01,  1.7100e-01,  ...,  1.6985e-02,\n",
       "             3.9593e-01,  1.1484e-01],\n",
       "           [ 2.4906e-01,  3.3268e-01,  5.4622e-02,  ...,  3.2568e-01,\n",
       "            -5.3012e-01,  2.4173e-01],\n",
       "           [ 1.0873e-01,  4.8699e-01,  3.5852e-02,  ..., -9.3116e-01,\n",
       "            -2.3495e-04,  2.2288e-02]]]], device='cuda:0'),\n",
       " tensor([[[[ 0.7177,  0.6180,  0.0923,  ..., -0.9116, -0.0457,  0.3804],\n",
       "           [-0.4750, -0.6640,  0.7227,  ..., -0.1094, -0.8308,  0.1122],\n",
       "           [ 0.3568,  0.4374, -0.1130,  ..., -0.1677, -0.4512,  0.3483],\n",
       "           ...,\n",
       "           [ 0.7240,  0.2970, -0.7184,  ..., -0.5730, -0.3910,  0.1518],\n",
       "           [ 0.1813,  0.0452,  0.1253,  ..., -0.5172,  0.8127,  0.5777],\n",
       "           [ 0.4506,  1.3515,  0.3400,  ..., -0.7572,  0.9122, -0.5716]],\n",
       " \n",
       "          [[ 1.0183, -0.2656,  0.2412,  ..., -0.4759, -1.0139,  1.0829],\n",
       "           [ 0.3999, -0.8645, -0.0797,  ..., -0.5018,  0.0349, -0.7541],\n",
       "           [-0.6701, -0.9493, -0.2530,  ..., -0.0684, -0.7903,  0.5252],\n",
       "           ...,\n",
       "           [-0.4786, -0.4113, -0.3730,  ..., -0.5959,  0.8468,  0.2872],\n",
       "           [ 0.7213, -0.4660, -0.1062,  ...,  0.3943,  1.1808,  0.3628],\n",
       "           [ 0.1860, -0.3894, -0.2370,  ..., -0.5438,  0.6700, -0.0327]],\n",
       " \n",
       "          [[ 0.3316, -0.2672,  0.6867,  ..., -0.4225, -0.5389,  0.2885],\n",
       "           [-0.2422,  0.2055, -0.2544,  ..., -0.3965, -0.5644,  0.8018],\n",
       "           [ 0.1922, -0.7427, -0.3316,  ..., -0.2769, -0.0647,  0.0154],\n",
       "           ...,\n",
       "           [-0.2065, -0.0816, -0.3159,  ...,  0.0846,  0.0449,  0.2607],\n",
       "           [ 0.4220,  0.7811, -0.7332,  ...,  0.1757, -0.5808,  0.8998],\n",
       "           [ 0.3900, -0.2868, -0.8381,  ...,  0.2325,  0.1843, -0.5884]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.1931,  0.6598, -1.1577,  ...,  0.2150,  0.4618,  0.2141],\n",
       "           [-0.4289,  0.5218, -0.5705,  ..., -0.1147,  0.3753,  0.4121],\n",
       "           [-0.2033, -0.4927, -0.7836,  ..., -0.4782, -0.0967, -0.6389],\n",
       "           ...,\n",
       "           [ 0.9003,  0.3284,  0.1130,  ...,  0.0986, -0.5460, -0.6847],\n",
       "           [-0.1623, -0.1481,  0.3240,  ..., -1.2942,  0.3852,  0.5163],\n",
       "           [ 1.0475,  0.6454, -0.5360,  ...,  0.3813, -0.3917,  0.1143]],\n",
       " \n",
       "          [[-0.0675,  0.7484,  0.2648,  ..., -1.2602,  0.8200,  0.2217],\n",
       "           [-0.0690, -0.9439,  0.2087,  ..., -0.1997,  0.2884, -0.4930],\n",
       "           [-0.2767, -0.1617, -0.2307,  ...,  0.0446,  0.4932, -0.1954],\n",
       "           ...,\n",
       "           [ 0.0654, -0.3014, -0.4509,  ...,  0.1535,  0.2551, -0.1833],\n",
       "           [-0.6034, -0.2008,  0.5652,  ..., -0.2992,  0.7283,  0.6020],\n",
       "           [-0.6328, -0.3579, -0.3556,  ..., -0.5443,  0.7007, -0.4696]],\n",
       " \n",
       "          [[ 0.5048, -0.9551,  0.0130,  ..., -0.5793,  0.4264,  0.2143],\n",
       "           [ 0.0663,  1.7418,  0.6615,  ..., -0.7452,  0.3694, -0.9369],\n",
       "           [ 0.3293,  0.1652,  0.3124,  ..., -0.5304,  0.4580, -0.5067],\n",
       "           ...,\n",
       "           [-1.0811,  0.1611, -1.1021,  ..., -1.1802,  0.1764,  0.0208],\n",
       "           [-1.3108,  0.1374, -0.5383,  ..., -0.7055,  0.0974,  0.9346],\n",
       "           [-1.7720, -0.0138, -1.4761,  ..., -1.0889,  0.5976, -0.1123]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-1.0822e-01,  5.8514e-01,  1.6628e-02,  ..., -4.4208e-01,\n",
       "            -2.7479e-01, -1.7634e-01],\n",
       "           [ 2.0707e-01,  5.7115e-01, -3.1767e-01,  ..., -4.1864e-01,\n",
       "            -3.3199e-01,  4.8352e-01],\n",
       "           [ 1.6920e+00, -5.4488e-01, -2.7893e-01,  ...,  8.5898e-01,\n",
       "            -8.3897e-01, -1.2962e+00],\n",
       "           ...,\n",
       "           [-1.5310e-01,  4.9291e-01, -4.1918e-01,  ...,  2.8879e-02,\n",
       "            -6.0335e-01,  2.4562e-01],\n",
       "           [-3.6529e-02, -3.7108e-01, -9.3264e-01,  ..., -1.9769e-01,\n",
       "            -1.8562e-02,  2.8350e-02],\n",
       "           [-8.7511e-01,  3.3414e-03, -3.8411e-01,  ..., -5.5413e-01,\n",
       "            -4.4077e-01,  4.6978e-02]],\n",
       " \n",
       "          [[ 1.7856e-01, -8.1775e-02, -4.4169e-01,  ...,  2.6068e-01,\n",
       "            -6.2402e-01, -5.5125e-02],\n",
       "           [ 2.1730e-01, -5.1722e-01, -7.4038e-01,  ...,  2.1456e-01,\n",
       "            -3.2586e-01,  1.9418e-01],\n",
       "           [ 2.6708e-01, -1.4639e+00, -6.1135e-01,  ...,  5.9838e-01,\n",
       "            -1.2267e+00,  6.2484e-01],\n",
       "           ...,\n",
       "           [-1.1211e-01, -1.7467e-02, -7.4887e-01,  ...,  7.3509e-01,\n",
       "            -1.3020e-01, -1.0365e-01],\n",
       "           [-1.9939e-01, -9.9113e-01, -4.2833e-01,  ...,  3.3815e-02,\n",
       "            -3.1419e-01, -2.6214e-01],\n",
       "           [-4.9212e-02, -1.8585e-01, -7.5819e-02,  ..., -5.4261e-02,\n",
       "            -1.5987e-01, -5.8789e-01]],\n",
       " \n",
       "          [[ 3.5432e-01,  2.0132e-01, -1.7988e-01,  ..., -1.3963e-01,\n",
       "            -4.8611e-01,  5.5800e-01],\n",
       "           [ 3.1317e-01,  5.0117e-01,  1.3053e-01,  ..., -6.9754e-01,\n",
       "            -9.1586e-01,  9.3719e-01],\n",
       "           [ 4.8926e-01, -3.3147e-01, -1.2404e-01,  ..., -1.5208e-01,\n",
       "            -5.4399e-01,  4.8947e-01],\n",
       "           ...,\n",
       "           [ 2.7615e-01, -5.0013e-01, -2.3675e-01,  ..., -1.4934e-03,\n",
       "             6.5129e-03, -3.3491e-01],\n",
       "           [ 1.0420e-01,  3.9416e-01,  8.5627e-03,  ..., -9.4080e-01,\n",
       "            -2.6319e-01, -8.6948e-01],\n",
       "           [-1.9290e-01, -1.4677e-01, -2.1171e-02,  ..., -2.0226e-01,\n",
       "             3.9206e-02, -5.2592e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.9287e-01, -9.9525e-01, -7.4259e-01,  ..., -1.1548e-01,\n",
       "            -4.1549e-02, -4.7417e-01],\n",
       "           [ 5.2118e-02, -3.6147e-01, -4.6307e-01,  ...,  4.9108e-02,\n",
       "            -3.5959e-01,  3.8710e-02],\n",
       "           [ 4.8674e-01, -4.9507e-02, -9.3411e-01,  ..., -3.1816e-02,\n",
       "             5.1502e-01,  2.4234e-01],\n",
       "           ...,\n",
       "           [-2.6251e-01, -1.6266e-01, -2.0694e-01,  ..., -1.7100e-01,\n",
       "             1.7757e-01,  4.1498e-01],\n",
       "           [-2.7717e-01,  3.3874e-01, -7.2997e-01,  ..., -4.2576e-01,\n",
       "            -7.9810e-01, -6.3720e-01],\n",
       "           [-3.5835e-01, -3.7151e-01, -9.8762e-01,  ..., -6.4460e-01,\n",
       "             1.1500e-01, -3.9240e-01]],\n",
       " \n",
       "          [[-1.1946e-02, -9.1835e-02,  2.5599e-01,  ...,  2.6656e-01,\n",
       "             5.6063e-01,  2.9552e-01],\n",
       "           [ 8.3672e-02,  7.7211e-03, -3.4742e-01,  ...,  9.2884e-02,\n",
       "             7.0182e-02,  4.2381e-01],\n",
       "           [-5.1509e-01,  3.6665e-01, -2.0950e-01,  ...,  2.2454e-01,\n",
       "            -1.0420e-01,  4.5070e-01],\n",
       "           ...,\n",
       "           [ 2.5492e-01, -3.0486e-01,  1.8774e-01,  ..., -2.6328e-01,\n",
       "             9.6595e-02,  5.5082e-01],\n",
       "           [ 3.4715e-02,  6.6445e-04,  1.8549e-01,  ...,  2.8214e-01,\n",
       "             4.4009e-02,  1.2213e+00],\n",
       "           [ 1.1942e-01, -3.1136e-01,  2.8166e-01,  ..., -8.5682e-02,\n",
       "            -3.8961e-01,  4.3607e-01]],\n",
       " \n",
       "          [[ 2.2071e-01,  5.9875e-01, -2.0199e-01,  ..., -1.0979e-02,\n",
       "            -1.0596e+00,  5.4311e-01],\n",
       "           [ 1.0036e-01,  1.7107e-01, -1.1672e-01,  ...,  5.0096e-01,\n",
       "            -7.5089e-01,  5.0852e-01],\n",
       "           [ 3.7648e-01,  2.1411e-01, -6.5581e-01,  ..., -2.9899e-01,\n",
       "            -1.0305e+00,  7.6757e-01],\n",
       "           ...,\n",
       "           [ 2.7674e-01, -4.4594e-02, -2.2300e-01,  ..., -2.3356e-03,\n",
       "             3.2452e-01, -5.7455e-02],\n",
       "           [ 6.7101e-01, -1.2281e-01, -7.8344e-01,  ...,  5.2032e-02,\n",
       "             1.1925e-01, -1.1062e-01],\n",
       "           [ 3.1752e-01, -6.2020e-01,  1.4861e-01,  ...,  5.3417e-01,\n",
       "            -7.6923e-03, -4.2149e-01]]]], device='cuda:0')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['past_key_value'].value_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练PQ并保存到硬盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss import IndexPQ\n",
    "import numpy as np\n",
    "\n",
    "M = 8\n",
    "nbits = 4\n",
    "dim = int(model.config.hidden_size / model.config.num_attention_heads)\n",
    "\n",
    "indices: List[List[IndexPQ]] = [\n",
    "    [IndexPQ(dim, M, nbits) for _ in range(model.config.num_attention_heads)] for _ in range(model.config.num_hidden_layers)\n",
    "]\n",
    "\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    for j in range(model.config.num_attention_heads):\n",
    "        tmp = model.model.layers[i].self_attn.middleware['value_states'][:, j, :4096, :].view(-1, 128).cpu().numpy()\n",
    "        # print(tmp.shape)\n",
    "        indices[i][j].train(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save indices to disk\n",
    "from faiss import write_index\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    for j in range(model.config.num_attention_heads):\n",
    "        index_filename = f\"./pq_index/pq_{i}_{j}.index\"\n",
    "        write_index(indices[i][j], index_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xupeng/miniconda3/envs/faiss/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv, LlamaDecoderLayer, LlamaMLP, LlamaRMSNorm, LlamaModel, LlamaSdpaAttention, LlamaPreTrainedModel\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "\n",
    "from faiss import IndexPQ, IndexFlatIP\n",
    "\n",
    "class KeyStateTensorMocker:\n",
    "    def __init__(self, key_states: Optional[torch.Tensor] = None) -> None:\n",
    "        self._cache = None\n",
    "        self._shape = [0] * 4\n",
    "\n",
    "        if key_states is not None:\n",
    "            bsz, num_heads, seq_len, head_dim = key_states.shape\n",
    "            # Initialize the cache list with IndexFlatIP instances\n",
    "            self._cache = [IndexFlatIP(head_dim) for _ in range(num_heads)]\n",
    "            self.cat(key_states)\n",
    "            # Store the shape when key_states is provided\n",
    "            self._shape = list(key_states.shape)\n",
    "            \n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Optional[Tuple[int, int, int, int]]:\n",
    "        # Return the shape if available\n",
    "        return tuple(self._shape)\n",
    "    \n",
    "    def cat(self, key_state: torch.Tensor):\n",
    "        bsz, num_heads, seq_len, head_dim = key_state.shape\n",
    "        for b in range(bsz):\n",
    "            for i in range(num_heads):\n",
    "                self._cache[i].add(key_state[b, i, :, :].cpu().numpy())\n",
    "\n",
    "        self._shape[2] += seq_len\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx: int) -> IndexFlatIP:\n",
    "        # print(\"idx\", idx)\n",
    "        return self._cache[idx]\n",
    "\n",
    "class DatabaseCache(DynamicCache):\n",
    "    def __init__(self) -> None:\n",
    "        self.key_cache : List[KeyStateTensorMocker] = []\n",
    "        self.value_cache : List[torch.Tensor] = []\n",
    "        self._debug_key_cache : List[torch.Tensor] = []\n",
    "        self._seen_tokens = 0\n",
    "    \n",
    "    def reorder_cache(self, beam_idx: torch.LongTensor):\n",
    "        raise NotImplementedError(\"Reordering the cache is not currently supported\")\n",
    "\n",
    "    def query(self, query_states, layer_idx):\n",
    "        '''\n",
    "        Basically implements SDPA with cache\n",
    "        '''\n",
    "        bsz, num_heads, query_len, head_dim = query_states.shape\n",
    "        seq_len = self._seen_tokens\n",
    "\n",
    "        assert bsz == 1, \"Batch size > 1 is not currently supported\"\n",
    "\n",
    "        attn_score = torch.zeros(bsz, num_heads, query_len, seq_len, device=query_states.device)\n",
    "        \n",
    "        # query the cache\n",
    "        # TODO: parallelize this\n",
    "        for b in range(bsz):\n",
    "            for h in range(num_heads):\n",
    "                D, I = self.key_cache[layer_idx][h].search(query_states[b, h, :, :].cpu().numpy(), seq_len) # TODO: specify k\n",
    "                # convert D to tensor\n",
    "                D = torch.tensor(D, device=query_states.device)\n",
    "                for (idx, cols) in enumerate(I):\n",
    "                    for (jdx, col) in enumerate(cols):\n",
    "                        attn_score[b, h, idx, col] = D[idx, jdx]\n",
    "        \n",
    "        # scale & softmax\n",
    "        scaling_factor = torch.sqrt(torch.tensor(head_dim))\n",
    "        attn_score = torch.softmax(attn_score / scaling_factor, dim=-1)\n",
    "\n",
    "        # weighted sum\n",
    "        attn_output = attn_score @ self.value_cache[layer_idx]\n",
    "        # attn_output = torch.zeros_like(query_states)\n",
    "        # for b in range(bsz):\n",
    "        #     for h in range(num_heads):\n",
    "        #         for i in range(query_len):\n",
    "        #             # Each output vector is a sum over all value vectors, weighted by the attention scores\n",
    "        #             for j in range(seq_len):\n",
    "        #                 attn_output[b, h, i, :] += attn_score[b, h, i, j] * self.value_cache[layer_idx][b, h, j, :]\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "    def update(self, key_states, value_states, layer_idx, cache_kwargs=None):\n",
    "        \n",
    "        # key_states is shaped (bsz, num_heads, seq_len, head_dim)\n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        bsz, num_heads, seq_len, head_dim = key_states.shape\n",
    "\n",
    "        # initialize the cache if it doesn't exist\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            self.key_cache.append(KeyStateTensorMocker(key_states))\n",
    "            self.value_cache.append(value_states)\n",
    "            # self._debug_key_cache.append(key_states)\n",
    "        else:\n",
    "            # update the cache\n",
    "            self.key_cache[layer_idx].cat(key_states)\n",
    "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "            # self._debug_key_cache[layer_idx] = torch.cat([self._debug_key_cache[layer_idx], key_states], dim=-2)\n",
    "\n",
    "       \n",
    "class LlamaForCausalLMDB(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "        self.model = LlamaModelDB(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "        self.middleware = {}\n",
    "        self.fwcall = 0\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        self.fwcall += 1\n",
    "        print(f\"fwcall: {self.fwcall}, key cache size(one layer): {past_key_values[0][0].shape if past_key_values is not None else None}\")\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "class LlamaModelDB(LlamaModel):\n",
    "    def __init__(self, config):\n",
    "        super(LlamaModel, self).__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayerDB(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        self.middleware = {}\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if self.gradient_checkpointing and self.training and use_cache:\n",
    "            use_cache = False\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        past_seen_tokens = 0\n",
    "        if use_cache:  # kept for BC (cache positions)\n",
    "            if not isinstance(past_key_values, StaticCache):\n",
    "                if past_key_values is None:\n",
    "                    past_key_values = DatabaseCache()\n",
    "                past_seen_tokens = past_key_values.get_seq_length()\n",
    "\n",
    "        if cache_position is None:\n",
    "            if isinstance(past_key_values, StaticCache):\n",
    "                raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds, cache_position, past_seen_tokens)\n",
    "\n",
    "        # embed positions\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    causal_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = None\n",
    "        if use_cache:\n",
    "            next_cache = next_decoder_cache\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "class LlamaDecoderLayerDB(LlamaDecoderLayer):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super(LlamaDecoderLayer, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # self.self_attn = LlamaSdpaAttention(config, layer_idx)\n",
    "        self.self_attn = LlamaSdpaAttentionDB(config, layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "class LlamaSdpaAttentionDB(LlamaSdpaAttention):\n",
    "    \"\"\"\n",
    "    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n",
    "    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n",
    "    SDPA API.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.middleware = {}\n",
    "\n",
    "    # Adapted from LlamaAttention.forward\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        if output_attentions:\n",
    "            return super().forward(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "            )\n",
    "        # self.middleware.update({\"hidden_states_input\" : hidden_states.clone()})\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            past_key_value.update(key_states, value_states, self.layer_idx)\n",
    "    \n",
    "        # sdpa is integrated with the cache\n",
    "        attn_output = past_key_value.query(query_states, self.layer_idx)\n",
    "\n",
    "        # to_compare = torch.nn.functional.scaled_dot_product_attention(\n",
    "        #     query_states, past_key_value._debug_key_cache[self.layer_idx], past_key_value.value_cache[self.layer_idx]\n",
    "        # )\n",
    "\n",
    "        self.middleware.update({\"query_states\" : query_states.clone()})\n",
    "        self.middleware.update({\"key_states\" : key_states.clone()})\n",
    "        self.middleware.update({\"value_states\" : value_states.clone()})\n",
    "        self.middleware.update({\"past_key_value\" : past_key_value})\n",
    "        # assert torch.allclose(attn_output, to_compare, atol=1e-5), f\"Mismatch between SDPA and cache query, layer: {self.layer_idx}\"\n",
    "\n",
    "        \n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        \n",
    "        return attn_output, None, past_key_value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用IndexFlatIP验证正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_states = m['key_states']\n",
    "value_states = m['value_states']\n",
    "query_states = m['query_states']\n",
    "\n",
    "sdpa = torch.nn.functional.scaled_dot_product_attention(\n",
    "    m['query_states'],\n",
    "    m['key_states'],\n",
    "    m['value_states']\n",
    ")\n",
    "\n",
    "dbcache = DatabaseCache()\n",
    "dbcache.update(key_states, value_states, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 0\n",
    "\n",
    "bsz, num_heads, query_len, head_dim = query_states.shape\n",
    "seq_len = dbcache._seen_tokens\n",
    "\n",
    "attn_score = torch.zeros(bsz, num_heads, query_len, seq_len)\n",
    "for b in range(bsz):\n",
    "    for h in range(num_heads):\n",
    "        D, I = dbcache.key_cache[layer_idx][h].search(query_states[b, h, :, :].cpu().numpy(), seq_len) # TODO: specify k\n",
    "        # convert D to tensor\n",
    "        D = torch.tensor(D, device=query_states.device)\n",
    "        for (idx, cols) in enumerate(I):\n",
    "            for (jdx, col) in enumerate(cols):\n",
    "                attn_score[b, h, idx, col] = D[idx, jdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_states[0, 0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = IndexFlatIP(head_dim)\n",
    "\n",
    "index.add(key_states[0, 0, :, :].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17.8612], device='cuda:0')\n",
      "17.861155\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(query_states[0, 0, :, :].cpu().numpy(), 5)\n",
    "\n",
    "print(query_states[0, 0, :, :] @ key_states[0, 0, I[0][0], :])\n",
    "print(D[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 17.8612]],\n",
       "\n",
       "         [[  2.9589]],\n",
       "\n",
       "         [[  4.6459]],\n",
       "\n",
       "         [[ 34.8256]],\n",
       "\n",
       "         [[ 26.3393]],\n",
       "\n",
       "         [[  4.4898]],\n",
       "\n",
       "         [[ 16.9595]],\n",
       "\n",
       "         [[-29.1442]],\n",
       "\n",
       "         [[ 15.6031]],\n",
       "\n",
       "         [[-16.4950]],\n",
       "\n",
       "         [[ 16.2660]],\n",
       "\n",
       "         [[ 18.6018]],\n",
       "\n",
       "         [[ 40.9244]],\n",
       "\n",
       "         [[-12.6539]],\n",
       "\n",
       "         [[ -3.8843]],\n",
       "\n",
       "         [[ 12.1046]],\n",
       "\n",
       "         [[ 12.3422]],\n",
       "\n",
       "         [[  9.4559]],\n",
       "\n",
       "         [[ 21.0365]],\n",
       "\n",
       "         [[ 36.2610]],\n",
       "\n",
       "         [[ 11.7032]],\n",
       "\n",
       "         [[ -4.3956]],\n",
       "\n",
       "         [[ 24.3023]],\n",
       "\n",
       "         [[ 17.2784]],\n",
       "\n",
       "         [[  3.4854]],\n",
       "\n",
       "         [[ 10.2540]],\n",
       "\n",
       "         [[ 16.6599]],\n",
       "\n",
       "         [[ -8.6022]],\n",
       "\n",
       "         [[ 16.4774]],\n",
       "\n",
       "         [[ 16.5451]],\n",
       "\n",
       "         [[-14.7055]],\n",
       "\n",
       "         [[  1.9440]]]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(query_states, key_states.transpose(-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 17.8612]],\n",
       "\n",
       "         [[  2.9589]],\n",
       "\n",
       "         [[  4.6459]],\n",
       "\n",
       "         [[ 34.8256]],\n",
       "\n",
       "         [[ 26.3393]],\n",
       "\n",
       "         [[  4.4898]],\n",
       "\n",
       "         [[ 16.9595]],\n",
       "\n",
       "         [[-29.1442]],\n",
       "\n",
       "         [[ 15.6031]],\n",
       "\n",
       "         [[-16.4950]],\n",
       "\n",
       "         [[ 16.2660]],\n",
       "\n",
       "         [[ 18.6018]],\n",
       "\n",
       "         [[ 40.9244]],\n",
       "\n",
       "         [[-12.6539]],\n",
       "\n",
       "         [[ -3.8843]],\n",
       "\n",
       "         [[ 12.1046]],\n",
       "\n",
       "         [[ 12.3422]],\n",
       "\n",
       "         [[  9.4559]],\n",
       "\n",
       "         [[ 21.0365]],\n",
       "\n",
       "         [[ 36.2610]],\n",
       "\n",
       "         [[ 11.7032]],\n",
       "\n",
       "         [[ -4.3956]],\n",
       "\n",
       "         [[ 24.3023]],\n",
       "\n",
       "         [[ 17.2784]],\n",
       "\n",
       "         [[  3.4854]],\n",
       "\n",
       "         [[ 10.2540]],\n",
       "\n",
       "         [[ 16.6599]],\n",
       "\n",
       "         [[ -8.6022]],\n",
       "\n",
       "         [[ 16.4774]],\n",
       "\n",
       "         [[ 16.5451]],\n",
       "\n",
       "         [[-14.7055]],\n",
       "\n",
       "         [[  1.9440]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale & softmax\n",
    "scaling_factor = torch.sqrt(torch.tensor(head_dim))\n",
    "attn_score = torch.softmax(attn_score / scaling_factor, dim=-1)\n",
    "\n",
    "# weighted sum\n",
    "attn_output = torch.zeros_like(query_states)\n",
    "for b in range(bsz):\n",
    "    for h in range(num_heads):\n",
    "        for i in range(query_len):\n",
    "            # Each output vector is a sum over all value vectors, weighted by the attention scores\n",
    "            for j in range(seq_len):\n",
    "                attn_output[b, h, i, :] += attn_score[b, h, i, j] * dbcache.value_cache[layer_idx][b, h, j, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output2 = attn_score @ dbcache.value_cache[layer_idx].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 7.5211e-03, -1.0375e-02,  3.5426e-03,  ..., -1.8492e-03,\n",
      "            3.9925e-03, -4.3669e-03]],\n",
      "\n",
      "         [[ 2.1980e-03,  3.3925e-03, -2.4338e-03,  ...,  2.2310e-03,\n",
      "            2.4552e-03,  5.0936e-03]],\n",
      "\n",
      "         [[-4.0987e-03, -3.7784e-03,  3.0119e-03,  ...,  7.2256e-04,\n",
      "           -4.6644e-05,  9.7175e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7973e-02, -6.9194e-03, -1.7842e-02,  ...,  7.5553e-03,\n",
      "            3.7649e-03, -1.2968e-02]],\n",
      "\n",
      "         [[-1.0510e-02,  1.2955e-03,  8.9638e-04,  ...,  7.3972e-04,\n",
      "            3.3017e-04,  4.8898e-03]],\n",
      "\n",
      "         [[-7.9546e-03,  8.9661e-03,  1.2278e-03,  ...,  4.6792e-05,\n",
      "            1.1711e-03, -9.9407e-03]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 7.5211e-03, -1.0375e-02,  3.5426e-03,  ..., -1.8492e-03,\n",
      "            3.9925e-03, -4.3669e-03]],\n",
      "\n",
      "         [[ 2.1980e-03,  3.3925e-03, -2.4338e-03,  ...,  2.2310e-03,\n",
      "            2.4552e-03,  5.0936e-03]],\n",
      "\n",
      "         [[-4.0987e-03, -3.7784e-03,  3.0119e-03,  ...,  7.2256e-04,\n",
      "           -4.6644e-05,  9.7175e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7973e-02, -6.9194e-03, -1.7842e-02,  ...,  7.5553e-03,\n",
      "            3.7649e-03, -1.2968e-02]],\n",
      "\n",
      "         [[-1.0510e-02,  1.2955e-03,  8.9638e-04,  ...,  7.3972e-04,\n",
      "            3.3017e-04,  4.8898e-03]],\n",
      "\n",
      "         [[-7.9546e-03,  8.9661e-03,  1.2278e-03,  ...,  4.6792e-05,\n",
      "            1.1711e-03, -9.9407e-03]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(sdpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 7.5211e-03, -1.0375e-02,  3.5426e-03,  ..., -1.8492e-03,\n",
      "            3.9925e-03, -4.3669e-03]],\n",
      "\n",
      "         [[ 2.1980e-03,  3.3925e-03, -2.4338e-03,  ...,  2.2310e-03,\n",
      "            2.4552e-03,  5.0936e-03]],\n",
      "\n",
      "         [[-4.0987e-03, -3.7784e-03,  3.0119e-03,  ...,  7.2256e-04,\n",
      "           -4.6644e-05,  9.7175e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7973e-02, -6.9194e-03, -1.7842e-02,  ...,  7.5553e-03,\n",
      "            3.7649e-03, -1.2968e-02]],\n",
      "\n",
      "         [[-1.0510e-02,  1.2955e-03,  8.9638e-04,  ...,  7.3972e-04,\n",
      "            3.3017e-04,  4.8898e-03]],\n",
      "\n",
      "         [[-7.9546e-03,  8.9661e-03,  1.2278e-03,  ...,  4.6792e-05,\n",
      "            1.1711e-03, -9.9407e-03]]]])\n"
     ]
    }
   ],
   "source": [
    "print(attn_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用pipeline进行文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "modelDB = LlamaForCausalLMDB.from_pretrained(\"llama-2-7b-hf\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaForCausalLMDB' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwcall: 1, key cache size(one layer): None\n",
      "fwcall: 2, key cache size(one layer): (1, 32, 8, 128)\n",
      "fwcall: 3, key cache size(one layer): (1, 32, 9, 128)\n",
      "fwcall: 4, key cache size(one layer): (1, 32, 10, 128)\n",
      "fwcall: 5, key cache size(one layer): (1, 32, 11, 128)\n",
      "fwcall: 6, key cache size(one layer): (1, 32, 12, 128)\n",
      "fwcall: 7, key cache size(one layer): (1, 32, 13, 128)\n",
      "fwcall: 8, key cache size(one layer): (1, 32, 14, 128)\n",
      "fwcall: 9, key cache size(one layer): (1, 32, 15, 128)\n",
      "fwcall: 10, key cache size(one layer): (1, 32, 16, 128)\n",
      "fwcall: 11, key cache size(one layer): (1, 32, 17, 128)\n",
      "fwcall: 12, key cache size(one layer): (1, 32, 18, 128)\n",
      "fwcall: 13, key cache size(one layer): (1, 32, 19, 128)\n",
      "fwcall: 14, key cache size(one layer): (1, 32, 20, 128)\n",
      "fwcall: 15, key cache size(one layer): (1, 32, 21, 128)\n",
      "fwcall: 16, key cache size(one layer): (1, 32, 22, 128)\n",
      "fwcall: 17, key cache size(one layer): (1, 32, 23, 128)\n",
      "fwcall: 18, key cache size(one layer): (1, 32, 24, 128)\n",
      "fwcall: 19, key cache size(one layer): (1, 32, 25, 128)\n",
      "fwcall: 20, key cache size(one layer): (1, 32, 26, 128)\n",
      "fwcall: 21, key cache size(one layer): (1, 32, 27, 128)\n",
      "fwcall: 22, key cache size(one layer): (1, 32, 28, 128)\n",
      "fwcall: 23, key cache size(one layer): (1, 32, 29, 128)\n",
      "fwcall: 24, key cache size(one layer): (1, 32, 30, 128)\n",
      "fwcall: 25, key cache size(one layer): (1, 32, 31, 128)\n",
      "fwcall: 26, key cache size(one layer): (1, 32, 32, 128)\n",
      "fwcall: 27, key cache size(one layer): (1, 32, 33, 128)\n",
      "fwcall: 28, key cache size(one layer): (1, 32, 34, 128)\n",
      "fwcall: 29, key cache size(one layer): (1, 32, 35, 128)\n",
      "fwcall: 30, key cache size(one layer): (1, 32, 36, 128)\n",
      "fwcall: 31, key cache size(one layer): (1, 32, 37, 128)\n",
      "fwcall: 32, key cache size(one layer): (1, 32, 38, 128)\n",
      "fwcall: 33, key cache size(one layer): (1, 32, 39, 128)\n",
      "fwcall: 34, key cache size(one layer): (1, 32, 40, 128)\n",
      "fwcall: 35, key cache size(one layer): (1, 32, 41, 128)\n",
      "fwcall: 36, key cache size(one layer): (1, 32, 42, 128)\n",
      "fwcall: 37, key cache size(one layer): (1, 32, 43, 128)\n",
      "fwcall: 38, key cache size(one layer): (1, 32, 44, 128)\n",
      "fwcall: 39, key cache size(one layer): (1, 32, 45, 128)\n",
      "fwcall: 40, key cache size(one layer): (1, 32, 46, 128)\n",
      "fwcall: 41, key cache size(one layer): (1, 32, 47, 128)\n",
      "fwcall: 42, key cache size(one layer): (1, 32, 48, 128)\n",
      "fwcall: 43, key cache size(one layer): (1, 32, 49, 128)\n",
      "fwcall: 44, key cache size(one layer): (1, 32, 50, 128)\n",
      "fwcall: 45, key cache size(one layer): (1, 32, 51, 128)\n",
      "fwcall: 46, key cache size(one layer): (1, 32, 52, 128)\n",
      "fwcall: 47, key cache size(one layer): (1, 32, 53, 128)\n",
      "fwcall: 48, key cache size(one layer): (1, 32, 54, 128)\n",
      "fwcall: 49, key cache size(one layer): (1, 32, 55, 128)\n",
      "fwcall: 50, key cache size(one layer): (1, 32, 56, 128)\n",
      "fwcall: 51, key cache size(one layer): (1, 32, 57, 128)\n",
      "fwcall: 52, key cache size(one layer): (1, 32, 58, 128)\n",
      "fwcall: 53, key cache size(one layer): (1, 32, 59, 128)\n",
      "fwcall: 54, key cache size(one layer): (1, 32, 60, 128)\n",
      "fwcall: 55, key cache size(one layer): (1, 32, 61, 128)\n",
      "fwcall: 56, key cache size(one layer): (1, 32, 62, 128)\n",
      "fwcall: 57, key cache size(one layer): (1, 32, 63, 128)\n",
      "fwcall: 58, key cache size(one layer): (1, 32, 64, 128)\n",
      "fwcall: 59, key cache size(one layer): (1, 32, 65, 128)\n",
      "fwcall: 60, key cache size(one layer): (1, 32, 66, 128)\n",
      "fwcall: 61, key cache size(one layer): (1, 32, 67, 128)\n",
      "fwcall: 62, key cache size(one layer): (1, 32, 68, 128)\n",
      "fwcall: 63, key cache size(one layer): (1, 32, 69, 128)\n",
      "fwcall: 64, key cache size(one layer): (1, 32, 70, 128)\n",
      "fwcall: 65, key cache size(one layer): (1, 32, 71, 128)\n",
      "fwcall: 66, key cache size(one layer): (1, 32, 72, 128)\n",
      "fwcall: 67, key cache size(one layer): (1, 32, 73, 128)\n",
      "fwcall: 68, key cache size(one layer): (1, 32, 74, 128)\n",
      "fwcall: 69, key cache size(one layer): (1, 32, 75, 128)\n",
      "fwcall: 70, key cache size(one layer): (1, 32, 76, 128)\n",
      "fwcall: 71, key cache size(one layer): (1, 32, 77, 128)\n",
      "fwcall: 72, key cache size(one layer): (1, 32, 78, 128)\n",
      "fwcall: 73, key cache size(one layer): (1, 32, 79, 128)\n",
      "fwcall: 74, key cache size(one layer): (1, 32, 80, 128)\n",
      "fwcall: 75, key cache size(one layer): (1, 32, 81, 128)\n",
      "fwcall: 76, key cache size(one layer): (1, 32, 82, 128)\n",
      "fwcall: 77, key cache size(one layer): (1, 32, 83, 128)\n",
      "fwcall: 78, key cache size(one layer): (1, 32, 84, 128)\n",
      "fwcall: 79, key cache size(one layer): (1, 32, 85, 128)\n",
      "fwcall: 80, key cache size(one layer): (1, 32, 86, 128)\n",
      "fwcall: 81, key cache size(one layer): (1, 32, 87, 128)\n",
      "fwcall: 82, key cache size(one layer): (1, 32, 88, 128)\n",
      "fwcall: 83, key cache size(one layer): (1, 32, 89, 128)\n",
      "fwcall: 84, key cache size(one layer): (1, 32, 90, 128)\n",
      "fwcall: 85, key cache size(one layer): (1, 32, 91, 128)\n",
      "fwcall: 86, key cache size(one layer): (1, 32, 92, 128)\n",
      "fwcall: 87, key cache size(one layer): (1, 32, 93, 128)\n",
      "fwcall: 88, key cache size(one layer): (1, 32, 94, 128)\n",
      "fwcall: 89, key cache size(one layer): (1, 32, 95, 128)\n",
      "fwcall: 90, key cache size(one layer): (1, 32, 96, 128)\n",
      "fwcall: 91, key cache size(one layer): (1, 32, 97, 128)\n",
      "fwcall: 92, key cache size(one layer): (1, 32, 98, 128)\n",
      "fwcall: 93, key cache size(one layer): (1, 32, 99, 128)\n",
      "fwcall: 94, key cache size(one layer): (1, 32, 100, 128)\n",
      "fwcall: 95, key cache size(one layer): (1, 32, 101, 128)\n",
      "fwcall: 96, key cache size(one layer): (1, 32, 102, 128)\n",
      "fwcall: 97, key cache size(one layer): (1, 32, 103, 128)\n",
      "fwcall: 98, key cache size(one layer): (1, 32, 104, 128)\n",
      "fwcall: 99, key cache size(one layer): (1, 32, 105, 128)\n",
      "fwcall: 100, key cache size(one layer): (1, 32, 106, 128)\n",
      "fwcall: 101, key cache size(one layer): (1, 32, 107, 128)\n",
      "fwcall: 102, key cache size(one layer): (1, 32, 108, 128)\n",
      "fwcall: 103, key cache size(one layer): (1, 32, 109, 128)\n",
      "fwcall: 104, key cache size(one layer): (1, 32, 110, 128)\n",
      "fwcall: 105, key cache size(one layer): (1, 32, 111, 128)\n",
      "fwcall: 106, key cache size(one layer): (1, 32, 112, 128)\n",
      "fwcall: 107, key cache size(one layer): (1, 32, 113, 128)\n",
      "fwcall: 108, key cache size(one layer): (1, 32, 114, 128)\n",
      "fwcall: 109, key cache size(one layer): (1, 32, 115, 128)\n",
      "fwcall: 110, key cache size(one layer): (1, 32, 116, 128)\n",
      "fwcall: 111, key cache size(one layer): (1, 32, 117, 128)\n",
      "fwcall: 112, key cache size(one layer): (1, 32, 118, 128)\n",
      "fwcall: 113, key cache size(one layer): (1, 32, 119, 128)\n",
      "fwcall: 114, key cache size(one layer): (1, 32, 120, 128)\n",
      "fwcall: 115, key cache size(one layer): (1, 32, 121, 128)\n",
      "fwcall: 116, key cache size(one layer): (1, 32, 122, 128)\n",
      "fwcall: 117, key cache size(one layer): (1, 32, 123, 128)\n",
      "fwcall: 118, key cache size(one layer): (1, 32, 124, 128)\n",
      "fwcall: 119, key cache size(one layer): (1, 32, 125, 128)\n",
      "fwcall: 120, key cache size(one layer): (1, 32, 126, 128)\n",
      "fwcall: 121, key cache size(one layer): (1, 32, 127, 128)\n",
      "fwcall: 122, key cache size(one layer): (1, 32, 128, 128)\n",
      "fwcall: 123, key cache size(one layer): (1, 32, 129, 128)\n",
      "fwcall: 124, key cache size(one layer): (1, 32, 130, 128)\n",
      "fwcall: 125, key cache size(one layer): (1, 32, 131, 128)\n",
      "fwcall: 126, key cache size(one layer): (1, 32, 132, 128)\n",
      "fwcall: 127, key cache size(one layer): (1, 32, 133, 128)\n",
      "fwcall: 128, key cache size(one layer): (1, 32, 134, 128)\n",
      "fwcall: 129, key cache size(one layer): (1, 32, 135, 128)\n",
      "fwcall: 130, key cache size(one layer): (1, 32, 136, 128)\n",
      "fwcall: 131, key cache size(one layer): (1, 32, 137, 128)\n",
      "fwcall: 132, key cache size(one layer): (1, 32, 138, 128)\n",
      "fwcall: 133, key cache size(one layer): (1, 32, 139, 128)\n",
      "fwcall: 134, key cache size(one layer): (1, 32, 140, 128)\n",
      "fwcall: 135, key cache size(one layer): (1, 32, 141, 128)\n",
      "fwcall: 136, key cache size(one layer): (1, 32, 142, 128)\n",
      "fwcall: 137, key cache size(one layer): (1, 32, 143, 128)\n",
      "fwcall: 138, key cache size(one layer): (1, 32, 144, 128)\n",
      "fwcall: 139, key cache size(one layer): (1, 32, 145, 128)\n",
      "fwcall: 140, key cache size(one layer): (1, 32, 146, 128)\n",
      "fwcall: 141, key cache size(one layer): (1, 32, 147, 128)\n",
      "fwcall: 142, key cache size(one layer): (1, 32, 148, 128)\n",
      "fwcall: 143, key cache size(one layer): (1, 32, 149, 128)\n",
      "fwcall: 144, key cache size(one layer): (1, 32, 150, 128)\n",
      "fwcall: 145, key cache size(one layer): (1, 32, 151, 128)\n",
      "fwcall: 146, key cache size(one layer): (1, 32, 152, 128)\n",
      "fwcall: 147, key cache size(one layer): (1, 32, 153, 128)\n",
      "fwcall: 148, key cache size(one layer): (1, 32, 154, 128)\n",
      "fwcall: 149, key cache size(one layer): (1, 32, 155, 128)\n",
      "fwcall: 150, key cache size(one layer): (1, 32, 156, 128)\n",
      "fwcall: 151, key cache size(one layer): (1, 32, 157, 128)\n",
      "fwcall: 152, key cache size(one layer): (1, 32, 158, 128)\n",
      "fwcall: 153, key cache size(one layer): (1, 32, 159, 128)\n",
      "fwcall: 154, key cache size(one layer): (1, 32, 160, 128)\n",
      "fwcall: 155, key cache size(one layer): (1, 32, 161, 128)\n",
      "fwcall: 156, key cache size(one layer): (1, 32, 162, 128)\n",
      "fwcall: 157, key cache size(one layer): (1, 32, 163, 128)\n",
      "fwcall: 158, key cache size(one layer): (1, 32, 164, 128)\n",
      "fwcall: 159, key cache size(one layer): (1, 32, 165, 128)\n",
      "fwcall: 160, key cache size(one layer): (1, 32, 166, 128)\n",
      "fwcall: 161, key cache size(one layer): (1, 32, 167, 128)\n",
      "fwcall: 162, key cache size(one layer): (1, 32, 168, 128)\n",
      "fwcall: 163, key cache size(one layer): (1, 32, 169, 128)\n",
      "fwcall: 164, key cache size(one layer): (1, 32, 170, 128)\n",
      "fwcall: 165, key cache size(one layer): (1, 32, 171, 128)\n",
      "fwcall: 166, key cache size(one layer): (1, 32, 172, 128)\n",
      "fwcall: 167, key cache size(one layer): (1, 32, 173, 128)\n",
      "fwcall: 168, key cache size(one layer): (1, 32, 174, 128)\n",
      "fwcall: 169, key cache size(one layer): (1, 32, 175, 128)\n",
      "fwcall: 170, key cache size(one layer): (1, 32, 176, 128)\n",
      "fwcall: 171, key cache size(one layer): (1, 32, 177, 128)\n",
      "fwcall: 172, key cache size(one layer): (1, 32, 178, 128)\n",
      "fwcall: 173, key cache size(one layer): (1, 32, 179, 128)\n",
      "fwcall: 174, key cache size(one layer): (1, 32, 180, 128)\n",
      "fwcall: 175, key cache size(one layer): (1, 32, 181, 128)\n",
      "fwcall: 176, key cache size(one layer): (1, 32, 182, 128)\n",
      "fwcall: 177, key cache size(one layer): (1, 32, 183, 128)\n",
      "fwcall: 178, key cache size(one layer): (1, 32, 184, 128)\n",
      "fwcall: 179, key cache size(one layer): (1, 32, 185, 128)\n",
      "fwcall: 180, key cache size(one layer): (1, 32, 186, 128)\n",
      "fwcall: 181, key cache size(one layer): (1, 32, 187, 128)\n",
      "fwcall: 182, key cache size(one layer): (1, 32, 188, 128)\n",
      "fwcall: 183, key cache size(one layer): (1, 32, 189, 128)\n",
      "fwcall: 184, key cache size(one layer): (1, 32, 190, 128)\n",
      "fwcall: 185, key cache size(one layer): (1, 32, 191, 128)\n",
      "fwcall: 186, key cache size(one layer): (1, 32, 192, 128)\n",
      "fwcall: 187, key cache size(one layer): (1, 32, 193, 128)\n",
      "fwcall: 188, key cache size(one layer): (1, 32, 194, 128)\n",
      "fwcall: 189, key cache size(one layer): (1, 32, 195, 128)\n",
      "fwcall: 190, key cache size(one layer): (1, 32, 196, 128)\n",
      "fwcall: 191, key cache size(one layer): (1, 32, 197, 128)\n",
      "fwcall: 192, key cache size(one layer): (1, 32, 198, 128)\n",
      "fwcall: 193, key cache size(one layer): (1, 32, 199, 128)\n",
      "fwcall: 194, key cache size(one layer): (1, 32, 200, 128)\n",
      "fwcall: 195, key cache size(one layer): (1, 32, 201, 128)\n",
      "fwcall: 196, key cache size(one layer): (1, 32, 202, 128)\n",
      "fwcall: 197, key cache size(one layer): (1, 32, 203, 128)\n",
      "fwcall: 198, key cache size(one layer): (1, 32, 204, 128)\n",
      "fwcall: 199, key cache size(one layer): (1, 32, 205, 128)\n",
      "fwcall: 200, key cache size(one layer): (1, 32, 206, 128)\n",
      "fwcall: 201, key cache size(one layer): (1, 32, 207, 128)\n",
      "fwcall: 202, key cache size(one layer): (1, 32, 208, 128)\n",
      "fwcall: 203, key cache size(one layer): (1, 32, 209, 128)\n",
      "fwcall: 204, key cache size(one layer): (1, 32, 210, 128)\n",
      "fwcall: 205, key cache size(one layer): (1, 32, 211, 128)\n",
      "fwcall: 206, key cache size(one layer): (1, 32, 212, 128)\n",
      "fwcall: 207, key cache size(one layer): (1, 32, 213, 128)\n",
      "fwcall: 208, key cache size(one layer): (1, 32, 214, 128)\n",
      "fwcall: 209, key cache size(one layer): (1, 32, 215, 128)\n",
      "fwcall: 210, key cache size(one layer): (1, 32, 216, 128)\n",
      "fwcall: 211, key cache size(one layer): (1, 32, 217, 128)\n",
      "fwcall: 212, key cache size(one layer): (1, 32, 218, 128)\n",
      "fwcall: 213, key cache size(one layer): (1, 32, 219, 128)\n",
      "fwcall: 214, key cache size(one layer): (1, 32, 220, 128)\n",
      "fwcall: 215, key cache size(one layer): (1, 32, 221, 128)\n",
      "fwcall: 216, key cache size(one layer): (1, 32, 222, 128)\n",
      "fwcall: 217, key cache size(one layer): (1, 32, 223, 128)\n",
      "fwcall: 218, key cache size(one layer): (1, 32, 224, 128)\n",
      "fwcall: 219, key cache size(one layer): (1, 32, 225, 128)\n",
      "fwcall: 220, key cache size(one layer): (1, 32, 226, 128)\n",
      "fwcall: 221, key cache size(one layer): (1, 32, 227, 128)\n",
      "fwcall: 222, key cache size(one layer): (1, 32, 228, 128)\n",
      "fwcall: 223, key cache size(one layer): (1, 32, 229, 128)\n",
      "fwcall: 224, key cache size(one layer): (1, 32, 230, 128)\n",
      "fwcall: 225, key cache size(one layer): (1, 32, 231, 128)\n",
      "fwcall: 226, key cache size(one layer): (1, 32, 232, 128)\n",
      "fwcall: 227, key cache size(one layer): (1, 32, 233, 128)\n",
      "fwcall: 228, key cache size(one layer): (1, 32, 234, 128)\n",
      "fwcall: 229, key cache size(one layer): (1, 32, 235, 128)\n",
      "fwcall: 230, key cache size(one layer): (1, 32, 236, 128)\n",
      "fwcall: 231, key cache size(one layer): (1, 32, 237, 128)\n",
      "fwcall: 232, key cache size(one layer): (1, 32, 238, 128)\n",
      "fwcall: 233, key cache size(one layer): (1, 32, 239, 128)\n",
      "fwcall: 234, key cache size(one layer): (1, 32, 240, 128)\n",
      "fwcall: 235, key cache size(one layer): (1, 32, 241, 128)\n",
      "fwcall: 236, key cache size(one layer): (1, 32, 242, 128)\n",
      "fwcall: 237, key cache size(one layer): (1, 32, 243, 128)\n",
      "fwcall: 238, key cache size(one layer): (1, 32, 244, 128)\n",
      "fwcall: 239, key cache size(one layer): (1, 32, 245, 128)\n",
      "fwcall: 240, key cache size(one layer): (1, 32, 246, 128)\n",
      "fwcall: 241, key cache size(one layer): (1, 32, 247, 128)\n",
      "fwcall: 242, key cache size(one layer): (1, 32, 248, 128)\n",
      "fwcall: 243, key cache size(one layer): (1, 32, 249, 128)\n",
      "fwcall: 244, key cache size(one layer): (1, 32, 250, 128)\n",
      "fwcall: 245, key cache size(one layer): (1, 32, 251, 128)\n",
      "fwcall: 246, key cache size(one layer): (1, 32, 252, 128)\n",
      "fwcall: 247, key cache size(one layer): (1, 32, 253, 128)\n",
      "fwcall: 248, key cache size(one layer): (1, 32, 254, 128)\n",
      "[{'generated_text': 'SJTU SEIEE isOjOOt?ON5OTOOOO\\x00BCO.lh. It\\nwas the first time in\\nhis 17 years with. the\\nU.S. Marine Corps.\\nLt. Col. Michael Pryor, CAP\\'s chief of staff, served\\nin the United States Army from 1998 to 2004.\\nPryor completed a combat tour of Iraq, serving as\\na civil-military affairs officer in the Al-Anbar\\nProvince. \"Serving a combat tour in Iraq was a\\nlife-changing event for me,\" he said.\\nIn early 2006, as Iraq was beginning what would\\nbe a violent year of sectarian violence, Pryor\\nconcluded his military career. He joined CAP that\\nsame year, where he has stayed for almost eight\\nyears. \"Serving in the military provided me with a\\ngreat opportunity,\" he said. \"I was able to serve\\nmy country, meet amazing people from all walks\\nof life, see new places and do what I felt was'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# device = \"cpu\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=modelDB,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = device,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "sequence = \"SJTU SEIEE is\"\n",
    "\n",
    "output = pipe(sequence, max_length=256, do_sample=True, temperature=0.9)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = modelDB.model.layers[0].self_attn.middleware\n",
    "cache = m['past_key_value']\n",
    "query_states = m['query_states']\n",
    "key_states = m['key_states']\n",
    "value_states = m['value_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 8, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 8, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query_states, key_states, value_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_compare.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = cache.query(query_states, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 3.6696e-03, -7.9229e-03,  8.4558e-03,  ..., -4.5834e-03,\n",
      "           -2.3658e-03, -1.7671e-04]],\n",
      "\n",
      "         [[ 4.3845e-04, -5.5317e-03,  5.8105e-03,  ...,  8.9691e-03,\n",
      "           -4.0985e-04, -5.5041e-03]],\n",
      "\n",
      "         [[-6.8097e-03,  3.5033e-03,  4.1462e-03,  ...,  5.9926e-03,\n",
      "            2.3143e-04, -2.0504e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8282e-02,  4.3157e-02, -1.2844e-02,  ...,  3.6885e-02,\n",
      "            7.5483e-03, -1.6709e-03]],\n",
      "\n",
      "         [[ 9.6602e-05,  6.6361e-03,  8.9406e-03,  ..., -1.1882e-02,\n",
      "           -5.2853e-03,  1.6323e-02]],\n",
      "\n",
      "         [[ 3.4548e-03,  4.0896e-03, -1.1464e-02,  ..., -3.3808e-04,\n",
      "            4.1257e-03,  6.8261e-03]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.3781e-03, -5.7980e-04,  2.9330e-03,  ...,  7.4228e-04,\n",
      "           -1.4491e-03, -1.3022e-04]],\n",
      "\n",
      "         [[-2.8401e-03, -1.2746e-03, -3.7840e-03,  ...,  2.6666e-03,\n",
      "           -4.3657e-04, -1.0268e-03]],\n",
      "\n",
      "         [[-3.1803e-04,  1.7788e-03,  2.1568e-03,  ...,  1.2615e-03,\n",
      "            2.9185e-03,  8.2977e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4362e-02, -2.4401e-03, -1.8101e-02,  ...,  2.2499e-02,\n",
      "           -1.3045e-02,  7.9051e-03]],\n",
      "\n",
      "         [[ 4.3792e-03,  4.5733e-03, -9.5786e-04,  ...,  1.0060e-03,\n",
      "           -8.3106e-05,  2.5171e-03]],\n",
      "\n",
      "         [[ 1.1982e-04,  3.7192e-03, -1.3928e-03,  ...,  3.4574e-03,\n",
      "            1.4829e-03,  1.6591e-03]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss import IndexPQ\n",
    "index = IndexPQ(128, 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m['value_states'].shape)\n",
    "\n",
    "indices = [IndexPQ(128, 8, 4) for _ in range(32)]\n",
    "\n",
    "for i, index in enumerate(indices):\n",
    "    tmp = m['value_states'][:, i, :, :].view(-1, 128).cpu().numpy()\n",
    "    index.train(tmp)\n",
    "    index.add(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['value_states'][:, i, :, :].view(-1, 128).cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(m['value_states'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"SEIEE in SJTU is\"\n",
    "output = pipe(sequence, max_length=512, do_sample=True, temperature=0.9)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从这里我们可以看到，一个新的文本生成任务会使用一个新的缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m['past_key_value']._seen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['hidden_states_input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__call__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 乘法器优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "    \"\"\"\n",
    "    tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "    new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "    return tensor.view(new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"k.shape\", m['past_key_value'].key_cache[0].shape)\n",
    "print(\"v.shape\", m['past_key_value'].value_cache[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of q, k\n",
    "q = m['past_key_value'].key_cache[0].clone()\n",
    "k = m['past_key_value'].value_cache[0].clone()\n",
    "\n",
    "# merge heads\n",
    "num_heads = 32\n",
    "attn_head_size = 128\n",
    "q = _merge_heads(q, num_heads, attn_head_size)\n",
    "k = _merge_heads(k, num_heads, attn_head_size)\n",
    "\n",
    "print(\"q.shape\", q.shape)\n",
    "print(\"k.shape\", k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming q and k are defined as torch.Tensor with the given shapes\n",
    "# q.shape == torch.Size([1, 357, 4096])\n",
    "# k.shape == torch.Size([1, 357, 4096])\n",
    "\n",
    "# Expand q and k to prepare for element-wise multiplication\n",
    "# Expand q to [1, 357, 1, 4096] and k to [1, 1, 357, 4096]\n",
    "q_expanded = q.unsqueeze(2)  # Adding a singleton dimension for broadcasting\n",
    "k_expanded = k.unsqueeze(1)  # Adding a different singleton dimension for broadcasting\n",
    "\n",
    "# Element-wise multiplication\n",
    "# Result shape will be [1, 357, 357, 4096], capturing each multiplication\n",
    "multiplication_results = q_expanded * k_expanded\n",
    "\n",
    "# To verify, let's examine the shape\n",
    "print(\"Shape of multiplication_results:\", multiplication_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplication_results.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multiplication_results.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(multiplication_results.reshape(-1).cpu().numpy(), bins=100, log=True)\n",
    "# cdf of abs(multiplication_results)\n",
    "\n",
    "plt.hist(multiplication_results.reshape(-1).abs().cpu().numpy(), bins=100, cumulative=True, density=True, histtype='step')\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.title(f\"Q*K^T, {q.shape[1]} * {q.shape[2]} * {k.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 32\n",
    "attn_head_size = 128\n",
    "\n",
    "# store multiplication results in a list\n",
    "multiplication_results = []\n",
    "\n",
    "\n",
    "# for i, layer in enumerate(model.model.layers):\n",
    "for i in [0,15,31]:\n",
    "    layer = model.model.layers[i]\n",
    "    \n",
    "\n",
    "    m = layer.self_attn.middleware\n",
    "    \n",
    "    q = m['past_key_value'].key_cache[0].clone()\n",
    "    k = m['past_key_value'].value_cache[0].clone()\n",
    "\n",
    "    q = _merge_heads(q, num_heads, attn_head_size)\n",
    "    k = _merge_heads(k, num_heads, attn_head_size)\n",
    "\n",
    "    q_expanded = q.unsqueeze(2)\n",
    "    k_expanded = k.unsqueeze(1)\n",
    "    tmp = q_expanded * k_expanded\n",
    "\n",
    "    # offload to cpu\n",
    "    tmp.cpu()\n",
    "\n",
    "    # free cuda memory\n",
    "    del q, k, q_expanded, k_expanded\n",
    "\n",
    "    multiplication_results.append(tmp)\n",
    "\n",
    "# flatten the list\n",
    "multiplication_results = torch.cat([tmp.reshape(-1) for tmp in multiplication_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(multiplication_results.reshape(-1).cpu().numpy(), bins=100, log=True)\n",
    "# cdf of abs(multiplication_results)\n",
    "\n",
    "plt.hist(multiplication_results.reshape(-1).abs().cpu().numpy(), bins=100, cumulative=True, density=True, histtype='step')\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.title(f\"Q*K^T, {q.shape[1]} * {q.shape[2]} * {k.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perplexity import perplexity\n",
    "\n",
    "device = 'cuda:0'\n",
    "root = '~/'\n",
    "dataset = 'PTB'\n",
    "\n",
    "stride = model.config.max_position_embeddings # 4096\n",
    "ppl_baseline = perplexity(model, tokenizer, dataset, device, verbose=True, stride=stride, root=root)\n",
    "print(ppl_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
