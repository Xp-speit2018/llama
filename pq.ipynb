{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xupeng/miniconda3/envs/faiss/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv, LlamaDecoderLayer, LlamaMLP, LlamaRMSNorm, LlamaModel, LlamaSdpaAttention, LlamaPreTrainedModel\n",
    "from transformers.cache_utils import Cache\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "\n",
    "class LlamaForCausalLMResearch(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "        self.model = LlamaModelResearch(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        self.middleware = {}\n",
    "        self.fwcall = 0\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        self.fwcall += 1\n",
    "        print(f\"fwcall: {self.fwcall}, key cache size(one layer): {past_key_values[0][0].shape if past_key_values is not None else None}\")\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "class LlamaModelResearch(LlamaModel):\n",
    "    def __init__(self, config):\n",
    "        super(LlamaModel, self).__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayerResearch(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        self.middleware = {}\n",
    "\n",
    "class LlamaDecoderLayerResearch(LlamaDecoderLayer):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super(LlamaDecoderLayer, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = LlamaSdpaAttentionResearch(config, layer_idx)\n",
    "        # self.self_attn = LlamaSdpaAttention(config, layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "class LlamaSdpaAttentionResearch(LlamaSdpaAttention):\n",
    "    \"\"\"\n",
    "    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n",
    "    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n",
    "    SDPA API.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.middleware = {}\n",
    "\n",
    "    # Adapted from LlamaAttention.forward\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        if output_attentions:\n",
    "            return super().forward(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "            )\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        self.middleware.update({\"query_states\": query_states, \"key_states\": key_states, \"value_states\": value_states})\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # In case static cache is used, it is an instance attribute.\n",
    "        # past_key_value = getattr(self, \"past_key_value\", past_key_value)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            # cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx)\n",
    "\n",
    "        # key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        # value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        # causal_mask = attention_mask\n",
    "        # if attention_mask is not None:\n",
    "        #     causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n",
    "\n",
    "        # # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n",
    "        # # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
    "        # if query_states.device.type == \"cuda\" and causal_mask is not None:\n",
    "        #     query_states = query_states.contiguous()\n",
    "        #     key_states = key_states.contiguous()\n",
    "        #     value_states = value_states.contiguous()\n",
    "        \n",
    "        causal_mask = None\n",
    "\n",
    "        # In case we are not compiling, we may set `causal_mask` to None, which is required to dispatch to SDPA's Flash Attention 2 backend, rather\n",
    "        # relying on the `is_causal` argument.\n",
    "        attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attn_mask=causal_mask,\n",
    "            dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "            is_causal=causal_mask is None and q_len > 1,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        self.middleware.update({\n",
    "            \"past_key_value\" : past_key_value,\n",
    "            \"key_states\": key_states\n",
    "        })\n",
    "\n",
    "        return attn_output, None, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "issubclass(LlamaForCausalLM, AutoModelForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLMResearch.from_pretrained(\"llama-2-7b-hf\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-31): 32 x LlamaDecoderLayerResearch(\n",
       "    (self_attn): LlamaSdpaAttentionResearch(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "      (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm()\n",
       "    (post_attention_layernorm): LlamaRMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先用4096个token生成KV Cache，用来训练PQ。32层、每层32个head，一共需要1024个PQ索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaForCausalLMResearch' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwcall: 3, key cache size(one layer): None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwcall: 4, key cache size(one layer): torch.Size([1, 32, 10869, 128])\n",
      "[{'generated_text': 'In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.We are releasing the following models to the general public for research and commercial use‡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§ 2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well.We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.To create the new family of Llama 2 models, we began with the pretraining approach described in Touvron et al. (2023), using an optimized auto-regressive transformer, but made several changes to improve performance. Specifically, we performed more robust data cleaning, updated our data mixes, trained on 40% more total tokens, doubled the context length, and used grouped-query attention (GQA) to improve inference scalability for our larger models. Table 1 compares the attributes of the new Llama 2 models with the Llama 1 models.Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations. We performed a variety of pretraining data investigations so that users can better understand the potential capabilities and limitations of our models; results can be found in Section 4.1.We adopt most of the pretraining setting and model architecture from Llama 1. We use the standard transformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and Sennrich, 2019), use the SwiGLU activation function (Shazeer, 2020), and rotary positional embeddings (RoPE, Su et al. 2022). The primary architectural differences from Llama 1 include increased context length and grouped-query attention (GQA). We detail in Appendix Section A.2.1 each of these differences with ablation experiments to demonstrate their importance.We trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with β1 = 0.9, β2 = 0.95, eps = 10−5. We use a cosine learning rate schedule, with warmup of 2000 steps, and decay final learning rate down to 10% of the peak learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. Figure 5 (a) shows the training loss for Llama 2 with these hyperparameters.Training Hardware. We pretrained our models on Meta’s Research Super Cluster (RSC) (Lee and Sengupta, 2022) as well as internal production clusters. Both clusters use NVIDIA A100s. There are two key differences between the two clusters, with the first being the type of interconnect available: RSC uses NVIDIA Quantum InfiniBand while our production cluster is equipped with a RoCE (RDMA over converged Ethernet) solution based on commodity ethernet Switches. Both of these solutions interconnect 200 Gbps end-points. The second difference is the per-GPU power consumption cap — RSC uses 400W while our production cluster uses 350W. With this two-cluster setup, we were able to compare the suitability of these different types of interconnect for large scale training. RoCE (which is a more affordable, commercial interconnect network)Table 2: CO2 emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta’s sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others. can scale almost as well as expensive Infiniband up to 2000 GPUs, which makes pretraining even more democratizable.Carbon Footprint of Pretraining. Following preceding research (Bender et al., 2021a; Patterson et al., 2021; Wu et al., 2022; Dodge et al., 2022) and using power consumption estimates of GPU devices and carbon efficiency, we aim to calculate the carbon emissions resulting from the pretraining of Llama 2 models. The actual power usage of a GPU is dependent on its utilization and is likely to vary from the Thermal Design Power (TDP) that we employ as an estimation for GPU power. It is important to note that our calculations do not account for further power demands, such as those from interconnect or non-GPU server power consumption, nor from datacenter cooling systems. Additionally, the carbon output related to the production of AI hardware, like GPUs, could add to the overall carbon footprint as suggested by Gupta et al. (2022b,a). Table 2 summarizes the carbon emission for pretraining the Llama 2 family of models. A cumulative of 3.3M GPU hours of computation was performed on hardware of type A100-80GB (TDP of 400W or 350W). We estimate the total emissions for training to be 539 tCO2eq, of which 100% were directly offset by Meta’s sustainability program.∗∗ Our open release strategy also means that these pretraining costs will not need to be incurred by other companies, saving more global resources.In this section, we report the results for the Llama 1 and Llama 2 base models, MosaicML Pretrained Transformer (MPT)†† models, and Falcon (Almazrouei et al., 2023) models on standard academic benchmarks. For all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon models internally. For these models, we always pick the best score between our evaluation framework and any publicly reported results. In Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety benchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The results for all the individual benchmarks are available in Section A.2.2.• Code. We report the average pass@1 scores of our models on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). • Commonsense Reasoning. We report the average of PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019a), WinoGrande (Sakaguchi et al., 2021), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and CommonsenseQA (Talmor et al., 2018). We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. • World Knowledge. We evaluate the 5-shot performance on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) and report the average. • Reading Comprehension. For reading comprehension, we report the 0-shot average on SQuAD (Rajpurkar et al., 2018), QuAC (Choi et al., 2018), and BoolQ (Clark et al., 2019). • MATH. We report the average of the GSM8K (8 shot) (Cobbe et al., 2021) and MATH (4 shot) (Hendrycks et al., 2021) benchmarks at top 1.As shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the results on MMLU and BBH by ≈5 and ≈8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 2 70B model outperforms all open-source models. In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L. We also analysed the potential data contamination and share the details in Section A.6.Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources. In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns (Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.Getting Started. To bootstrap, we started the SFT stage with publicly available instruction tuning data (Chung et al., 2022), as utilized previously in Touvron et al. (2023). Quality Is All You Need. Third-party SFT data is available from many different sources, but we found that many of these have insufficient diversity and quality — in particular for aligning LLMs towards dialogue-style instructions. As a result, we focused first on collecting several thousand examples of high-quality SFT data, as illustrated in Table 5. By setting aside millions of examples from third-party datasets and using fewer but higher-quality examples from our own vendor-based annotation efforts, our results notably improved. These findings are similar in spirit to Zhou et al. (2023), which also finds that a limited set of clean instruction-tuning data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of 27,540 annotations. Note that we do not include any Meta user data. We also observed that different annotation platforms and vendors can result in markedly different downstream model performance, highlighting the importance of data checks even when using vendors to source annotations. To validate our data quality, we carefully examined a set of 180 examples, comparing the annotations provided by humans with the samples generated by the model through manual scrutiny. Surprisingly, we found that the outputs sampled from the resulting SFT model were often competitive with SFT data handwritten by human annotators, suggesting that we could reprioritize and devote more annotation effort to preference-based annotation for RLHF. Fine-Tuning Details. For supervised fine-tuning, we use a cosine learning rate schedule with an initial learning rate of 2 × 10−5, a weight decay of 0.1, a batch size of 64, and a sequence length of 4096 tokens. For the fine-tuning process, each sample consists of a prompt and an answer. To ensure the model sequence length is properly filled, we concatenate all the prompts and answers from the training set. A special token is utilized to separate the prompt and answer segments. We utilize an autoregressive objective and zero-out the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we fine-tune the model for 2 epochs.Transformer-based large language models (LLMs) have achieved great success with the growing model size. LLMs’ size grows by 240× every two years, which outpaces the hardware progress and makes model inference increasingly costly. Model quantization is a promising approach to mitigate the widening gap between LLM size and hardware capacity. However, the existence of outliers, values with significant magnitudes, in LLMs makes existing quantization methods less effective. Prior outlier-aware quantization schemes adopt sparsity encoding techniques to separate outliers from normal values where the process requires global coordination (e.g., a global sparsity coordination list). This incurs complex encoding/decoding hardware logics and an extra orchestration controller for the computation between outlier and normal values. As such, it is not hardware-efficient and hence only achieves sub-optimal quantization benefits.We propose OliVe, an algorithm/architecture co-designed solution that adopts an outlier-victim pair (OVP) quantization and handles outlier values locally with low hardware overheads and high performance gains. The key insight of OliVe is that outliers are important while the normal values next to them are not. Thus those normal values (called victims) can be sacrificed to accommodate outliers. This enables a memory-aligned OVP encoding scheme, which can be efficiently integrated to the existing hardware accelerators like systolic array and tensor core. As a result, OliVe-based accelerator surpasses the existing outlier-aware accelerator, GOBO, by 4.5× speedup and 4.0× energy reduction, respectively, with a superior model accuracy.Large language models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferencesThe aforementioned outlier-aware architectures separate normal values from outliers in a global way. For instance, GOBO [85] involves a global sparse coordinate list in the quantization and computation, leading to a large hardware overhead and low performance benefits. In this work, we aim to design an architecture to handle outliers in a localized way with high hardware efficiency. To achieve that, we group two consecutive fixed-size values in a tensor and analyze their impact to model accuracy. There can be three kinds of pairs: i) a normal pair with two normal values, ii) one-outlier pair with one normal value and one outlier value, iii) two-outlier pair with two outlier values. We observe that the third two-outlier pair almost never shows up in well-trained LLMs. For the second one-outlier pair, we find that only keeping its outlier value while pruning its normal value (i.e., treating it as zero) is sufficient to maintain the model accuracy. Based on the above observations, we propose a novel outlieraware quantization architecture, called OliVe, based on the outliervictim pair (OVP) encoding. The salient feature of OliVe is memoryaligned and therefore hardware-friendly. As illustrated in Fig. 1b, OliVe first prunes normal values that are adjacent to the outliers as zero. These pruned normal values are called victims, which sacrifice themselves and make space for outliers. Then, we exploit the extra space provided by victims and embed the outliers into the low-precision matrix.OliVe is able to maintain a high accuracy for large Transformer models with a low hardware overhead due to the following reasons. First, OliVe incorporates victims to tackle outliers in LLMs. The effects of victims resemble model pruning [36]. Although clipping a few (0.1%) outliers will lead to a disastrous accuracy drop [18, 82], pruning the same amount of “normal” values will only impact model accuracy slightly (< 0.1% drop). Therefore, OliVe sacrifices (“prunes”) those insignificant values as victims for the outliers, allowing a more aggressive encoding scheme to accommodate extremely significant values. Second, the OVP encoding follows a specific outlier-victim (or victim-outlier) pattern to achieve memory alignment with little hardware overheads. Each victim is adjacent to an outlier, and the outlier-victim pair must align the memory access pattern. For example, in Fig. 1b, right outlier −98 in the OV pair needs a left victim, and left outliers 17.6 and 30.7 require the right victims. That can align 8-bit (1-byte) memory accesses with high efficiency. This design enables a completely localized outlier decoding/encoding process.The secure container that hosts a single container in a micro virtual machine (VM) is now used in serverless computing, as the containers are isolated through the microVMs. There are high demands on the high-density container deployment and high-concurrency container startup to improve both the resource utilization and user experience, as user functions are fine-grained in serverless platforms. Our investigation shows that the entire software stacks, containing the cgroups in the host operating system, the guest operating system, and the container rootfs for the function workload, together result in low deployment density and slow startup performance at high-concurrency. We propose and implement a lightweight secure container runtime, named RunD, to resolve the above problems through a holistic guest-tohost solution. With RunD, over 200 secure containers can be started in a second, and over 2,500 secure containers can be deployed on a node with 384GB of memory. RunD is adopted as Alibaba serverless container runtime to support high-density deployment and high-concurrency startup.Based on different levels of security/isolation requirements, there are generally two categories of secure containers in the production environments. Figure 2(a) shows the multi-container-per-VM secure container model that only isolates functions. In the model, a virtual machine (VM) hosts the containers for the invocations of the same function. The containers in the same VM share the guest operating system of the VM. In this case, the invocations to different functions are isolated, but the invocations to the same function are not isolated. Since the number of required containers for each function varies, this model results in memory fragmentations [34]. Though the memory fragmentations can be reclaimed at runtime, it may significantly affect the function performance, and even crash the VM when the memory hot-unplug fails. Figure 2(b) shows the single-container-per-VM secure container model that isolates each function invocation. Current serverless computing providers [1, 20] mainly use this secure container model. In this model, each invocation is served with a container in a microVM. This model does not introduce memory fragmentations, but the microVMs themselves show heavy memory overhead. It is obvious that each microVM needs to run its exclusive guest operating system, multiplying the memory footprints.Requirement on high-concurrency container startup. In serverless platforms, each function invocation is short, and a large number of function invocations may arrive in a short time. For example, in Alibaba serverless platform, more than 200 container-launch requests arrive nearly simultaneously on a node. The latency until all containers have entered main() can swell super-proportionally due to resource contention among the simultaneously launching VMs. Meanwhile, emerging internet services often show a diurnal load pattern and have bursty loads [18]. A large number of containers are required to be created when the load bursts. Some techniques, such as prewarming containers [31, 42, 49], are able to alleviate container cold startups. However, bursty loads are inevitable can easily exhaust the limited prewarmed containers. The ability to startup containers at high-concurrency is crucial for serverless platforms. Requirement on high-density container deployment. The small container specification in a serverless computing platform brings the requirement to deploy containers densely on a node. For instance, 47% of lambda functions run with the minimum memory specification of 128MB in AWS [5]. The actual memory usage of a container may also be smaller than its specifications. As Azure reports [49], about 90% of the applications never consume more than 400MB of memory. A node with 256GB of memory can host 8 × 256 = 2048 containers if there is no other overhead. In Alibaba serverless platform, over 2,500 secure containers that 128MB-sized can be deployed on a node with 384GB memory. Without proactive customizations, secure containers incur extra memory overhead, reducing deployment density in serverless computing. Increasing deployment density greatly improves resource utilization and multi-tenant serving efficiency with the same infrastructure.With the default configuration (cache enabled), virtio-blk performs best at random/sequential writing. However, the device-mapper who prepares the block device in the host cannot meet the high-concurrency requirement [59]. According to our measurement, it takes as high as 10 seconds to prepare a rootfs when 200 containers are started concurrently, while it only takes about 30 milliseconds for a single container startup. In this case, the operation of preparing rootfs timeouts, resulting in the container breakdown. Moreover, virtio-blk inherently does not support the page cache sharing between host and guest operating systems. When virtio-blk backend reads rootfs files into the host page cache, the mapped content reproduces the same page cache in the guest. The issue of duplicated page cache brings a high memory footprint overhead.Except for the memory used by the user function, the memory footprint of other components in the secure container is the memory overhead. The 5MB memory overhead reported in FireCracker [52] is the overhead of the FireCracker VMM itself. In the microVM of a secure container, the guest operating system, the struct page for memory management, and other components (e.g., baseOS, shimv2, agent) also consume additional memory space [52]. Figure 5 shows the per-container memory overhead of secure containers with different memory specifications and at different deployment densities. In the figure, Kata-qemu is the secure container that uses qemu as the hypervisor, and Kata-FireCracker uses FireCracker as the hypervisor. As observed in Figure 5(a), the memory overheads of a 128MB container are 94MB and 168MB with Kata-FireCracker and Kata-qemu, respectively. The overhead increases with the memory specification of the container. The average memory footprint of a single microVM can be reduced by sharing the text/rodata segment among multiple microVMs. Mainstream MicroVMs achieve it by mapping the kernel file to the guest memory directly using mmap. As shown in Figure 5(b), the per-microVM memory overhead of kata-qemu and kata-FireCracker reduce to 145MB and 71MB when 1,000 VMs are deployed on a node. However, the overhead is still too large for a serverless container with only 128MB memory specification.However, the template technique is not as efficient as we thought, due to the self-modifying codes in the operating system kernel [24, 25]. The self-modifying code technique alters the instructions on-demand as it runs, and the Linux kernel relies heavily on self-modification code to improve performance on boot and during runtime. We start a clean microVM with CentOS 4.19 guest kernel from a template to investigate the impact of self-modifying codes. The investigation shows that 10,012KB of the code and the read-only data is accessed in the memory, but 7,928KB of them were modified during boot. This case in point reveals that the selfmodifying codes degrade the efficiency when using mmap for less memory consumption of kernel image files.Cgroup is designed for resource control and abstraction of processes. In serverless computing, the frequency of function invocations shows high variation. In this case, the corresponding secure containers are frequently created and recycled. For instance, in our serverless platform, at most 200 containers would be created and recycled on a physical node concurrently in a second. The frequent creating and recycling challenge the cgroup mechanism on the host. We measure the performance of cgroup operations when creating 2, 000 containers concurrently. In the experiment, we use different numbers of threads to perform cgroup operations. Figure 6(a) shows the cumulative distribution of container creating latencies. Counter-intuitively, the latency increases when more threads are used, even if each thread needs to create fewer containers. The reason behind the above fact is that the Linux kernel introduces several global locks (e.g., cgroup_mutex, css_set_lock, freezer_mutex) to serialize cgroup operations. The global locks are used to coordinate more than 10 resource subsystems (aka. the cgroup subsys) involved in cgroup. Figure 6(b) shows the flame graph of creating 2, 000 cgroups using 10 threads concurrently. In the figure, the red parts show the case that “mutex locks” are active. When the cgroup mutex uses the optimistic spinning by default, the spinner cgroups experience the optimistic spinning if they fail to acquire the lock. It will lead to heavy CPU consumption and belated exiting of the critical section in the multi-Figure 7 shows the RunD design and summarizes our methodologies. RunD runtime makes a read/write splitting by providing the read-only layer to virtio-fs, using the builtin storage file to create a volatile writeable layer to virtioblk, and mounting the former and latter as the final container rootfs using overlayfs. RunD leverages the microVM template that integrates the condensed kernel and adopts the prepatched image to create a new microVM, further amortizing the overhead across different microVMs. RunD renames and attaches a lightweight cgroup from the cgroup pool for management when a secure container is created. Based on the above optimizations, a secure container (referred to as a “sandbox”) is started in the following steps, when RunD is used as the secure container runtime. • In the first step, once containerd receives a user invocation, it forwards the request to RunD runtime. • Second, RunD prepares the runc-container rootfs for the virtual machine hypervisor. The rootfs is separated into read-only layer and writable layer. (Section 4.2). • Third, the hypervisor uses the microVM template to create the required sandbox (Section 4.3), and mount the runc-container rootfs into the sandbox by overlayfs. • Lastly, a lightweight cgroup is attached to the sandbox (Section 4.4), to manage the resource allocation for this sandbox in the host.We investigate the data in a sandbox in the serverless computing scenario, and find that user-provided code/data is read-only for the operating system, and the systemprovided runtime files are also read-only for user functions. Meanwhile, the data in the local memory or storage generated in a sandbox will not be used by subsequent function invocations, due to the stateless feature of serverless computing. The temporary and intermediate data generated during the function execution is not required to be persisted. Based on the above finding, it is possible to split the rootfs into a read-only layer and a writable layer, and then handle them in different ways [32]. The sandboxes can share the read-only layer on the same node, and the writable layer has to be prepared separately for each sandbox. Figure 8 shows the way to split rootfs into a read-only layer and a volatile writable layer. According to the investigation in Section 3.1, virtio-fs is used to handle the read-only layer, and virtio-blk is used to handle the volatile writable layer for better performance. The read-only layer is stored in the host and can be prepared in negligible time when using the overlay snapshotter provided by the container runtime. However, it is challenging to handle the volatile writable layer efficiently. By default, the host operating system needs to prepare a logic storage volume for the sandbox. This operation is time-consuming and is one of the most important reasons that result in the long latency of preparing rootfs.Following the abstraction premise in current serverless platforms, the guest environment management for serverless containers is offloaded to the cloud provider. Meanwhile, RunD depends on the security model of hardware virtualization and VMM, explicitly treating the guest kernel as untrusted through syscall inspections. Based on this fact, there is an opportunity to condense the guest kernel for the lightweight characteristic of serverless functions. Considering that several features in the guest kernel are redundant and memory intensive in the serverless context, RunD condenses these features at compile-time. When customizing the condensed guest kernel, the principles behind it are as follows: - Minimize kernel memory footprint and image size. - Retain features required in the serverless context. - Without runtime performance degradation. Following the above principles, we build the condensed kernel for the guest operating system based on Linux kernel, by disabling features: - Do not pre-create loop device (2.2MB Mem reduced). - Disable acpi and ftrace (2MB and 6MB Mem reduced). - Disable graphics-related items (2MB Mem reduced). - Disable i2c and ceph (3MB Mem reduced, and 4MB reduced of kernel image size). - Kernel files (560K Mem and 571K image size reduced). Validating all features at compile-time case by case, RunD effectively reduces the memory footprint of a CentOS 4.19 Linux kernel by about 16MB and condenses the kernel image by about 4MB. Based on this condensed guest kernel, weAs mentioned before, cloud providers manage and maintain the underlying hardware and execution runtimes in serverless context, standing for that all microVMs on the same node generally use the same guest kernel. In this scenario, the sandboxes on the same node generate the same patched kernel code, even if they execute the self-modification patch logic. This is because the self-modifying code of kernel text segments only occurs at the startup phase, after which the kernel code area becomes “read-only after initialization”. In this case, sandboxes experience the same initialization phase and generate predicable self-modifying code segments. Based on the above observation, there is an opportunity to generate a pre-patch guest kernel image file already patched with self-modified code segments. The MicroVM template technique discussed in Section 3.2 may work efficiently without self-modifying code. Adapting to this optimization, we also resolve the potential kernel panic issues when loading the pre-patched kernel image for higher stability. RunD tries to share as many kernel files as possible across different secure containers. With a pre-patched microVM template, RunD not only reduces the memory footprint of a single container for higherdensity deployment, but also allows to quickly fork multiple instances [29, 52].The cgroup pool with renaming mechanism eliminates the time-consuming cgroup creation and initialization. RunD pre-creates corresponding lightweight cgroups and maintains them in a cgroup pool based on the pre-defined node capacity. These cgroups are marked idle when initialized, and are protected in a linked list. For each created container, RunD simply allocates an idle cgroup, updates the state to busy, performs the cgroup rename operation, and then attaches the container to this renamed cgroup when a container is started. If a container triggers recycling, RunD will take the cgroup back to the pool, kill the corresponding instance process, and then update the returned cgroup state to idle for subsequent allocating and renaming. Adopting the above optimizations in kernel mode, we replay the evaluation in Section 3.3. The cgroups creation only consumes 0.09s (1 thread), 0.1s (50 threads), and 0.14s (200 threads), respectively. Compared with the default mechanism, the lightweight cgroup and the rename-based cgroup pool reduce 94% of the cgroups creation time.Baselines: we compare RunD with the state-of-theart secure container, Kata Containers [19]. Specifically, we use three popular configurations of Kata containers: Kata-qemu, Kata-template, and Kata-FC. Kata-qemu uses QEMU [15, 23] as the microVM hypervisor, Kata-template uses QEMU while integrating container template, Kata-FC uses lightweight FireCracker [20] as the microVM hypervisor. Kata-qemu and kata-template use an old version of Kata Containers, as the new version has some bugs that result in poor performance. Table 1 shows the detailed setups. Testbed: we run the experiments on a node with 104 virtual cores, 384GB memory, and two SSD drives of 100GB and 500GB. Such specification is widely-used in production clouds. The 100GB drive is used as the root filesystem of the host operating system, and the 500GB drive is used by the secure containers. We use Alibaba Cloud Linux 2 for RunD and Alpine Linux [3] for others, as the guest operating systems in the microVM for a low memory footprint. Measurement: in the CRI specification [6], a pod sandbox refers to a microVM with a lightweight pause container [12]. In all the tests, we only create the pod sandboxes without other containers inside, through the crictl command. In the following evaluations, the memory specification of a container denotes the size of memory that can be used by itself. The actual memory usage of a container is collected using the smem command. As RunD is proposed to maximize the supported container startup concurrency and deployment density, in the experiment, we start empty secure containers without user codes or data considering that it is a common practice in FaaS to start empty containers concurrently for prewarming. The inproduction results show the performance of RunD for actual workloads with all the steps involved.As shown in the figure, RunD uses the shortest time to start a large number of sandboxes for all concurrency levels. When 200 containers are created concurrently (we already observe such high-concurrency in Alibaba serverless platform), Kata-FC, kata-qemu, kata-template, and RunD needs 47.6s, 6.85s and 2.98s and 1s to create them. Kata-FC requires a much longer time to startup the sandboxes when the concurrency is high. This is because Kata-FC uses virtioblk to create rootfs, and the performance is poor at highconcurrency, as we measured in Section 3. There is no such bottleneck in Kata-template and Kata-qemu. Kata-template simply uses template to reduce the overhead of guest kernel and rootfs loading, but the inefficient rootfs mapping, code self-modification and high host-side overhead of the cgroup operations still exists. As a result, it performs worse than RunD at high startup concurrency. The overall optimizations suggest that RunD provides the performance improvement of about 40% over its nearest baseline, Kata-template, at highconcurrency (e.g., 400-way) startup. As for the second metric, Figure 10(b) shows the latency distribution of starting each sandbox, when 200 sandboxes are started concurrently. RunD and Kata-template are able to start sandboxes in a stable short time, but the latencies of starting sandboxes with others are out of expected. Users can have identical good experiences with RunD. As for the CPU overhead, Figure 10(c) shows the CPU time needed on the host to startup sandboxes. When the concurrency is high, RunD greatly reduces the CPU overhead. For instance, when 200 sandboxes are started concurrently, RunD reduces 89.3%, 74.5% and 62.1% CPU overhead compared with Kata-qemu, Kata-template, and KataFC, respectively. In addition, the CPU overhead of RunD only increases slightly, when the concurrency increases. This is due to the read/write split policy and the reduction of compute-intensive operations in cgroups. Therefore, RunDThe memory overhead of RunD is 5MB, which is the overhead of the RunD runtime itself. The memory overhead of the microVM is 94MB for a 128MB container, and 168MB for a 256MB container. The memory overhead increases with the memory specification of the container. The average memory overhead of a single microVM can be reduced by sharing the text/rodata segment among multiple microVMs. As shown in Figure 5(b), the per-microVM memory overhead of kata-qemu and kata-FireCracker reduce to 145MB and 71MB when 1,000 VMs are deployed on a node. However, the overhead is still too large for a serverless container with only 128MB memory specification.Currently, Alibaba serverless computing platform has adopted RunD. The platform serves almost 4 billion invocations from more than 1 million different functions per day. Figure 14 reports the sandbox startup concurrency and the corresponding startup latency from six nodes. The specification of each node is the same as our experimental setup in Table 1. The data is collected between 08:00 and 18:00 of Jan 10th, 2022. There are about 800 active sandboxes on each node, when the concurrency data is collected. The inproduction startup latency of sandboxes at high-concurrency is consistent with that reported in Section 5.4. As observed from the figure, the startup concurrency bursts at the beginning of each hour. At most 191 sandboxes are started concurrently around 10:00. RunD starts the 191 sandboxes in 1.6 seconds. We look into the function invocation logs, and find that the periodic burst is caused by the an-hour time trigger and cluster-level load balancing. The periodical burst is pervasive, as the Azure serverless platform traces [14] show the same pattern. In the figure, the sandbox startup latency occasionally increases when the concurrency is low. The long time results from the operation in loading large-scale workloads from the tenants. Although the startupThe most closely related work to RunD is FireCracker [20], which proposes a lightweight VMM for serverless runtime. It provides fast startup within 125ms, allowing 150 VMs to start concurrently per second per node, with less than 5MB footprint per VM. However, FireCracker only serves as the hypervisor stack in the Security Container model, without other complex related processes, e.g., rootfs [52]. By contrast, RunD investigates the guest-to-host solution through all stacks and provides higher concurrency and density. Higher-density deployment. Regarding serverless computing, in the space of higher function deployment density of Secure Containers and VMs [57], the key is designing a more lightweight container runtime both in guest and host. Unikernel [36, 37, 43, 47] runs as a built-in GuestOS without necessary add-ons, demonstrating great potential for deploying containers with less overhead. Kuo [33] Explores lightweight guest kernel configurations for use in Unikernel environments, which has similarity to the approach towards reducing guest kernel size. However, Unikernel is hard to be changed once after compilation with the application. Its compile-time invariance results in poor flexibility in practice. SAND [21] adopts the multi-container-per-VM model to amortize the memory footprint of sandboxing. However, they do not further investigate the utilization impact of memory fragmentations in a real-system with high-density deployment. Gsight [61] observes that fine-grained functionlevel profiling can expose more predictability system-level features in the partial interference. With a more accurate interference predicting [27, 44], the function density can get improved with QoS guaranteed. The above studies make sense in improving the effective density with less interference for serverless. They are orthogonal to our work, because RunD is motivated to improve the maximum deployment density on a signe node. Higher-concurrency startup. In the space of higher function startup concurrency, recent approaches leverage the container prewarm pool [9, 40, 49, 58]. The state-of-the-art on container prewarming, SOCK [42], uses a benefit-to-cost model to select packages pre-installed in zygotes, and builds a tree cache to ensure that the forked zygote container does not import any additional packages other than the private ones the handler specifies. The C/R (Checkpoint/Restore) [7, 31, 39] supporting the VM snapshotting [10, 28, 29, 41, 54] captures the state of a running instance as a checkpoint, and then restores it once cold startup. Observing that most functions only access a small fraction of the files and mem- NarodЪ'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "# device = \"cuda:0\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = device,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# sequence = \"SJTU SEIEE is\"\n",
    "sequence =  \"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\"\n",
    "sequence += \"Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.\"\n",
    "sequence += \"The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.\"\n",
    "sequence += \"In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.\"\n",
    "sequence += \"We are releasing the following models to the general public for research and commercial use‡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.§ 2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well.\"\n",
    "sequence += \"We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\"\n",
    "sequence += \"To create the new family of Llama 2 models, we began with the pretraining approach described in Touvron et al. (2023), using an optimized auto-regressive transformer, but made several changes to improve performance. Specifically, we performed more robust data cleaning, updated our data mixes, trained on 40% more total tokens, doubled the context length, and used grouped-query attention (GQA) to improve inference scalability for our larger models. Table 1 compares the attributes of the new Llama 2 models with the Llama 1 models.\"\n",
    "sequence += \"Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations. We performed a variety of pretraining data investigations so that users can better understand the potential capabilities and limitations of our models; results can be found in Section 4.1.\"\n",
    "sequence += \"We adopt most of the pretraining setting and model architecture from Llama 1. We use the standard transformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and Sennrich, 2019), use the SwiGLU activation function (Shazeer, 2020), and rotary positional embeddings (RoPE, Su et al. 2022). The primary architectural differences from Llama 1 include increased context length and grouped-query attention (GQA). We detail in Appendix Section A.2.1 each of these differences with ablation experiments to demonstrate their importance.\"\n",
    "sequence += \"We trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with β1 = 0.9, β2 = 0.95, eps = 10−5. We use a cosine learning rate schedule, with warmup of 2000 steps, and decay final learning rate down to 10% of the peak learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. Figure 5 (a) shows the training loss for Llama 2 with these hyperparameters.\"\n",
    "sequence += \"Training Hardware. We pretrained our models on Meta’s Research Super Cluster (RSC) (Lee and Sengupta, 2022) as well as internal production clusters. Both clusters use NVIDIA A100s. There are two key differences between the two clusters, with the first being the type of interconnect available: RSC uses NVIDIA Quantum InfiniBand while our production cluster is equipped with a RoCE (RDMA over converged Ethernet) solution based on commodity ethernet Switches. Both of these solutions interconnect 200 Gbps end-points. The second difference is the per-GPU power consumption cap — RSC uses 400W while our production cluster uses 350W. With this two-cluster setup, we were able to compare the suitability of these different types of interconnect for large scale training. RoCE (which is a more affordable, commercial interconnect network)\"\n",
    "sequence += \"Table 2: CO2 emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta’s sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others. can scale almost as well as expensive Infiniband up to 2000 GPUs, which makes pretraining even more democratizable.\"\n",
    "sequence += \"Carbon Footprint of Pretraining. Following preceding research (Bender et al., 2021a; Patterson et al., 2021; Wu et al., 2022; Dodge et al., 2022) and using power consumption estimates of GPU devices and carbon efficiency, we aim to calculate the carbon emissions resulting from the pretraining of Llama 2 models. The actual power usage of a GPU is dependent on its utilization and is likely to vary from the Thermal Design Power (TDP) that we employ as an estimation for GPU power. It is important to note that our calculations do not account for further power demands, such as those from interconnect or non-GPU server power consumption, nor from datacenter cooling systems. Additionally, the carbon output related to the production of AI hardware, like GPUs, could add to the overall carbon footprint as suggested by Gupta et al. (2022b,a). Table 2 summarizes the carbon emission for pretraining the Llama 2 family of models. A cumulative of 3.3M GPU hours of computation was performed on hardware of type A100-80GB (TDP of 400W or 350W). We estimate the total emissions for training to be 539 tCO2eq, of which 100% were directly offset by Meta’s sustainability program.∗∗ Our open release strategy also means that these pretraining costs will not need to be incurred by other companies, saving more global resources.\"\n",
    "sequence += \"In this section, we report the results for the Llama 1 and Llama 2 base models, MosaicML Pretrained Transformer (MPT)†† models, and Falcon (Almazrouei et al., 2023) models on standard academic benchmarks. For all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon models internally. For these models, we always pick the best score between our evaluation framework and any publicly reported results. In Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety benchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The results for all the individual benchmarks are available in Section A.2.2.\"\n",
    "sequence += \"• Code. We report the average pass@1 scores of our models on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). • Commonsense Reasoning. We report the average of PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019a), WinoGrande (Sakaguchi et al., 2021), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and CommonsenseQA (Talmor et al., 2018). We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. • World Knowledge. We evaluate the 5-shot performance on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017) and report the average. • Reading Comprehension. For reading comprehension, we report the 0-shot average on SQuAD (Rajpurkar et al., 2018), QuAC (Choi et al., 2018), and BoolQ (Clark et al., 2019). • MATH. We report the average of the GSM8K (8 shot) (Cobbe et al., 2021) and MATH (4 shot) (Hendrycks et al., 2021) benchmarks at top 1.\"\n",
    "sequence += \"As shown in Table 3, Llama 2 models outperform Llama 1 models. In particular, Llama 2 70B improves the results on MMLU and BBH by ≈5 and ≈8 points, respectively, compared to Llama 1 65B. Llama 2 7B and 30B models outperform MPT models of the corresponding size on all categories besides code benchmarks. For the Falcon models, Llama 2 7B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks. Additionally, Llama 2 70B model outperforms all open-source models. In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al., 2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L. We also analysed the potential data contamination and share the details in Section A.6.\"\n",
    "sequence += \"Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques, including both instruction tuning and RLHF, requiring significant computational and annotation resources. In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns (Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\"\n",
    "sequence += \"Getting Started. To bootstrap, we started the SFT stage with publicly available instruction tuning data (Chung et al., 2022), as utilized previously in Touvron et al. (2023). Quality Is All You Need. Third-party SFT data is available from many different sources, but we found that many of these have insufficient diversity and quality — in particular for aligning LLMs towards dialogue-style instructions. As a result, we focused first on collecting several thousand examples of high-quality SFT data, as illustrated in Table 5. By setting aside millions of examples from third-party datasets and using fewer but higher-quality examples from our own vendor-based annotation efforts, our results notably improved. These findings are similar in spirit to Zhou et al. (2023), which also finds that a limited set of clean instruction-tuning data can be sufficient to reach a high level of quality. We found that SFT annotations in the order of tens of thousands was enough to achieve a high-quality result. We stopped annotating SFT after collecting a total of 27,540 annotations. Note that we do not include any Meta user data. We also observed that different annotation platforms and vendors can result in markedly different downstream model performance, highlighting the importance of data checks even when using vendors to source annotations. To validate our data quality, we carefully examined a set of 180 examples, comparing the annotations provided by humans with the samples generated by the model through manual scrutiny. Surprisingly, we found that the outputs sampled from the resulting SFT model were often competitive with SFT data handwritten by human annotators, suggesting that we could reprioritize and devote more annotation effort to preference-based annotation for RLHF. Fine-Tuning Details. For supervised fine-tuning, we use a cosine learning rate schedule with an initial learning rate of 2 × 10−5, a weight decay of 0.1, a batch size of 64, and a sequence length of 4096 tokens. For the fine-tuning process, each sample consists of a prompt and an answer. To ensure the model sequence length is properly filled, we concatenate all the prompts and answers from the training set. A special token is utilized to separate the prompt and answer segments. We utilize an autoregressive objective and zero-out the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we fine-tune the model for 2 epochs.\"\n",
    "sequence += \"Transformer-based large language models (LLMs) have achieved great success with the growing model size. LLMs’ size grows by 240× every two years, which outpaces the hardware progress and makes model inference increasingly costly. Model quantization is a promising approach to mitigate the widening gap between LLM size and hardware capacity. However, the existence of outliers, values with significant magnitudes, in LLMs makes existing quantization methods less effective. Prior outlier-aware quantization schemes adopt sparsity encoding techniques to separate outliers from normal values where the process requires global coordination (e.g., a global sparsity coordination list). This incurs complex encoding/decoding hardware logics and an extra orchestration controller for the computation between outlier and normal values. As such, it is not hardware-efficient and hence only achieves sub-optimal quantization benefits.\"\n",
    "sequence += \"We propose OliVe, an algorithm/architecture co-designed solution that adopts an outlier-victim pair (OVP) quantization and handles outlier values locally with low hardware overheads and high performance gains. The key insight of OliVe is that outliers are important while the normal values next to them are not. Thus those normal values (called victims) can be sacrificed to accommodate outliers. This enables a memory-aligned OVP encoding scheme, which can be efficiently integrated to the existing hardware accelerators like systolic array and tensor core. As a result, OliVe-based accelerator surpasses the existing outlier-aware accelerator, GOBO, by 4.5× speedup and 4.0× energy reduction, respectively, with a superior model accuracy.\"\n",
    "sequence += \"Large language models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.\"\n",
    "sequence += \"The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences\"\n",
    "sequence += \"The aforementioned outlier-aware architectures separate normal values from outliers in a global way. For instance, GOBO [85] involves a global sparse coordinate list in the quantization and computation, leading to a large hardware overhead and low performance benefits. In this work, we aim to design an architecture to handle outliers in a localized way with high hardware efficiency. To achieve that, we group two consecutive fixed-size values in a tensor and analyze their impact to model accuracy. There can be three kinds of pairs: i) a normal pair with two normal values, ii) one-outlier pair with one normal value and one outlier value, iii) two-outlier pair with two outlier values. We observe that the third two-outlier pair almost never shows up in well-trained LLMs. For the second one-outlier pair, we find that only keeping its outlier value while pruning its normal value (i.e., treating it as zero) is sufficient to maintain the model accuracy. Based on the above observations, we propose a novel outlieraware quantization architecture, called OliVe, based on the outliervictim pair (OVP) encoding. The salient feature of OliVe is memoryaligned and therefore hardware-friendly. As illustrated in Fig. 1b, OliVe first prunes normal values that are adjacent to the outliers as zero. These pruned normal values are called victims, which sacrifice themselves and make space for outliers. Then, we exploit the extra space provided by victims and embed the outliers into the low-precision matrix.\"\n",
    "sequence += \"OliVe is able to maintain a high accuracy for large Transformer models with a low hardware overhead due to the following reasons. First, OliVe incorporates victims to tackle outliers in LLMs. The effects of victims resemble model pruning [36]. Although clipping a few (0.1%) outliers will lead to a disastrous accuracy drop [18, 82], pruning the same amount of “normal” values will only impact model accuracy slightly (< 0.1% drop). Therefore, OliVe sacrifices (“prunes”) those insignificant values as victims for the outliers, allowing a more aggressive encoding scheme to accommodate extremely significant values. Second, the OVP encoding follows a specific outlier-victim (or victim-outlier) pattern to achieve memory alignment with little hardware overheads. Each victim is adjacent to an outlier, and the outlier-victim pair must align the memory access pattern. For example, in Fig. 1b, right outlier −98 in the OV pair needs a left victim, and left outliers 17.6 and 30.7 require the right victims. That can align 8-bit (1-byte) memory accesses with high efficiency. This design enables a completely localized outlier decoding/encoding process.\"\n",
    "sequence += \"The secure container that hosts a single container in a micro virtual machine (VM) is now used in serverless computing, as the containers are isolated through the microVMs. There are high demands on the high-density container deployment and high-concurrency container startup to improve both the resource utilization and user experience, as user functions are fine-grained in serverless platforms. Our investigation shows that the entire software stacks, containing the cgroups in the host operating system, the guest operating system, and the container rootfs for the function workload, together result in low deployment density and slow startup performance at high-concurrency. We propose and implement a lightweight secure container runtime, named RunD, to resolve the above problems through a holistic guest-tohost solution. With RunD, over 200 secure containers can be started in a second, and over 2,500 secure containers can be deployed on a node with 384GB of memory. RunD is adopted as Alibaba serverless container runtime to support high-density deployment and high-concurrency startup.\"\n",
    "sequence += \"Based on different levels of security/isolation requirements, there are generally two categories of secure containers in the production environments. Figure 2(a) shows the multi-container-per-VM secure container model that only isolates functions. In the model, a virtual machine (VM) hosts the containers for the invocations of the same function. The containers in the same VM share the guest operating system of the VM. In this case, the invocations to different functions are isolated, but the invocations to the same function are not isolated. Since the number of required containers for each function varies, this model results in memory fragmentations [34]. Though the memory fragmentations can be reclaimed at runtime, it may significantly affect the function performance, and even crash the VM when the memory hot-unplug fails. Figure 2(b) shows the single-container-per-VM secure container model that isolates each function invocation. Current serverless computing providers [1, 20] mainly use this secure container model. In this model, each invocation is served with a container in a microVM. This model does not introduce memory fragmentations, but the microVMs themselves show heavy memory overhead. It is obvious that each microVM needs to run its exclusive guest operating system, multiplying the memory footprints.\"\n",
    "sequence += \"Requirement on high-concurrency container startup. In serverless platforms, each function invocation is short, and a large number of function invocations may arrive in a short time. For example, in Alibaba serverless platform, more than 200 container-launch requests arrive nearly simultaneously on a node. The latency until all containers have entered main() can swell super-proportionally due to resource contention among the simultaneously launching VMs. Meanwhile, emerging internet services often show a diurnal load pattern and have bursty loads [18]. A large number of containers are required to be created when the load bursts. Some techniques, such as prewarming containers [31, 42, 49], are able to alleviate container cold startups. However, bursty loads are inevitable can easily exhaust the limited prewarmed containers. The ability to startup containers at high-concurrency is crucial for serverless platforms. Requirement on high-density container deployment. The small container specification in a serverless computing platform brings the requirement to deploy containers densely on a node. For instance, 47% of lambda functions run with the minimum memory specification of 128MB in AWS [5]. The actual memory usage of a container may also be smaller than its specifications. As Azure reports [49], about 90% of the applications never consume more than 400MB of memory. A node with 256GB of memory can host 8 × 256 = 2048 containers if there is no other overhead. In Alibaba serverless platform, over 2,500 secure containers that 128MB-sized can be deployed on a node with 384GB memory. Without proactive customizations, secure containers incur extra memory overhead, reducing deployment density in serverless computing. Increasing deployment density greatly improves resource utilization and multi-tenant serving efficiency with the same infrastructure.\"\n",
    "sequence += \"With the default configuration (cache enabled), virtio-blk performs best at random/sequential writing. However, the device-mapper who prepares the block device in the host cannot meet the high-concurrency requirement [59]. According to our measurement, it takes as high as 10 seconds to prepare a rootfs when 200 containers are started concurrently, while it only takes about 30 milliseconds for a single container startup. In this case, the operation of preparing rootfs timeouts, resulting in the container breakdown. Moreover, virtio-blk inherently does not support the page cache sharing between host and guest operating systems. When virtio-blk backend reads rootfs files into the host page cache, the mapped content reproduces the same page cache in the guest. The issue of duplicated page cache brings a high memory footprint overhead.\"\n",
    "sequence += \"Except for the memory used by the user function, the memory footprint of other components in the secure container is the memory overhead. The 5MB memory overhead reported in FireCracker [52] is the overhead of the FireCracker VMM itself. In the microVM of a secure container, the guest operating system, the struct page for memory management, and other components (e.g., baseOS, shimv2, agent) also consume additional memory space [52]. Figure 5 shows the per-container memory overhead of secure containers with different memory specifications and at different deployment densities. In the figure, Kata-qemu is the secure container that uses qemu as the hypervisor, and Kata-FireCracker uses FireCracker as the hypervisor. As observed in Figure 5(a), the memory overheads of a 128MB container are 94MB and 168MB with Kata-FireCracker and Kata-qemu, respectively. The overhead increases with the memory specification of the container. The average memory footprint of a single microVM can be reduced by sharing the text/rodata segment among multiple microVMs. Mainstream MicroVMs achieve it by mapping the kernel file to the guest memory directly using mmap. As shown in Figure 5(b), the per-microVM memory overhead of kata-qemu and kata-FireCracker reduce to 145MB and 71MB when 1,000 VMs are deployed on a node. However, the overhead is still too large for a serverless container with only 128MB memory specification.\"\n",
    "sequence += \"However, the template technique is not as efficient as we thought, due to the self-modifying codes in the operating system kernel [24, 25]. The self-modifying code technique alters the instructions on-demand as it runs, and the Linux kernel relies heavily on self-modification code to improve performance on boot and during runtime. We start a clean microVM with CentOS 4.19 guest kernel from a template to investigate the impact of self-modifying codes. The investigation shows that 10,012KB of the code and the read-only data is accessed in the memory, but 7,928KB of them were modified during boot. This case in point reveals that the selfmodifying codes degrade the efficiency when using mmap for less memory consumption of kernel image files.\"\n",
    "sequence += \"Cgroup is designed for resource control and abstraction of processes. In serverless computing, the frequency of function invocations shows high variation. In this case, the corresponding secure containers are frequently created and recycled. For instance, in our serverless platform, at most 200 containers would be created and recycled on a physical node concurrently in a second. The frequent creating and recycling challenge the cgroup mechanism on the host. We measure the performance of cgroup operations when creating 2, 000 containers concurrently. In the experiment, we use different numbers of threads to perform cgroup operations. Figure 6(a) shows the cumulative distribution of container creating latencies. Counter-intuitively, the latency increases when more threads are used, even if each thread needs to create fewer containers. The reason behind the above fact is that the Linux kernel introduces several global locks (e.g., cgroup_mutex, css_set_lock, freezer_mutex) to serialize cgroup operations. The global locks are used to coordinate more than 10 resource subsystems (aka. the cgroup subsys) involved in cgroup. Figure 6(b) shows the flame graph of creating 2, 000 cgroups using 10 threads concurrently. In the figure, the red parts show the case that “mutex locks” are active. When the cgroup mutex uses the optimistic spinning by default, the spinner cgroups experience the optimistic spinning if they fail to acquire the lock. It will lead to heavy CPU consumption and belated exiting of the critical section in the multi-\"\n",
    "sequence += \"Figure 7 shows the RunD design and summarizes our methodologies. RunD runtime makes a read/write splitting by providing the read-only layer to virtio-fs, using the builtin storage file to create a volatile writeable layer to virtioblk, and mounting the former and latter as the final container rootfs using overlayfs. RunD leverages the microVM template that integrates the condensed kernel and adopts the prepatched image to create a new microVM, further amortizing the overhead across different microVMs. RunD renames and attaches a lightweight cgroup from the cgroup pool for management when a secure container is created. Based on the above optimizations, a secure container (referred to as a “sandbox”) is started in the following steps, when RunD is used as the secure container runtime. • In the first step, once containerd receives a user invocation, it forwards the request to RunD runtime. • Second, RunD prepares the runc-container rootfs for the virtual machine hypervisor. The rootfs is separated into read-only layer and writable layer. (Section 4.2). • Third, the hypervisor uses the microVM template to create the required sandbox (Section 4.3), and mount the runc-container rootfs into the sandbox by overlayfs. • Lastly, a lightweight cgroup is attached to the sandbox (Section 4.4), to manage the resource allocation for this sandbox in the host.\"\n",
    "sequence += \"We investigate the data in a sandbox in the serverless computing scenario, and find that user-provided code/data is read-only for the operating system, and the systemprovided runtime files are also read-only for user functions. Meanwhile, the data in the local memory or storage generated in a sandbox will not be used by subsequent function invocations, due to the stateless feature of serverless computing. The temporary and intermediate data generated during the function execution is not required to be persisted. Based on the above finding, it is possible to split the rootfs into a read-only layer and a writable layer, and then handle them in different ways [32]. The sandboxes can share the read-only layer on the same node, and the writable layer has to be prepared separately for each sandbox. Figure 8 shows the way to split rootfs into a read-only layer and a volatile writable layer. According to the investigation in Section 3.1, virtio-fs is used to handle the read-only layer, and virtio-blk is used to handle the volatile writable layer for better performance. The read-only layer is stored in the host and can be prepared in negligible time when using the overlay snapshotter provided by the container runtime. However, it is challenging to handle the volatile writable layer efficiently. By default, the host operating system needs to prepare a logic storage volume for the sandbox. This operation is time-consuming and is one of the most important reasons that result in the long latency of preparing rootfs.\"\n",
    "sequence += \"Following the abstraction premise in current serverless platforms, the guest environment management for serverless containers is offloaded to the cloud provider. Meanwhile, RunD depends on the security model of hardware virtualization and VMM, explicitly treating the guest kernel as untrusted through syscall inspections. Based on this fact, there is an opportunity to condense the guest kernel for the lightweight characteristic of serverless functions. Considering that several features in the guest kernel are redundant and memory intensive in the serverless context, RunD condenses these features at compile-time. When customizing the condensed guest kernel, the principles behind it are as follows: - Minimize kernel memory footprint and image size. - Retain features required in the serverless context. - Without runtime performance degradation. Following the above principles, we build the condensed kernel for the guest operating system based on Linux kernel, by disabling features: - Do not pre-create loop device (2.2MB Mem reduced). - Disable acpi and ftrace (2MB and 6MB Mem reduced). - Disable graphics-related items (2MB Mem reduced). - Disable i2c and ceph (3MB Mem reduced, and 4MB reduced of kernel image size). - Kernel files (560K Mem and 571K image size reduced). Validating all features at compile-time case by case, RunD effectively reduces the memory footprint of a CentOS 4.19 Linux kernel by about 16MB and condenses the kernel image by about 4MB. Based on this condensed guest kernel, we\"\n",
    "sequence += \"As mentioned before, cloud providers manage and maintain the underlying hardware and execution runtimes in serverless context, standing for that all microVMs on the same node generally use the same guest kernel. In this scenario, the sandboxes on the same node generate the same patched kernel code, even if they execute the self-modification patch logic. This is because the self-modifying code of kernel text segments only occurs at the startup phase, after which the kernel code area becomes “read-only after initialization”. In this case, sandboxes experience the same initialization phase and generate predicable self-modifying code segments. Based on the above observation, there is an opportunity to generate a pre-patch guest kernel image file already patched with self-modified code segments. The MicroVM template technique discussed in Section 3.2 may work efficiently without self-modifying code. Adapting to this optimization, we also resolve the potential kernel panic issues when loading the pre-patched kernel image for higher stability. RunD tries to share as many kernel files as possible across different secure containers. With a pre-patched microVM template, RunD not only reduces the memory footprint of a single container for higherdensity deployment, but also allows to quickly fork multiple instances [29, 52].\"\n",
    "sequence += \"The cgroup pool with renaming mechanism eliminates the time-consuming cgroup creation and initialization. RunD pre-creates corresponding lightweight cgroups and maintains them in a cgroup pool based on the pre-defined node capacity. These cgroups are marked idle when initialized, and are protected in a linked list. For each created container, RunD simply allocates an idle cgroup, updates the state to busy, performs the cgroup rename operation, and then attaches the container to this renamed cgroup when a container is started. If a container triggers recycling, RunD will take the cgroup back to the pool, kill the corresponding instance process, and then update the returned cgroup state to idle for subsequent allocating and renaming. Adopting the above optimizations in kernel mode, we replay the evaluation in Section 3.3. The cgroups creation only consumes 0.09s (1 thread), 0.1s (50 threads), and 0.14s (200 threads), respectively. Compared with the default mechanism, the lightweight cgroup and the rename-based cgroup pool reduce 94% of the cgroups creation time.\"\n",
    "sequence += \"Baselines: we compare RunD with the state-of-theart secure container, Kata Containers [19]. Specifically, we use three popular configurations of Kata containers: Kata-qemu, Kata-template, and Kata-FC. Kata-qemu uses QEMU [15, 23] as the microVM hypervisor, Kata-template uses QEMU while integrating container template, Kata-FC uses lightweight FireCracker [20] as the microVM hypervisor. Kata-qemu and kata-template use an old version of Kata Containers, as the new version has some bugs that result in poor performance. Table 1 shows the detailed setups. Testbed: we run the experiments on a node with 104 virtual cores, 384GB memory, and two SSD drives of 100GB and 500GB. Such specification is widely-used in production clouds. The 100GB drive is used as the root filesystem of the host operating system, and the 500GB drive is used by the secure containers. We use Alibaba Cloud Linux 2 for RunD and Alpine Linux [3] for others, as the guest operating systems in the microVM for a low memory footprint. Measurement: in the CRI specification [6], a pod sandbox refers to a microVM with a lightweight pause container [12]. In all the tests, we only create the pod sandboxes without other containers inside, through the crictl command. In the following evaluations, the memory specification of a container denotes the size of memory that can be used by itself. The actual memory usage of a container is collected using the smem command. As RunD is proposed to maximize the supported container startup concurrency and deployment density, in the experiment, we start empty secure containers without user codes or data considering that it is a common practice in FaaS to start empty containers concurrently for prewarming. The inproduction results show the performance of RunD for actual workloads with all the steps involved.\"\n",
    "sequence += \"As shown in the figure, RunD uses the shortest time to start a large number of sandboxes for all concurrency levels. When 200 containers are created concurrently (we already observe such high-concurrency in Alibaba serverless platform), Kata-FC, kata-qemu, kata-template, and RunD needs 47.6s, 6.85s and 2.98s and 1s to create them. Kata-FC requires a much longer time to startup the sandboxes when the concurrency is high. This is because Kata-FC uses virtioblk to create rootfs, and the performance is poor at highconcurrency, as we measured in Section 3. There is no such bottleneck in Kata-template and Kata-qemu. Kata-template simply uses template to reduce the overhead of guest kernel and rootfs loading, but the inefficient rootfs mapping, code self-modification and high host-side overhead of the cgroup operations still exists. As a result, it performs worse than RunD at high startup concurrency. The overall optimizations suggest that RunD provides the performance improvement of about 40% over its nearest baseline, Kata-template, at highconcurrency (e.g., 400-way) startup. As for the second metric, Figure 10(b) shows the latency distribution of starting each sandbox, when 200 sandboxes are started concurrently. RunD and Kata-template are able to start sandboxes in a stable short time, but the latencies of starting sandboxes with others are out of expected. Users can have identical good experiences with RunD. As for the CPU overhead, Figure 10(c) shows the CPU time needed on the host to startup sandboxes. When the concurrency is high, RunD greatly reduces the CPU overhead. For instance, when 200 sandboxes are started concurrently, RunD reduces 89.3%, 74.5% and 62.1% CPU overhead compared with Kata-qemu, Kata-template, and KataFC, respectively. In addition, the CPU overhead of RunD only increases slightly, when the concurrency increases. This is due to the read/write split policy and the reduction of compute-intensive operations in cgroups. Therefore, RunD\"\n",
    "sequence += \"The memory overhead of RunD is 5MB, which is the overhead of the RunD runtime itself. The memory overhead of the microVM is 94MB for a 128MB container, and 168MB for a 256MB container. The memory overhead increases with the memory specification of the container. The average memory overhead of a single microVM can be reduced by sharing the text/rodata segment among multiple microVMs. As shown in Figure 5(b), the per-microVM memory overhead of kata-qemu and kata-FireCracker reduce to 145MB and 71MB when 1,000 VMs are deployed on a node. However, the overhead is still too large for a serverless container with only 128MB memory specification.\"\n",
    "sequence += \"Currently, Alibaba serverless computing platform has adopted RunD. The platform serves almost 4 billion invocations from more than 1 million different functions per day. Figure 14 reports the sandbox startup concurrency and the corresponding startup latency from six nodes. The specification of each node is the same as our experimental setup in Table 1. The data is collected between 08:00 and 18:00 of Jan 10th, 2022. There are about 800 active sandboxes on each node, when the concurrency data is collected. The inproduction startup latency of sandboxes at high-concurrency is consistent with that reported in Section 5.4. As observed from the figure, the startup concurrency bursts at the beginning of each hour. At most 191 sandboxes are started concurrently around 10:00. RunD starts the 191 sandboxes in 1.6 seconds. We look into the function invocation logs, and find that the periodic burst is caused by the an-hour time trigger and cluster-level load balancing. The periodical burst is pervasive, as the Azure serverless platform traces [14] show the same pattern. In the figure, the sandbox startup latency occasionally increases when the concurrency is low. The long time results from the operation in loading large-scale workloads from the tenants. Although the startup\"\n",
    "sequence += \"The most closely related work to RunD is FireCracker [20], which proposes a lightweight VMM for serverless runtime. It provides fast startup within 125ms, allowing 150 VMs to start concurrently per second per node, with less than 5MB footprint per VM. However, FireCracker only serves as the hypervisor stack in the Security Container model, without other complex related processes, e.g., rootfs [52]. By contrast, RunD investigates the guest-to-host solution through all stacks and provides higher concurrency and density. Higher-density deployment. Regarding serverless computing, in the space of higher function deployment density of Secure Containers and VMs [57], the key is designing a more lightweight container runtime both in guest and host. Unikernel [36, 37, 43, 47] runs as a built-in GuestOS without necessary add-ons, demonstrating great potential for deploying containers with less overhead. Kuo [33] Explores lightweight guest kernel configurations for use in Unikernel environments, which has similarity to the approach towards reducing guest kernel size. However, Unikernel is hard to be changed once after compilation with the application. Its compile-time invariance results in poor flexibility in practice. SAND [21] adopts the multi-container-per-VM model to amortize the memory footprint of sandboxing. However, they do not further investigate the utilization impact of memory fragmentations in a real-system with high-density deployment. Gsight [61] observes that fine-grained functionlevel profiling can expose more predictability system-level features in the partial interference. With a more accurate interference predicting [27, 44], the function density can get improved with QoS guaranteed. The above studies make sense in improving the effective density with less interference for serverless. They are orthogonal to our work, because RunD is motivated to improve the maximum deployment density on a signe node. Higher-concurrency startup. In the space of higher function startup concurrency, recent approaches leverage the container prewarm pool [9, 40, 49, 58]. The state-of-the-art on container prewarming, SOCK [42], uses a benefit-to-cost model to select packages pre-installed in zygotes, and builds a tree cache to ensure that the forked zygote container does not import any additional packages other than the private ones the handler specifies. The C/R (Checkpoint/Restore) [7, 31, 39] supporting the VM snapshotting [10, 28, 29, 41, 54] captures the state of a running instance as a checkpoint, and then restores it once cold startup. Observing that most functions only access a small fraction of the files and mem-\"\n",
    "\n",
    "\n",
    "output = pipe(sequence, max_new_tokens=2, do_sample=True, temperature=0.9)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextGenerationPipeline中使用了自回归。首次运行时使用给定token作为输入，后面每次输入一个token。key cache size的解释为`(num_batch, num_heads, seq_len, head_dim)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力层中的`q_proj`, `k_proj`, `v_proj`分别是生成$Q$、$K$、$V$的线性层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           Linear\n",
      "\u001b[0;31mString form:\u001b[0m    Linear(in_features=4096, out_features=4096, bias=False)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/faiss/lib/python3.11/site-packages/torch/nn/modules/linear.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
      "\n",
      "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "Args:\n",
      "    in_features: size of each input sample\n",
      "    out_features: size of each output sample\n",
      "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "        Default: ``True``\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "    - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "Attributes:\n",
      "    weight: the learnable weights of the module of shape\n",
      "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "            If :attr:`bias` is ``True``, the values are initialized from\n",
      "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> m = nn.Linear(20, 30)\n",
      "    >>> input = torch.randn(128, 20)\n",
      "    >>> output = m(input)\n",
      "    >>> print(output.size())\n",
      "    torch.Size([128, 30])\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule."
     ]
    }
   ],
   "source": [
    "attn.q_proj?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Llama的transformers实现中, kv cache由注意力层的中间变量`past_key_value`表示。对于生成式模型，这是一个`DynamicCache`类，定义如下：\n",
    "```python\n",
    "class DynamicCache(Cache):\n",
    "    \"\"\"\n",
    "    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n",
    "\n",
    "    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n",
    "    `[batch_size, num_heads, seq_len, head_dim]`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.key_cache: List[torch.Tensor] = []\n",
    "        self.value_cache: List[torch.Tensor] = []\n",
    "        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n",
    "```\n",
    "Cache在LlamaModel前向传播之前被创建，并作为forward的参数流经所有层。生命周期为一次前向传播，在整个模型一次推理之后会被返回。  \n",
    "Pipeline使用了某种自回归手段（TODO: 查清pipeline的源码）使得一次推理的输出（包括缓存）传给了下一次输入。这使得Cache的生命周期扩展到了一次pipeline。\n",
    "```python\n",
    "class LlamaForCausalLM(LlamaPreTrainedModel):\n",
    "    ...\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        ...\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sadly past_key_value is not present as a property\n",
    "getattr(attn, \"past_key_value\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicCache()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = attn.middleware\n",
    "m['past_key_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-0.3384,  0.0469,  0.0529,  ...,  0.2462, -0.1481,  0.2926],\n",
       "           [-0.0552, -0.1907,  0.0224,  ..., -0.0031,  0.0241,  0.0113],\n",
       "           [ 0.1668, -0.2733, -0.3675,  ...,  0.0711,  0.1794,  0.1150],\n",
       "           ...,\n",
       "           [-0.1707,  0.5088,  0.7048,  ..., -0.0091,  0.0510, -0.0450],\n",
       "           [-0.5672, -0.1113,  0.3188,  ..., -0.3986, -0.5565, -0.1698],\n",
       "           [ 0.1198, -0.0046, -0.0380,  ...,  0.4629,  0.6372,  0.3343]],\n",
       " \n",
       "          [[ 0.6409,  0.8173,  0.1992,  ..., -0.5353,  0.1221, -0.4024],\n",
       "           [-0.2616,  0.2207,  0.2423,  ..., -0.8128,  0.2036, -0.6366],\n",
       "           [-0.2250,  0.2389,  0.1041,  ..., -0.2533,  0.4262, -0.2529],\n",
       "           ...,\n",
       "           [-0.5541, -0.4107, -0.2449,  ..., -0.2505, -0.4789, -0.1813],\n",
       "           [ 0.7244,  1.0885,  0.3598,  ..., -0.2154, -0.0510, -0.4382],\n",
       "           [ 0.0048,  0.2987, -0.1986,  ..., -0.0262, -0.3108,  0.2186]],\n",
       " \n",
       "          [[-0.4030,  0.0613,  0.3968,  ...,  1.6628,  1.7025,  1.5833],\n",
       "           [ 0.1904,  0.2751,  0.2702,  ...,  1.7054,  1.7855,  1.6563],\n",
       "           [ 0.2003,  0.5984, -0.1705,  ..., -0.1777, -0.2218, -0.1902],\n",
       "           ...,\n",
       "           [ 0.6000, -0.5643,  0.1844,  ..., -1.0044, -0.7116, -0.5344],\n",
       "           [-0.2824,  0.0945,  0.2416,  ..., -1.7823, -0.9702, -1.1098],\n",
       "           [-0.0366, -0.6077, -0.1400,  ...,  0.5183,  0.2557,  0.4989]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.9503, -0.1799, -0.5858,  ..., -0.2368, -0.4630, -0.9233],\n",
       "           [-1.1513,  0.5311, -0.5277,  ..., -0.5139, -0.8576, -1.1831],\n",
       "           [ 0.1447,  0.1474,  0.0481,  ..., -0.6042,  0.0436, -0.6264],\n",
       "           ...,\n",
       "           [ 0.1466, -0.1013, -0.0281,  ..., -0.2035,  0.7612, -0.1796],\n",
       "           [ 1.1157, -1.2339, -0.7844,  ...,  1.0270,  0.6859,  1.3678],\n",
       "           [-0.1611,  0.0501,  0.0045,  ..., -0.6024,  0.1171,  0.4662]],\n",
       " \n",
       "          [[-0.3664, -0.4570, -0.4648,  ..., -0.3221,  0.1075,  0.1106],\n",
       "           [-0.0526,  0.3218, -0.9441,  ..., -0.4499,  0.1924,  0.1916],\n",
       "           [-0.1769,  0.7228,  0.5400,  ...,  0.4064, -0.3086, -0.3082],\n",
       "           ...,\n",
       "           [ 0.0348, -0.6472, -1.0847,  ...,  0.2297,  0.1687,  0.1579],\n",
       "           [ 0.0333, -0.7496, -1.0082,  ..., -0.1440, -0.4782, -0.4454],\n",
       "           [ 0.4348, -0.3008,  0.1041,  ...,  0.2989,  0.4315,  0.4079]],\n",
       " \n",
       "          [[-0.0164, -0.1588, -0.2939,  ..., -0.8892,  0.4568, -0.0116],\n",
       "           [ 0.2795,  0.0824,  0.1320,  ..., -1.6940,  1.1669, -0.2308],\n",
       "           [ 0.5796, -0.1934, -0.0034,  ...,  0.7817, -0.2901,  0.1933],\n",
       "           ...,\n",
       "           [ 0.8008,  0.4834, -1.0366,  ...,  0.2148, -0.0622,  0.1807],\n",
       "           [-0.1599,  0.1590,  0.1264,  ...,  0.3842,  1.9476, -1.3997],\n",
       "           [-0.9721, -1.0551, -0.5327,  ...,  0.2719, -0.9282,  0.6047]]]]),\n",
       " tensor([[[[-4.9817e-01,  7.3213e-01,  9.3675e-01,  ..., -8.2748e-01,\n",
       "            -1.1967e+00,  5.9211e-01],\n",
       "           [-2.2452e+00, -1.7700e-01, -3.6718e-01,  ..., -8.3135e-01,\n",
       "            -1.0723e+00,  6.6087e-01],\n",
       "           [-7.4804e-01, -5.1749e-02, -1.3955e+00,  ..., -1.3555e+00,\n",
       "            -1.0654e+00,  6.8278e-01],\n",
       "           ...,\n",
       "           [ 3.7249e+00,  2.1145e+00,  2.8127e+00,  ..., -3.9981e-01,\n",
       "             1.7462e-01,  1.0284e+00],\n",
       "           [ 4.0604e+00,  1.1594e+00,  2.5265e+00,  ..., -2.8753e-01,\n",
       "             8.0432e-01,  9.9549e-01],\n",
       "           [ 3.6129e-01, -2.7139e-01,  5.8654e-01,  ..., -3.0379e-01,\n",
       "             2.9741e-01,  1.0798e+00]],\n",
       " \n",
       "          [[-6.9524e-03, -9.2644e-01, -5.2831e-02,  ..., -3.7011e-01,\n",
       "            -1.8979e-01, -4.6880e-02],\n",
       "           [-1.4920e-02,  3.2483e-01, -7.0123e-01,  ..., -8.8781e-01,\n",
       "            -3.2510e-01, -1.9994e-01],\n",
       "           [ 2.7369e-01,  9.3105e-01, -8.3872e-01,  ..., -1.2488e+00,\n",
       "            -6.6737e-01, -5.3127e-01],\n",
       "           ...,\n",
       "           [-1.8223e+00,  2.1303e+00, -3.4365e-01,  ...,  3.8022e-01,\n",
       "             1.1361e-01, -1.4039e-02],\n",
       "           [-1.3139e+00,  1.8921e+00, -1.2932e+00,  ...,  4.7314e-01,\n",
       "            -3.1278e-01, -1.9661e-01],\n",
       "           [-7.6572e-01, -2.0130e-01, -2.2654e+00,  ...,  5.2126e-01,\n",
       "            -5.5315e-02, -4.7649e-02]],\n",
       " \n",
       "          [[ 2.6911e-01,  3.1031e-01,  2.2342e-01,  ...,  1.3754e+00,\n",
       "             8.9894e-01,  3.5824e-01],\n",
       "           [ 2.4852e-01, -4.1727e-01, -1.4933e-01,  ...,  1.2739e+00,\n",
       "             7.1388e-01,  6.1079e-01],\n",
       "           [-3.1383e-02, -5.3300e-01, -2.0671e-01,  ...,  1.0642e+00,\n",
       "             5.1122e-01,  6.0203e-01],\n",
       "           ...,\n",
       "           [-8.6291e-01, -5.9171e-04, -2.2111e-01,  ..., -3.4777e-01,\n",
       "            -7.0460e-01, -8.7654e-01],\n",
       "           [ 4.6236e-01,  8.8565e-01, -1.7927e-01,  ..., -2.8710e-01,\n",
       "            -7.0449e-01, -8.7102e-01],\n",
       "           [ 1.6271e+00,  1.0597e+00, -1.0529e-01,  ..., -3.4444e-01,\n",
       "            -6.9726e-01, -8.8468e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0058e+00, -4.5012e-02,  1.0713e-01,  ...,  1.4225e-01,\n",
       "             2.9864e-02, -2.9882e-01],\n",
       "           [-2.5263e-01, -2.1867e-01,  1.9271e-01,  ...,  2.7029e-01,\n",
       "            -2.2507e-01,  2.1184e-01],\n",
       "           [-3.0800e-02, -1.8429e-01, -3.1578e-01,  ...,  6.4995e-02,\n",
       "            -3.3161e-01,  2.5884e-01],\n",
       "           ...,\n",
       "           [-2.3225e+00, -9.8772e-01,  3.2928e-01,  ...,  4.0864e-01,\n",
       "            -1.2205e-01, -8.2674e-02],\n",
       "           [-1.8126e+00, -1.2393e+00,  9.5805e-01,  ...,  3.0083e-01,\n",
       "            -1.0181e-01, -2.6280e-01],\n",
       "           [ 7.5422e-01, -1.0210e+00,  1.5671e+00,  ...,  4.6766e-01,\n",
       "            -1.7472e-01,  2.5421e-03]],\n",
       " \n",
       "          [[ 5.7542e-01,  8.0676e-02,  1.2441e-01,  ..., -8.5266e-02,\n",
       "            -6.4620e-02,  3.6647e-02],\n",
       "           [-5.5297e-02, -3.1464e-01, -1.3466e-01,  ..., -7.5455e-02,\n",
       "            -1.5786e-01,  2.8757e-03],\n",
       "           [-6.0916e-01, -1.1630e-01, -3.7554e-01,  ..., -1.9503e-03,\n",
       "            -2.7784e-01,  2.5815e-01],\n",
       "           ...,\n",
       "           [-1.8369e+00,  7.2112e-02,  1.1057e-01,  ..., -2.7121e-01,\n",
       "             8.9857e-02, -1.3840e-01],\n",
       "           [-5.8518e-01,  5.5840e-01, -4.8766e-02,  ...,  6.4751e-03,\n",
       "             2.0993e-01,  8.7601e-02],\n",
       "           [ 1.2873e+00,  4.2661e-01,  5.4362e-02,  ..., -1.3900e-01,\n",
       "             1.3552e-02, -5.1438e-02]],\n",
       " \n",
       "          [[-3.7140e-01,  2.0295e-01,  2.2803e-01,  ...,  1.0709e+00,\n",
       "            -1.0720e+00,  7.3023e-01],\n",
       "           [-2.2695e-01,  1.7067e-01,  1.7795e-01,  ...,  1.1898e+00,\n",
       "            -1.2253e+00,  8.4369e-01],\n",
       "           [ 2.6732e-01,  3.3393e-01,  1.9035e-01,  ...,  1.3207e+00,\n",
       "            -1.3247e+00,  9.0555e-01],\n",
       "           ...,\n",
       "           [ 3.1291e+00, -1.0296e+00, -1.0458e+00,  ...,  6.8363e-02,\n",
       "            -2.7152e-01, -3.6361e-01],\n",
       "           [-8.5753e-01, -2.5128e-01, -1.1947e+00,  ...,  1.8632e-04,\n",
       "            -1.3065e-01, -2.9965e-01],\n",
       "           [-4.0475e+00,  5.9147e-01, -4.3312e-01,  ...,  1.7261e-02,\n",
       "            -2.0212e-01, -3.0338e-01]]]]),\n",
       " tensor([[[[ 1.5633e-02,  3.3631e-02,  1.3669e-02,  ..., -1.7583e-01,\n",
       "            -1.0866e-01,  1.4917e-01],\n",
       "           [-1.3177e+00, -6.9871e-01,  1.8225e-01,  ...,  9.1350e-01,\n",
       "             2.3922e-01, -4.0645e-01],\n",
       "           [-2.7878e+00,  3.4683e-01,  3.7548e-01,  ...,  1.6278e+00,\n",
       "             1.4231e+00, -9.7454e-01],\n",
       "           ...,\n",
       "           [-1.6692e+00, -5.0905e-01, -2.9282e-01,  ..., -6.5591e-01,\n",
       "             1.1694e-01, -6.6482e-01],\n",
       "           [ 2.5561e+00,  4.4921e-01,  1.9947e-02,  ..., -7.7185e-01,\n",
       "            -1.0731e-02, -2.1507e-01],\n",
       "           [ 5.6164e+00,  1.0767e+00,  6.4258e-01,  ..., -5.4821e-01,\n",
       "             2.5252e-01, -3.5172e-01]],\n",
       " \n",
       "          [[-2.2819e-02,  2.5394e-02,  5.7541e-02,  ...,  2.6887e-02,\n",
       "             3.2430e-01, -3.3557e-01],\n",
       "           [-3.7976e-01, -3.1051e-01, -5.6757e-01,  ..., -3.7673e-01,\n",
       "            -2.0968e+00,  2.2198e+00],\n",
       "           [ 5.5434e-02, -5.5898e-01,  6.3056e-01,  ..., -6.4617e-01,\n",
       "            -1.7926e+00,  1.9183e+00],\n",
       "           ...,\n",
       "           [-3.6226e-01,  1.8779e-01, -5.1741e-01,  ...,  9.2327e-04,\n",
       "            -3.1565e-01,  3.3084e-01],\n",
       "           [ 2.0960e-01,  5.9946e-01, -7.6994e-01,  ...,  1.8617e-01,\n",
       "            -4.2438e-01,  1.2861e-01],\n",
       "           [-3.4469e-02,  8.7269e-01, -6.6632e-01,  ..., -7.2858e-03,\n",
       "            -2.0350e-01,  3.3116e-01]],\n",
       " \n",
       "          [[-3.3004e-02,  7.9955e-02,  2.8741e-02,  ..., -8.1134e-02,\n",
       "             3.3295e-01, -3.4069e-01],\n",
       "           [ 3.2193e+00, -2.6782e+00,  4.8037e-01,  ...,  1.8429e+00,\n",
       "            -1.4440e+00,  2.2669e+00],\n",
       "           [ 1.4805e+00, -2.5362e+00,  2.2273e-01,  ...,  2.2756e+00,\n",
       "            -1.9403e+00,  2.2518e+00],\n",
       "           ...,\n",
       "           [ 2.9697e-01,  2.3898e+00,  7.7021e-01,  ..., -1.2865e-02,\n",
       "            -3.4118e-01, -1.1516e-01],\n",
       "           [-2.1393e+00,  2.5680e+00,  5.0257e-01,  ..., -2.4534e-01,\n",
       "            -5.2534e-01, -1.3372e-01],\n",
       "           [-3.3774e+00,  1.9830e+00,  4.3073e-01,  ..., -2.6248e-01,\n",
       "            -5.1593e-01, -5.4185e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.9511e-02,  3.2425e-03,  1.9655e-02,  ...,  3.1791e-02,\n",
       "             4.1645e-02,  9.5145e-02],\n",
       "           [ 4.9408e-01,  5.7900e-02,  2.0330e-01,  ..., -1.4538e-01,\n",
       "            -2.9623e-01, -1.7965e-01],\n",
       "           [ 1.1101e+00, -4.3077e-01,  8.1976e-01,  ...,  3.4201e-01,\n",
       "            -5.7987e-01, -2.0704e-01],\n",
       "           ...,\n",
       "           [-3.3386e+00, -9.1329e-02,  4.5623e-01,  ..., -3.6127e-03,\n",
       "            -4.6386e-01, -9.2625e-02],\n",
       "           [-1.3182e+00,  4.0420e-01,  3.3108e-01,  ...,  2.2220e-03,\n",
       "            -1.5003e-01, -3.2534e-01],\n",
       "           [ 1.3459e+00,  6.8077e-01,  2.9593e-01,  ..., -2.5131e-02,\n",
       "            -1.7637e-01, -3.2703e-01]],\n",
       " \n",
       "          [[-3.3469e-03,  3.1766e-02,  7.5788e-03,  ..., -5.7694e-02,\n",
       "            -7.6306e-02,  9.6938e-03],\n",
       "           [-1.3680e+00,  1.2786e+00, -1.8221e-01,  ...,  1.1415e+00,\n",
       "            -7.4741e-02,  1.5643e-01],\n",
       "           [-2.9238e+00,  2.6592e+00,  2.3606e-01,  ...,  5.7878e-01,\n",
       "             7.3253e-01,  6.1589e-01],\n",
       "           ...,\n",
       "           [ 2.6110e-01, -4.3825e-01, -7.3080e-01,  ..., -1.1326e-01,\n",
       "             6.8599e-01,  8.1627e-01],\n",
       "           [ 2.6157e+00, -1.7849e+00, -6.1015e-01,  ..., -7.9912e-02,\n",
       "             5.3183e-01,  1.9957e-01],\n",
       "           [ 3.5044e+00, -3.2575e+00, -8.5082e-01,  ..., -2.6392e-01,\n",
       "             4.0115e-01,  7.2903e-01]],\n",
       " \n",
       "          [[-4.3523e-02, -1.9361e-02, -1.7399e-02,  ...,  1.0747e-01,\n",
       "             4.5828e-01,  7.3194e-02],\n",
       "           [ 7.8640e-01, -9.5881e-01, -1.3530e+00,  ..., -1.6999e+00,\n",
       "            -2.7614e+00, -1.5211e+00],\n",
       "           [-9.9187e-01, -1.3672e+00, -1.2042e+00,  ..., -1.3850e+00,\n",
       "            -2.3764e+00,  4.2061e-02],\n",
       "           ...,\n",
       "           [-1.8337e+00, -3.1561e-01, -7.9379e-01,  ..., -7.2077e-01,\n",
       "             1.3719e+00, -1.6869e+00],\n",
       "           [ 5.5798e-01,  4.9405e-01, -1.4554e+00,  ..., -2.7310e-01,\n",
       "             1.2046e+00, -1.0754e+00],\n",
       "           [ 2.5360e+00,  1.3496e+00, -2.3944e+00,  ..., -6.4409e-01,\n",
       "             1.3903e+00, -1.8339e+00]]]]),\n",
       " tensor([[[[ 1.1148e-02,  6.3947e-04,  6.8938e-03,  ...,  5.3908e-01,\n",
       "             5.4885e-02, -5.3505e-01],\n",
       "           [-2.5436e-01,  1.4658e-01, -5.4411e-01,  ..., -2.5491e+00,\n",
       "            -1.6604e+00,  2.6665e+00],\n",
       "           [ 8.4083e-01,  5.4299e-01, -1.6027e-01,  ..., -2.5816e+00,\n",
       "            -6.4257e-01,  2.6841e+00],\n",
       "           ...,\n",
       "           [ 8.7381e-01,  7.2941e-01,  2.5682e-01,  ...,  1.5712e+00,\n",
       "            -1.7747e+00, -9.2232e-03],\n",
       "           [ 5.1084e-01, -5.0460e-02,  6.1109e-01,  ...,  1.3542e+00,\n",
       "            -1.3687e+00, -7.7284e-01],\n",
       "           [ 4.6394e-01, -1.2132e-01,  5.4974e-01,  ...,  1.5260e+00,\n",
       "            -1.7646e+00, -5.3155e-01]],\n",
       " \n",
       "          [[ 1.7163e-03,  3.4960e-02, -2.0693e-02,  ..., -4.1766e-01,\n",
       "            -4.5312e-01, -3.5903e-01],\n",
       "           [ 1.9381e-01, -2.2870e-01,  1.2501e-01,  ...,  5.0552e-01,\n",
       "             1.7333e+00,  1.3028e+00],\n",
       "           [-4.1903e-01, -2.1091e-01,  1.0945e-01,  ...,  1.1580e+00,\n",
       "             2.0132e+00,  1.1714e+00],\n",
       "           ...,\n",
       "           [ 1.5182e+00,  1.3236e-01, -1.4701e+00,  ..., -2.0274e+00,\n",
       "             2.9792e-02,  1.3660e+00],\n",
       "           [ 1.9424e+00, -1.8450e-01, -6.2800e-01,  ..., -1.3161e+00,\n",
       "            -1.2425e-01,  9.3698e-01],\n",
       "           [ 8.1694e-01, -5.1509e-01,  1.1218e-01,  ..., -2.0235e+00,\n",
       "             7.4961e-02,  1.3673e+00]],\n",
       " \n",
       "          [[ 1.0808e-02, -1.4700e-02, -4.9651e-03,  ...,  1.1388e+00,\n",
       "            -1.0894e-01,  1.9452e-01],\n",
       "           [-4.4661e-01,  3.3404e-01, -3.0155e-01,  ..., -5.0941e+00,\n",
       "             5.8206e-01, -6.1413e-01],\n",
       "           [ 8.3679e-01, -5.0870e-01, -9.9263e-01,  ..., -5.1080e+00,\n",
       "             8.2686e-01, -1.4995e+00],\n",
       "           ...,\n",
       "           [ 4.7570e+00,  3.2463e-01,  1.8739e-02,  ...,  4.4173e-01,\n",
       "            -1.1210e+00, -5.7721e-01],\n",
       "           [ 1.7399e+00,  3.0705e-02, -5.8411e-01,  ...,  8.5353e-01,\n",
       "            -2.8830e-01, -2.6291e-01],\n",
       "           [-2.8842e+00, -1.8439e-01, -7.2744e-01,  ...,  4.2486e-01,\n",
       "            -7.8473e-01, -6.2175e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.7466e-02, -1.0033e-02,  4.8836e-03,  ...,  8.0099e-01,\n",
       "            -7.6850e-02,  6.3907e-02],\n",
       "           [-1.3587e-01, -1.2453e-01,  1.7857e-01,  ..., -2.8526e+00,\n",
       "            -4.3046e-01, -7.6981e-01],\n",
       "           [ 4.9581e-01,  1.6097e-01,  1.2555e-01,  ..., -2.7057e+00,\n",
       "            -2.8502e-01,  5.8804e-01],\n",
       "           ...,\n",
       "           [-1.3234e+00, -7.8997e-01,  7.2497e-01,  ...,  2.9641e+00,\n",
       "            -2.4440e-02, -9.4654e-01],\n",
       "           [-2.7432e-01, -4.8935e-01,  5.7509e-01,  ...,  2.4641e+00,\n",
       "            -1.2632e-01, -4.2174e-01],\n",
       "           [ 6.1650e-01,  8.7978e-01,  3.9324e-01,  ...,  2.8910e+00,\n",
       "            -1.0596e-01, -8.6634e-01]],\n",
       " \n",
       "          [[ 2.5199e-02,  3.3679e-02, -3.1029e-02,  ...,  9.9796e-02,\n",
       "            -9.6687e-02, -1.9233e-02],\n",
       "           [-3.1004e-01, -2.6455e-01,  1.0364e-01,  ...,  1.9835e-01,\n",
       "             1.0338e+00,  1.1686e+00],\n",
       "           [-1.7465e-01, -4.5834e-01,  3.1882e-01,  ..., -2.2676e-01,\n",
       "            -1.2753e+00,  1.5491e+00],\n",
       "           ...,\n",
       "           [ 1.8218e+00,  8.4494e-01,  2.3607e-01,  ..., -2.6397e-01,\n",
       "             1.3824e+00,  7.1969e-01],\n",
       "           [ 3.2649e+00, -6.8863e-02,  3.7850e-02,  ..., -1.3965e-01,\n",
       "             9.5181e-01,  3.8901e-01],\n",
       "           [ 2.0735e+00, -9.7519e-01,  7.6786e-02,  ..., -2.8445e-01,\n",
       "             1.2202e+00,  8.0184e-01]],\n",
       " \n",
       "          [[ 2.5995e-02,  2.1602e-02, -2.0517e-03,  ...,  6.9037e-01,\n",
       "            -6.8450e-01, -5.6963e-01],\n",
       "           [ 1.2810e-01, -9.0583e-02,  2.4489e-01,  ..., -3.5125e+00,\n",
       "             1.0258e+00,  3.0011e+00],\n",
       "           [ 1.8771e-01,  2.8861e-01,  3.0552e-01,  ..., -4.4029e+00,\n",
       "             1.9978e+00,  1.3269e+00],\n",
       "           ...,\n",
       "           [ 3.8791e-01, -6.8014e-01, -2.3938e-01,  ..., -1.8201e+00,\n",
       "             1.1861e+00,  1.0605e+00],\n",
       "           [-2.0797e-01,  1.1043e-02, -4.3909e-01,  ..., -9.6244e-01,\n",
       "             1.1480e+00,  1.1075e+00],\n",
       "           [-5.0040e-01,  4.3280e-01, -2.3350e-01,  ..., -1.1734e+00,\n",
       "             2.1028e+00,  2.7069e-01]]]]),\n",
       " tensor([[[[-5.6063e-03,  4.8642e-02,  2.8940e-02,  ...,  5.0698e-02,\n",
       "             9.7031e-02,  1.8213e-01],\n",
       "           [ 1.5078e+00,  5.5229e-01,  1.9266e-01,  ...,  1.1191e+00,\n",
       "            -5.4725e-01, -1.6098e+00],\n",
       "           [ 4.9109e-01,  1.6222e-01, -1.0477e-02,  ..., -8.0023e-02,\n",
       "            -7.1553e-01, -2.1087e+00],\n",
       "           ...,\n",
       "           [-5.1389e+00,  5.3299e-01,  4.1152e-01,  ...,  1.3985e+00,\n",
       "            -1.3995e+00, -1.1636e+00],\n",
       "           [-4.6369e+00,  5.9513e-01,  1.6913e-01,  ...,  4.6687e-01,\n",
       "            -1.0242e+00, -2.5445e-01],\n",
       "           [-2.5786e-01, -9.6582e-02, -2.8180e-01,  ...,  1.3426e+00,\n",
       "            -1.3427e+00, -1.1175e+00]],\n",
       " \n",
       "          [[ 9.8497e-04, -1.5522e-02, -1.4622e-02,  ...,  7.5054e-01,\n",
       "             2.1193e-01,  4.8619e-01],\n",
       "           [ 1.9055e-01,  3.1092e-01,  2.5758e-01,  ..., -1.1334e+00,\n",
       "            -1.4531e+00,  2.9304e-01],\n",
       "           [-7.4982e-02, -3.0991e-01, -8.0556e-02,  ..., -6.4791e-01,\n",
       "            -6.3227e-01,  1.6742e-01],\n",
       "           ...,\n",
       "           [-1.1010e+00,  6.8003e-01,  3.9145e-01,  ...,  2.3658e+00,\n",
       "             1.7506e+00, -4.1240e-01],\n",
       "           [-2.3722e+00,  8.2053e-01,  9.6577e-02,  ...,  1.9698e+00,\n",
       "             1.5713e+00, -6.6995e-01],\n",
       "           [-1.8892e+00,  6.6553e-01, -3.7136e-01,  ...,  1.8654e+00,\n",
       "             1.6079e+00, -6.7016e-01]],\n",
       " \n",
       "          [[-5.9725e-03, -4.7265e-02,  5.8450e-02,  ..., -5.8465e-02,\n",
       "            -1.5182e-01,  1.4593e-01],\n",
       "           [ 1.3189e-01,  5.4680e-02,  3.2645e-01,  ...,  9.2911e-01,\n",
       "             1.8414e+00,  2.3053e-01],\n",
       "           [-2.7009e-02, -2.7638e-01,  3.6760e-01,  ..., -1.6196e-01,\n",
       "             1.4514e+00,  5.5949e-01],\n",
       "           ...,\n",
       "           [-1.6170e+00,  2.3740e-02, -5.6743e-02,  ..., -6.5824e-01,\n",
       "            -5.0364e-02, -1.4665e-02],\n",
       "           [-9.1083e-01,  2.4013e-01,  5.7864e-01,  ..., -3.2547e-01,\n",
       "            -9.7079e-02, -1.4330e-01],\n",
       "           [ 1.1194e+00,  4.1966e-01,  1.0831e+00,  ..., -8.0748e-01,\n",
       "             1.6107e-01, -8.7995e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.0318e-02, -1.0182e-02,  2.6482e-02,  ..., -1.5553e-01,\n",
       "            -2.2534e-01, -1.5685e-01],\n",
       "           [ 1.5547e-01,  6.1509e-01, -8.9723e-01,  ...,  6.7041e-01,\n",
       "            -9.3042e-02,  1.3555e-01],\n",
       "           [ 3.3164e-01,  6.5974e-01, -8.9269e-01,  ..., -4.1650e-01,\n",
       "            -9.5297e-01, -2.4636e-01],\n",
       "           ...,\n",
       "           [ 6.0929e-01, -1.1158e-01, -4.0750e-01,  ...,  4.4697e-01,\n",
       "             9.5094e-01,  1.4240e+00],\n",
       "           [-2.4720e+00, -1.0202e-01, -2.6978e-01,  ...,  5.4234e-01,\n",
       "             8.6434e-01,  1.3033e+00],\n",
       "           [-3.9438e+00, -2.8355e-02, -2.6640e-01,  ...,  3.8854e-01,\n",
       "             8.5018e-01,  1.5428e+00]],\n",
       " \n",
       "          [[-1.0794e-02, -3.3850e-02,  2.2665e-02,  ..., -2.5653e-01,\n",
       "            -4.1183e-01, -4.0122e-01],\n",
       "           [ 1.4357e-01,  4.7627e-01, -5.0332e-01,  ...,  1.9139e+00,\n",
       "             2.1248e+00, -3.8505e-01],\n",
       "           [ 8.1612e-01, -4.2728e-01, -4.7886e-01,  ...,  1.0489e+00,\n",
       "            -2.7638e-01,  4.4470e-01],\n",
       "           ...,\n",
       "           [-2.2211e+00,  2.0795e-02, -7.2777e-01,  ...,  1.0930e+00,\n",
       "            -1.7145e+00,  1.5295e+00],\n",
       "           [-4.2917e+00,  2.3043e-01, -5.2691e-01,  ...,  1.1156e+00,\n",
       "            -1.3820e+00,  1.3490e+00],\n",
       "           [-3.0801e+00,  3.8960e-01, -6.5848e-01,  ...,  1.1645e+00,\n",
       "            -1.6994e+00,  1.1298e+00]],\n",
       " \n",
       "          [[ 2.5607e-04, -2.9421e-02,  1.6259e-03,  ..., -1.8425e-01,\n",
       "            -4.4435e-01, -1.2758e-01],\n",
       "           [-2.2457e-01,  3.6427e-01,  6.7250e-02,  ...,  7.8462e-01,\n",
       "             2.2553e+00, -1.1897e+00],\n",
       "           [-4.1508e-01, -2.9502e-01,  2.1989e-01,  ..., -5.0259e-01,\n",
       "            -2.2227e-01, -1.0562e+00],\n",
       "           ...,\n",
       "           [ 5.9257e+00, -6.5416e-01,  1.6438e-01,  ...,  1.1064e+00,\n",
       "            -1.5208e+00,  2.2395e-01],\n",
       "           [ 3.9666e+00, -3.0629e-01, -3.2288e-01,  ...,  1.0586e+00,\n",
       "            -1.1231e+00,  2.5732e-01],\n",
       "           [-1.3662e+00,  4.1783e-01, -6.6153e-01,  ...,  1.2581e+00,\n",
       "            -1.3209e+00,  3.3394e-01]]]]),\n",
       " tensor([[[[-5.0690e-02,  1.7129e-03, -3.8491e-03,  ...,  2.2208e-02,\n",
       "            -2.1718e-01,  2.7559e-03],\n",
       "           [-4.2563e-01, -5.0621e-01,  2.6280e-01,  ..., -4.4681e-02,\n",
       "             1.6265e-01, -3.9833e-01],\n",
       "           [-5.3765e-01, -4.8997e-01,  3.7707e-01,  ...,  5.5554e-01,\n",
       "             6.1445e-01, -3.4610e-01],\n",
       "           ...,\n",
       "           [-6.2317e-01, -3.2621e-01, -1.3613e+00,  ..., -3.1364e-01,\n",
       "            -4.1704e-01,  5.7783e-01],\n",
       "           [ 2.9515e+00,  9.2487e-01, -3.8935e-01,  ..., -5.5882e-01,\n",
       "            -2.5005e-01,  3.6645e-01],\n",
       "           [ 4.0741e+00,  1.5860e+00,  7.7619e-01,  ..., -3.6159e-01,\n",
       "            -3.5100e-01,  5.2040e-01]],\n",
       " \n",
       "          [[-1.1009e-02,  2.9769e-02, -2.6417e-02,  ...,  2.3187e-03,\n",
       "            -1.7838e-02, -1.6454e-02],\n",
       "           [-3.2565e-01, -3.2752e-01, -2.4711e-01,  ...,  1.6141e+00,\n",
       "             2.9223e-01, -3.1763e-01],\n",
       "           [ 1.2935e-01,  1.2072e-01, -3.8100e-01,  ...,  1.1430e+00,\n",
       "             1.8921e+00,  4.0476e-01],\n",
       "           ...,\n",
       "           [-1.9314e+00, -4.4667e-02, -5.8565e-01,  ...,  1.4133e-01,\n",
       "             9.9045e-01, -4.6615e-01],\n",
       "           [-9.0127e-01, -7.2422e-01, -6.6051e-01,  ...,  1.4286e-01,\n",
       "             7.3927e-01, -4.9873e-01],\n",
       "           [ 9.6622e-01, -8.2775e-01, -4.1801e-01,  ...,  5.8859e-02,\n",
       "             7.7219e-01, -3.5330e-01]],\n",
       " \n",
       "          [[ 1.1011e-02,  1.3647e-02, -1.3086e-04,  ..., -1.5939e-01,\n",
       "             3.5254e-01, -8.3968e-01],\n",
       "           [-9.3610e-01,  4.5199e-02,  8.6328e-02,  ..., -3.7093e-01,\n",
       "            -1.4413e-02,  3.4557e+00],\n",
       "           [ 1.6261e+00,  4.5516e-01, -9.7840e-01,  ...,  5.4683e-01,\n",
       "            -8.4438e-01,  2.6265e+00],\n",
       "           ...,\n",
       "           [-4.7067e-01,  3.3941e-01,  4.8549e-01,  ...,  6.0740e-01,\n",
       "            -5.7276e-01, -1.3431e-02],\n",
       "           [-1.0386e+00,  4.2983e-01,  4.9010e-01,  ...,  5.6615e-01,\n",
       "            -5.1743e-01,  1.9246e-01],\n",
       "           [-4.8150e-01, -2.2498e-01,  3.4449e-01,  ...,  7.3735e-01,\n",
       "            -4.9319e-01,  6.1414e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.1933e-02, -1.3467e-02,  1.0286e-02,  ..., -4.1545e-01,\n",
       "            -4.0028e-01,  7.4273e-02],\n",
       "           [-2.7257e-01,  3.5864e-01,  1.0060e-01,  ...,  2.1306e+00,\n",
       "             1.3199e+00, -2.0527e-01],\n",
       "           [-6.1051e-01,  3.1768e-01,  2.6355e-01,  ...,  9.5391e-01,\n",
       "             1.8052e+00,  1.1460e+00],\n",
       "           ...,\n",
       "           [-1.8352e+00, -6.2602e-01,  4.1926e-01,  ...,  2.0250e+00,\n",
       "            -4.6576e-01,  1.5756e-01],\n",
       "           [ 9.7089e-01, -5.5092e-01,  3.0619e-01,  ...,  1.7858e+00,\n",
       "            -5.0891e-01,  2.1042e-01],\n",
       "           [ 3.2785e+00, -2.0173e-01,  1.8161e-01,  ...,  1.9532e+00,\n",
       "            -4.1275e-01,  2.6102e-01]],\n",
       " \n",
       "          [[-3.6209e-02,  2.2174e-02, -7.5109e-03,  ..., -2.2214e-01,\n",
       "            -1.0314e-01, -1.0649e-01],\n",
       "           [ 2.7347e-01, -4.7298e-01,  9.1648e-02,  ...,  9.3050e-01,\n",
       "             6.1494e-01,  1.6371e-01],\n",
       "           [ 7.3172e-01, -7.1802e-02, -5.0726e-02,  ...,  5.2100e-01,\n",
       "             1.7241e+00, -8.3255e-02],\n",
       "           ...,\n",
       "           [ 7.1985e-01,  1.3174e+00,  1.6374e-01,  ...,  1.0357e+00,\n",
       "             1.5155e+00, -1.2105e-01],\n",
       "           [-3.3499e+00, -2.8274e-01,  8.5132e-02,  ...,  1.3053e+00,\n",
       "             1.2436e+00, -1.9910e-01],\n",
       "           [-4.7145e+00, -1.4560e+00, -1.5443e-01,  ...,  9.6093e-01,\n",
       "             1.4281e+00, -5.7583e-04]],\n",
       " \n",
       "          [[ 3.1072e-02, -1.2230e-03,  2.0690e-03,  ...,  8.8320e-01,\n",
       "            -1.1501e-01,  7.3023e-01],\n",
       "           [ 3.9752e-02,  5.2387e-01, -4.4668e-01,  ..., -2.0089e+00,\n",
       "             1.3859e+00, -2.2431e+00],\n",
       "           [-6.4738e-01, -4.1588e-01, -3.0187e-01,  ..., -1.2251e+00,\n",
       "             2.0114e+00, -2.1677e+00],\n",
       "           ...,\n",
       "           [-2.7441e-02,  6.1690e-01, -6.7657e-01,  ...,  1.0925e+00,\n",
       "            -2.6790e-01, -1.4613e+00],\n",
       "           [ 6.2576e-01, -1.5207e-02, -1.2098e-01,  ...,  1.2320e+00,\n",
       "            -8.2041e-02, -1.7696e+00],\n",
       "           [ 8.3148e-01, -7.2484e-01,  2.9405e-01,  ...,  9.4537e-01,\n",
       "            -9.2958e-02, -1.5208e+00]]]]),\n",
       " tensor([[[[-1.1460e-02,  7.9478e-03,  3.3263e-03,  ..., -1.2876e-01,\n",
       "            -2.7588e-01,  5.2371e-01],\n",
       "           [ 1.1268e+00, -6.3933e-01,  6.4735e-03,  ..., -3.2822e-01,\n",
       "             1.8415e+00, -1.5190e+00],\n",
       "           [ 8.0103e-01, -4.7670e-01,  6.5688e-01,  ...,  6.8749e-01,\n",
       "             2.0780e+00, -2.0041e+00],\n",
       "           ...,\n",
       "           [-2.8284e-01, -1.8136e-02,  5.8076e-02,  ..., -4.1983e-01,\n",
       "            -1.7700e+00, -7.5734e-02],\n",
       "           [-8.9648e-01, -3.3270e-01, -1.2831e-01,  ..., -1.6798e-01,\n",
       "            -1.6496e+00,  2.6072e-01],\n",
       "           [-7.9813e-01, -3.7997e-01, -3.4729e-01,  ..., -4.8071e-01,\n",
       "            -1.7362e+00,  2.1885e-02]],\n",
       " \n",
       "          [[-5.7569e-03,  1.8988e-02, -1.2445e-02,  ...,  1.2560e+00,\n",
       "            -2.7708e-01, -3.4085e-01],\n",
       "           [ 9.5057e-02,  3.1269e-01,  4.5723e-01,  ..., -2.7118e+00,\n",
       "             2.4815e-01,  1.6598e+00],\n",
       "           [ 7.3804e-01, -5.2301e-01,  4.3968e-01,  ..., -3.6329e+00,\n",
       "             1.0878e-01,  1.9886e+00],\n",
       "           ...,\n",
       "           [-1.7008e+00,  3.7452e-01, -3.4312e-01,  ...,  6.9844e-01,\n",
       "             1.0011e+00, -1.0017e+00],\n",
       "           [-2.6265e+00,  4.0474e-01, -1.3169e-01,  ...,  9.0453e-01,\n",
       "             8.6743e-01, -1.0979e+00],\n",
       "           [-1.1036e+00,  8.9494e-02,  1.0863e-01,  ...,  5.6619e-01,\n",
       "             8.3871e-01, -1.0705e+00]],\n",
       " \n",
       "          [[ 3.7096e-02, -7.2613e-03,  4.3130e-02,  ...,  6.3814e-02,\n",
       "            -1.6501e-03, -5.1956e-02],\n",
       "           [ 5.3273e-01,  4.0891e-01,  4.4563e-01,  ..., -1.0777e+00,\n",
       "             2.3162e-01, -5.0258e-01],\n",
       "           [ 2.3587e-01,  2.6381e-01,  1.6875e-01,  ..., -1.2582e+00,\n",
       "             7.8332e-01, -2.1457e+00],\n",
       "           ...,\n",
       "           [ 1.0950e+00, -1.7001e-01, -4.2316e-01,  ..., -1.1928e+00,\n",
       "            -4.6960e-02, -1.3018e-01],\n",
       "           [ 1.0211e+00, -9.3739e-02, -4.0197e-01,  ..., -9.8660e-01,\n",
       "            -2.0693e-01, -1.7570e-01],\n",
       "           [-1.2981e-01,  1.1100e-01, -2.9257e-01,  ..., -1.2236e+00,\n",
       "            -1.2359e-01, -2.3157e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.8798e-02,  4.4662e-02, -9.8228e-04,  ...,  1.5894e-01,\n",
       "             1.4402e-01, -1.0915e-01],\n",
       "           [ 5.3667e-01,  3.1156e-01,  1.9432e+00,  ...,  4.0445e-01,\n",
       "            -2.5841e-02,  2.2268e-01],\n",
       "           [ 6.9867e-01,  8.2077e-01,  1.6605e+00,  ..., -4.0982e-01,\n",
       "            -1.9672e+00, -1.6210e-02],\n",
       "           ...,\n",
       "           [ 1.7783e+00, -8.7875e-01,  1.8595e-01,  ...,  4.6891e-01,\n",
       "             6.1119e-01,  3.7629e-01],\n",
       "           [-1.5795e+00, -7.8676e-01, -3.1694e-01,  ...,  3.0785e-01,\n",
       "             5.8984e-01,  3.7086e-01],\n",
       "           [-3.7541e+00, -5.4235e-02, -4.5853e-01,  ...,  3.7725e-01,\n",
       "             5.9537e-01,  3.9154e-01]],\n",
       " \n",
       "          [[-4.0421e-02, -2.2515e-02,  1.1399e-02,  ..., -2.0077e-01,\n",
       "             3.0072e-01,  5.7903e-01],\n",
       "           [-3.8156e-01, -1.4253e-01,  1.0815e-01,  ...,  1.4428e+00,\n",
       "            -2.0565e+00, -9.8469e-01],\n",
       "           [-4.7187e-01, -2.4884e-01,  1.0703e-01,  ...,  3.6365e-01,\n",
       "            -1.7405e+00, -2.4993e+00],\n",
       "           ...,\n",
       "           [ 2.5153e-01, -2.0645e-02, -4.5491e-01,  ..., -2.3950e+00,\n",
       "            -3.9794e-01,  6.5038e-01],\n",
       "           [-3.1580e-01,  6.8840e-02, -4.3660e-01,  ..., -2.3280e+00,\n",
       "            -4.7531e-01,  7.0490e-01],\n",
       "           [-6.4552e-01, -4.7937e-02, -7.3866e-02,  ..., -2.3096e+00,\n",
       "            -3.5984e-01,  1.0243e+00]],\n",
       " \n",
       "          [[ 9.4549e-03,  1.0606e-02,  3.4733e-02,  ..., -2.4043e-01,\n",
       "            -1.2637e-01,  8.7189e-02],\n",
       "           [ 5.9955e-01,  5.2784e-01,  2.3704e-01,  ...,  1.5629e+00,\n",
       "            -1.0650e+00,  1.1406e+00],\n",
       "           [-1.5040e-01, -3.1629e-02, -5.9646e-01,  ...,  1.5990e+00,\n",
       "            -4.7928e-01, -3.1599e-01],\n",
       "           ...,\n",
       "           [ 7.2927e-01,  5.1780e-01, -1.4811e-01,  ..., -1.6461e+00,\n",
       "            -1.2564e-01,  1.5752e-01],\n",
       "           [-7.0279e-01,  5.4327e-01, -1.2907e-01,  ..., -1.5552e+00,\n",
       "            -6.3943e-02,  3.1451e-01],\n",
       "           [-1.4209e+00,  4.5533e-01,  2.1858e-01,  ..., -1.6174e+00,\n",
       "            -3.3771e-02,  1.9329e-01]]]]),\n",
       " tensor([[[[-3.3293e-02,  7.4935e-03,  3.3442e-02,  ...,  8.3523e-02,\n",
       "             2.2470e-01,  4.3852e-02],\n",
       "           [ 1.4354e-01, -2.6369e-01,  5.1091e-01,  ...,  5.0689e-01,\n",
       "             1.4999e+00, -4.8117e-01],\n",
       "           [-4.8111e-01,  4.5776e-02,  2.9087e-01,  ...,  1.5879e+00,\n",
       "             1.1709e+00, -1.0241e+00],\n",
       "           ...,\n",
       "           [ 5.1832e-01,  1.5562e+00, -7.1231e-01,  ..., -3.3149e-01,\n",
       "             7.4299e-01,  1.5693e-01],\n",
       "           [ 1.8006e+00,  1.4949e+00, -8.3434e-01,  ..., -2.8576e-01,\n",
       "             6.7159e-01,  2.3681e-01],\n",
       "           [ 1.3119e+00,  3.8714e-01, -4.1885e-01,  ..., -3.8150e-01,\n",
       "             6.0711e-01,  2.0800e-01]],\n",
       " \n",
       "          [[-1.3079e-02,  2.1671e-03,  1.2532e-02,  ..., -1.6683e-01,\n",
       "            -4.8518e-03,  1.8434e-02],\n",
       "           [-3.7682e-01, -1.6313e-01, -2.5940e-02,  ...,  6.7754e-01,\n",
       "            -4.8328e-01,  5.4180e-01],\n",
       "           [ 1.6367e-01, -5.5188e-01, -8.5043e-01,  ...,  6.5157e-02,\n",
       "            -2.2283e-01,  5.8499e-01],\n",
       "           ...,\n",
       "           [-1.7215e+00,  7.6829e-01, -3.5068e-01,  ...,  3.7531e-01,\n",
       "             7.0901e-02,  7.2237e-01],\n",
       "           [-1.2891e+00,  1.2104e+00,  5.0562e-02,  ...,  3.9580e-01,\n",
       "            -2.2997e-02,  6.6219e-01],\n",
       "           [ 2.7385e-01,  6.8632e-01,  4.8018e-01,  ...,  3.4224e-01,\n",
       "             1.6503e-01,  7.5431e-01]],\n",
       " \n",
       "          [[ 3.7940e-02, -3.5633e-02, -1.4053e-02,  ..., -9.8385e-02,\n",
       "            -6.8396e-02, -8.2265e-02],\n",
       "           [-1.0485e-01,  1.0026e+00, -1.3375e-01,  ...,  1.2292e+00,\n",
       "             3.8361e-01, -2.0918e+00],\n",
       "           [-1.3326e-01,  6.1989e-01, -4.7439e-02,  ...,  3.6177e-01,\n",
       "            -4.2310e-01, -9.9196e-01],\n",
       "           ...,\n",
       "           [ 3.6176e-01, -3.1845e-01, -2.5928e-01,  ..., -5.9219e-01,\n",
       "             2.4551e-01,  7.5350e-01],\n",
       "           [ 6.7814e-01,  1.5406e-01,  3.0028e-01,  ..., -6.1371e-01,\n",
       "             5.2500e-02,  6.8069e-01],\n",
       "           [ 2.5041e-01,  4.6442e-01,  6.1695e-01,  ..., -5.5720e-01,\n",
       "             2.3278e-01,  6.0801e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.2458e-03,  2.9707e-02, -5.0323e-03,  ..., -1.6370e-01,\n",
       "            -1.2513e-01, -2.9078e-03],\n",
       "           [ 2.3915e-01,  1.0680e-01, -2.4090e-01,  ...,  5.5138e-01,\n",
       "             1.1411e+00,  4.8873e-02],\n",
       "           [ 2.8780e-02,  4.2462e-02,  2.0497e-01,  ..., -1.6266e-02,\n",
       "             2.1377e+00,  6.6704e-01],\n",
       "           ...,\n",
       "           [ 4.5119e-01, -1.3701e+00,  9.0581e-01,  ..., -1.7128e+00,\n",
       "            -5.8676e-01,  1.0328e+00],\n",
       "           [-1.0427e+00,  5.0960e-01, -5.7833e-01,  ..., -1.5515e+00,\n",
       "            -4.8985e-01,  1.1215e+00],\n",
       "           [-1.3176e+00,  1.9730e+00, -1.9937e+00,  ..., -1.7346e+00,\n",
       "            -7.0812e-01,  1.0010e+00]],\n",
       " \n",
       "          [[-6.4680e-03,  3.4100e-03,  5.4426e-02,  ..., -6.8641e-01,\n",
       "            -3.0081e-03,  4.1656e-02],\n",
       "           [-3.7191e-01, -1.0226e-01, -1.0308e+00,  ...,  4.3821e+00,\n",
       "             1.5743e-01, -2.0310e+00],\n",
       "           [-3.5105e-01,  1.0421e-01, -1.0258e-02,  ...,  5.3315e+00,\n",
       "            -7.8906e-01, -1.5200e+00],\n",
       "           ...,\n",
       "           [-1.6305e+00,  5.1996e-01,  1.8110e-01,  ...,  1.4034e+00,\n",
       "             4.0161e-02, -2.5146e+00],\n",
       "           [ 5.7844e-01,  7.7711e-01, -3.5077e-01,  ...,  1.2070e+00,\n",
       "            -1.4125e-01, -2.3980e+00],\n",
       "           [ 2.4926e+00,  4.5648e-01, -7.7875e-01,  ...,  1.3411e+00,\n",
       "             1.5838e-01, -2.4715e+00]],\n",
       " \n",
       "          [[-6.6866e-03, -2.8782e-02,  1.7758e-02,  ..., -1.6256e+00,\n",
       "             9.9534e-02,  3.4481e-02],\n",
       "           [ 1.1518e+00,  5.6933e-01, -2.1101e-01,  ...,  4.7641e+00,\n",
       "            -4.5272e-01,  7.3531e-01],\n",
       "           [-3.1173e-01, -1.0031e-01, -2.0701e-01,  ...,  3.5202e+00,\n",
       "            -7.5674e-01,  1.6860e+00],\n",
       "           ...,\n",
       "           [ 1.3909e-01, -1.8330e-01, -1.7388e-01,  ..., -8.5604e-01,\n",
       "            -3.6414e-01,  2.9086e-01],\n",
       "           [ 4.5761e-01, -1.1322e-02,  2.5759e-01,  ..., -9.2537e-01,\n",
       "            -1.7947e-01,  3.9586e-01],\n",
       "           [ 3.9299e-01,  9.6998e-02,  9.2573e-02,  ..., -8.0165e-01,\n",
       "            -4.8396e-01,  2.3733e-01]]]]),\n",
       " tensor([[[[ 1.3052e-02,  7.5221e-03, -6.3435e-03,  ..., -1.5181e-01,\n",
       "             3.1876e-01,  5.5445e-01],\n",
       "           [-1.8806e-01,  2.5810e-01, -1.2649e-01,  ...,  1.2208e+00,\n",
       "            -9.9535e-01,  6.9925e-02],\n",
       "           [ 3.0263e-02, -5.1835e-02, -5.2638e-01,  ...,  6.3949e-01,\n",
       "            -4.5568e-01,  2.7766e-01],\n",
       "           ...,\n",
       "           [ 9.7568e-02, -1.4726e-01, -2.7610e-01,  ..., -5.7800e-02,\n",
       "            -6.9199e-01,  2.2720e+00],\n",
       "           [ 1.6996e+00, -5.5082e-01, -2.8217e-01,  ..., -3.0684e-01,\n",
       "            -5.9792e-01,  2.0480e+00],\n",
       "           [ 1.7097e+00, -4.4231e-01, -2.4538e-01,  ..., -7.4539e-02,\n",
       "            -6.3847e-01,  2.2617e+00]],\n",
       " \n",
       "          [[ 1.7257e-02, -1.5331e-02,  2.9086e-02,  ..., -1.0373e-01,\n",
       "            -1.5370e-01,  4.6831e-01],\n",
       "           [ 8.5427e-01, -3.8208e-01, -4.9692e-02,  ...,  1.0515e+00,\n",
       "             1.4912e+00,  3.1897e-01],\n",
       "           [-5.6137e-02,  2.7970e-01, -5.7111e-01,  ...,  7.8570e-01,\n",
       "             7.3068e-01, -1.7658e+00],\n",
       "           ...,\n",
       "           [-3.6113e+00, -6.0348e-01, -1.0998e+00,  ...,  1.8676e-02,\n",
       "             1.3686e+00, -1.5758e+00],\n",
       "           [ 3.5930e-01, -7.7178e-01, -6.1121e-01,  ..., -1.5048e-01,\n",
       "             1.1480e+00, -1.2878e+00],\n",
       "           [ 3.9830e+00, -4.0126e-01,  1.0117e-02,  ..., -1.1973e-02,\n",
       "             1.2760e+00, -1.4720e+00]],\n",
       " \n",
       "          [[ 2.3807e-02,  9.2062e-03, -3.4588e-02,  ..., -1.4310e-01,\n",
       "             3.1485e-01,  2.6896e-01],\n",
       "           [ 4.7608e-01, -8.7423e-02,  2.2352e-03,  ...,  5.3508e-01,\n",
       "            -1.2242e+00,  3.1533e-01],\n",
       "           [-3.2633e-01, -6.6480e-01, -3.0923e-01,  ...,  4.0713e-01,\n",
       "            -8.1636e-01, -8.0063e-01],\n",
       "           ...,\n",
       "           [-4.8519e+00,  6.5242e-01,  7.6926e-01,  ..., -3.1624e-01,\n",
       "            -8.5120e-01, -1.0354e+00],\n",
       "           [-3.9193e+00,  4.2267e-01,  3.4487e-01,  ..., -4.4790e-01,\n",
       "            -9.2259e-01, -8.1773e-01],\n",
       "           [ 5.9345e-01,  7.8938e-02, -1.9750e-01,  ..., -2.6645e-01,\n",
       "            -9.2058e-01, -9.8398e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.4841e-03,  2.3419e-02,  1.2714e-02,  ...,  1.2538e-01,\n",
       "            -2.1114e-01, -1.5325e-01],\n",
       "           [-3.8094e-01,  4.2036e-01, -2.5263e-01,  ...,  1.4179e-01,\n",
       "            -5.2265e-01, -8.0155e-01],\n",
       "           [ 3.9213e-01,  1.7468e-02,  2.6642e-01,  ...,  8.6956e-01,\n",
       "            -5.4141e-01, -6.0892e-01],\n",
       "           ...,\n",
       "           [ 5.1770e+00, -1.4118e-01, -1.4577e-01,  ..., -1.8316e+00,\n",
       "             1.9335e+00, -1.1778e-02],\n",
       "           [ 2.4145e+00, -7.7275e-01, -2.9886e-01,  ..., -1.8002e+00,\n",
       "             1.6884e+00,  1.9377e-01],\n",
       "           [-2.6235e+00, -1.0176e+00, -2.8829e-01,  ..., -1.7142e+00,\n",
       "             1.9232e+00, -9.6163e-02]],\n",
       " \n",
       "          [[-6.7457e-03,  4.8589e-05, -7.4475e-03,  ...,  1.9889e-02,\n",
       "            -1.2631e-01, -3.3558e-01],\n",
       "           [-1.0492e-01,  6.0650e-01,  2.5436e-01,  ...,  5.2096e-01,\n",
       "             4.4012e-01,  4.6356e-01],\n",
       "           [-5.1371e-01, -1.0302e-01,  1.6526e-01,  ..., -7.3756e-03,\n",
       "             4.8487e-01,  1.2930e+00],\n",
       "           ...,\n",
       "           [ 4.7328e+00, -2.5362e-01,  6.2760e-01,  ...,  2.1753e-01,\n",
       "             1.4365e+00,  5.5362e-01],\n",
       "           [ 2.4573e+00,  3.0481e-01,  1.0545e-01,  ...,  1.7640e-02,\n",
       "             1.2690e+00,  5.1641e-01],\n",
       "           [-1.9814e+00,  8.3496e-01, -3.3502e-01,  ...,  1.8102e-01,\n",
       "             1.3197e+00,  4.7715e-01]],\n",
       " \n",
       "          [[-1.4671e-02,  2.3851e-02,  1.2763e-02,  ...,  1.6485e+00,\n",
       "            -1.3237e-01, -2.7425e-02],\n",
       "           [ 1.6695e-01, -1.3820e-01,  3.4483e-01,  ..., -4.7319e+00,\n",
       "             3.2434e-01, -1.2335e+00],\n",
       "           [-1.4299e-01,  4.0480e-01, -1.4143e-01,  ..., -4.6124e+00,\n",
       "             1.5772e+00, -1.7048e+00],\n",
       "           ...,\n",
       "           [-5.8147e-02, -4.0017e-01,  3.5833e-01,  ...,  6.6113e-01,\n",
       "            -5.5616e-01, -1.7856e+00],\n",
       "           [ 3.2316e-01, -6.0849e-01, -2.8522e-03,  ...,  6.5531e-01,\n",
       "            -4.6334e-01, -1.7669e+00],\n",
       "           [ 4.4527e-01, -3.9106e-01, -4.2028e-01,  ...,  6.7996e-01,\n",
       "            -4.9079e-01, -1.8193e+00]]]]),\n",
       " tensor([[[[ 2.7290e-02, -8.9390e-03,  9.6592e-03,  ..., -1.1689e-01,\n",
       "             4.4520e-01,  1.6552e+00],\n",
       "           [-7.7938e-01,  5.9898e-01, -1.4226e-01,  ...,  1.1725e+00,\n",
       "            -8.9285e-01, -4.9098e+00],\n",
       "           [-4.7462e-01,  4.2003e-01, -4.4149e-01,  ...,  1.6520e+00,\n",
       "            -2.2229e+00, -5.9524e+00],\n",
       "           ...,\n",
       "           [-2.2978e+00, -4.8460e-01, -6.2307e-01,  ..., -6.3781e-01,\n",
       "            -1.1565e+00, -3.0617e+00],\n",
       "           [ 3.7904e-01, -9.4492e-01, -4.6919e-01,  ..., -7.8348e-01,\n",
       "            -1.0719e+00, -2.7523e+00],\n",
       "           [ 2.6473e+00, -7.0391e-01,  1.0118e-01,  ..., -6.1295e-01,\n",
       "            -1.1887e+00, -2.9862e+00]],\n",
       " \n",
       "          [[-1.8826e-02,  1.1109e-02, -8.7038e-03,  ..., -3.5727e-01,\n",
       "             2.2353e-01,  2.2043e-01],\n",
       "           [ 1.3808e+00, -1.8308e-01, -2.1020e-01,  ..., -4.6775e-01,\n",
       "            -1.3664e+00,  1.3772e-01],\n",
       "           [ 5.6866e-01, -4.3550e-01,  5.1618e-01,  ...,  3.4667e-01,\n",
       "            -2.4545e+00,  1.0061e+00],\n",
       "           ...,\n",
       "           [-2.8645e+00, -8.3991e-01, -2.9508e-02,  ..., -1.3927e+00,\n",
       "             3.4116e-01, -2.2676e+00],\n",
       "           [ 2.8740e+00, -4.4934e-01, -1.7484e-01,  ..., -1.3452e+00,\n",
       "             3.7735e-01, -2.1850e+00],\n",
       "           [ 6.0607e+00,  3.6119e-01, -2.5412e-01,  ..., -1.4961e+00,\n",
       "             3.6587e-01, -2.2229e+00]],\n",
       " \n",
       "          [[ 1.8683e-02,  1.8860e-02, -2.6715e-02,  ..., -7.8809e-01,\n",
       "             3.8268e-01,  3.8960e-01],\n",
       "           [ 3.7250e-01,  2.4048e-01,  1.3831e-01,  ...,  3.0752e-01,\n",
       "            -3.0158e-01, -2.1109e+00],\n",
       "           [-1.4351e-01,  4.3100e-01, -3.0022e-01,  ..., -1.2002e+00,\n",
       "            -5.6355e-01, -3.0427e+00],\n",
       "           ...,\n",
       "           [-4.1958e+00,  6.6636e-01, -7.3751e-02,  ..., -7.9810e-01,\n",
       "            -2.1214e+00, -2.1208e-01],\n",
       "           [ 9.7203e-01,  6.4594e-01, -4.5242e-01,  ..., -8.3381e-01,\n",
       "            -2.0002e+00, -2.4455e-01],\n",
       "           [ 5.3117e+00,  2.6194e-01, -6.3305e-01,  ..., -8.1938e-01,\n",
       "            -2.0999e+00, -2.1238e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 7.6011e-03, -1.5939e-02, -6.5407e-03,  ..., -2.3354e-02,\n",
       "             2.3864e-01, -1.1569e+00],\n",
       "           [ 1.1647e-01, -1.1383e-01,  1.0216e+00,  ...,  2.4084e-02,\n",
       "             1.4632e+00,  3.9811e+00],\n",
       "           [-1.4073e-01,  8.2819e-01,  1.7611e-01,  ...,  6.1885e-01,\n",
       "             1.3153e+00,  3.5216e+00],\n",
       "           ...,\n",
       "           [ 9.1433e-01, -7.7775e-02, -1.0459e-02,  ..., -5.2388e-01,\n",
       "            -3.1127e+00,  3.5448e+00],\n",
       "           [ 3.5124e-01, -3.1112e-01, -1.2551e-01,  ..., -5.9509e-01,\n",
       "            -2.8271e+00,  3.2493e+00],\n",
       "           [-5.4705e-01, -3.6976e-01, -1.0416e-01,  ..., -5.0884e-01,\n",
       "            -3.0437e+00,  3.4442e+00]],\n",
       " \n",
       "          [[-3.7637e-02, -2.3803e-02,  2.2285e-03,  ...,  4.3566e-01,\n",
       "             4.0147e-01, -1.6407e-01],\n",
       "           [-6.9317e-02,  1.1896e-01,  5.3398e-01,  ..., -2.4301e+00,\n",
       "             4.6041e-02,  9.7970e-01],\n",
       "           [-5.6991e-02,  8.9482e-02, -2.9623e-01,  ..., -2.5333e+00,\n",
       "             2.6126e-01,  1.4075e+00],\n",
       "           ...,\n",
       "           [-1.9674e+00,  2.5111e-01, -1.9845e-01,  ..., -6.7103e-01,\n",
       "             1.1325e+00, -8.5977e-01],\n",
       "           [-2.0276e+00,  2.2250e-01,  1.4557e-01,  ..., -6.4784e-01,\n",
       "             1.0069e+00, -8.1715e-01],\n",
       "           [-4.4196e-01,  5.1346e-02,  4.9093e-01,  ..., -6.8729e-01,\n",
       "             1.1346e+00, -8.4885e-01]],\n",
       " \n",
       "          [[-1.1152e-02,  1.3315e-02,  6.8936e-04,  ..., -7.1766e-01,\n",
       "            -4.4069e-01,  1.0775e-01],\n",
       "           [ 3.5415e-02, -5.5140e-01,  2.2876e-01,  ...,  1.3019e+00,\n",
       "            -4.5311e-02,  1.4315e-01],\n",
       "           [-7.7396e-02, -2.3272e-01,  3.4234e-01,  ...,  7.4288e-01,\n",
       "             1.2521e+00,  2.2302e+00],\n",
       "           ...,\n",
       "           [ 6.3337e-01, -3.6985e-01, -9.1262e-01,  ..., -3.1142e-01,\n",
       "             1.1798e+00, -2.9919e+00],\n",
       "           [ 2.9281e+00, -2.8017e+00, -5.8395e-01,  ..., -3.3309e-01,\n",
       "             1.0539e+00, -2.9664e+00],\n",
       "           [ 2.3322e+00, -3.4196e+00,  1.0855e-01,  ..., -3.3662e-01,\n",
       "             1.1415e+00, -2.9996e+00]]]]),\n",
       " tensor([[[[-1.6517e-02, -8.5834e-03,  1.8454e-02,  ...,  1.3111e+00,\n",
       "             4.2230e-02,  3.9615e-01],\n",
       "           [-1.0961e-02,  6.7733e-02, -6.1808e-02,  ..., -2.0839e+00,\n",
       "            -1.8144e+00,  1.6862e-01],\n",
       "           [-2.1200e-01, -2.1713e-01, -9.7743e-01,  ..., -1.9338e+00,\n",
       "            -9.7676e-01,  6.6979e-02],\n",
       "           ...,\n",
       "           [-1.6618e+00,  6.4993e-01, -8.5157e-01,  ...,  2.1174e+00,\n",
       "            -2.3845e-01, -8.4698e-02],\n",
       "           [-3.5294e-01,  9.3698e-01, -9.4567e-01,  ...,  2.0971e+00,\n",
       "            -1.7190e-01, -3.6072e-02],\n",
       "           [ 1.1955e+00,  7.2823e-01, -6.6292e-01,  ...,  2.0877e+00,\n",
       "            -2.1007e-01, -4.2343e-02]],\n",
       " \n",
       "          [[-1.0384e-02, -1.6206e-02, -2.3825e-03,  ..., -2.4465e-01,\n",
       "            -1.5514e-01, -1.2950e-01],\n",
       "           [-5.8510e-01, -5.3670e-01,  9.8204e-02,  ...,  2.7878e-01,\n",
       "             2.6827e-01, -7.1787e-01],\n",
       "           [-1.0042e+00,  7.8755e-01,  1.7884e-01,  ..., -2.2622e+00,\n",
       "            -5.0520e-01, -1.2947e+00],\n",
       "           ...,\n",
       "           [ 2.1111e-01,  5.6694e-02, -5.4494e-01,  ..., -1.7078e+00,\n",
       "            -1.3740e-01,  6.9330e-01],\n",
       "           [ 3.2511e+00,  3.4351e-01, -1.9531e-01,  ..., -1.5529e+00,\n",
       "            -1.4132e-01,  7.1310e-01],\n",
       "           [ 3.2893e+00,  3.3883e-01,  1.8472e-01,  ..., -1.7182e+00,\n",
       "            -2.1287e-01,  7.2803e-01]],\n",
       " \n",
       "          [[-1.0983e-02, -1.3435e-02, -3.2016e-03,  ...,  2.8700e-01,\n",
       "             6.8729e-02, -6.2017e-01],\n",
       "           [-3.3297e-01,  1.9998e-02, -2.0488e-01,  ...,  3.1571e-01,\n",
       "             8.5041e-02,  3.4282e-01],\n",
       "           [-1.6566e-02, -6.9548e-01,  1.4735e-01,  ..., -8.6118e-02,\n",
       "            -4.9997e-01, -9.1208e-01],\n",
       "           ...,\n",
       "           [-1.5697e+00,  6.6301e-01, -4.7082e-01,  ...,  1.9869e-01,\n",
       "            -1.6198e-01, -7.7458e-01],\n",
       "           [-1.7230e-01,  1.2551e+00, -7.3916e-01,  ...,  2.0890e-01,\n",
       "            -3.0238e-02, -7.6905e-01],\n",
       "           [ 1.3062e+00,  1.2225e+00, -7.1855e-01,  ...,  1.9270e-01,\n",
       "            -1.0108e-01, -7.5398e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.7800e-02,  1.6452e-02, -5.9476e-05,  ...,  1.8194e-01,\n",
       "             2.3714e+00, -2.0196e-02],\n",
       "           [-2.7290e-01, -3.4207e-01,  5.3018e-01,  ..., -1.3062e+00,\n",
       "            -3.1429e+00, -1.1331e-01],\n",
       "           [-3.5947e-01,  3.0052e-01,  3.1137e-01,  ..., -2.7508e-02,\n",
       "            -3.6428e+00,  7.7946e-01],\n",
       "           ...,\n",
       "           [ 9.8861e-01,  5.5117e-01, -7.4331e-01,  ...,  5.5881e-01,\n",
       "             2.5225e+00, -2.6814e+00],\n",
       "           [ 7.6704e-02,  1.7765e-02, -3.2927e-01,  ...,  4.2813e-01,\n",
       "             2.2234e+00, -2.7336e+00],\n",
       "           [-1.0151e+00, -6.9944e-01,  3.0102e-01,  ...,  4.9569e-01,\n",
       "             2.4115e+00, -2.7145e+00]],\n",
       " \n",
       "          [[ 3.5422e-02,  8.0307e-05,  2.8740e-03,  ..., -1.0082e+00,\n",
       "             1.1033e+00, -8.3083e-01],\n",
       "           [-4.8175e-01, -5.3240e-01,  5.6391e-03,  ..., -1.2973e+00,\n",
       "            -7.2227e-01,  6.6614e-01],\n",
       "           [-6.1439e-01,  4.4528e-01, -1.8494e-01,  ..., -1.6199e+00,\n",
       "            -7.3047e-01,  1.1300e+00],\n",
       "           ...,\n",
       "           [-3.1044e+00,  4.1803e-01, -5.9632e-01,  ...,  1.2653e+00,\n",
       "            -2.4062e+00,  1.3292e+00],\n",
       "           [-7.5449e-01, -1.9331e-01, -1.6407e-01,  ...,  1.2233e+00,\n",
       "            -2.2067e+00,  1.1572e+00],\n",
       "           [ 2.3593e+00, -7.8182e-01,  3.3361e-01,  ...,  1.2166e+00,\n",
       "            -2.2354e+00,  1.2708e+00]],\n",
       " \n",
       "          [[ 3.7319e-02, -5.9383e-02, -1.0288e-02,  ..., -1.2605e-01,\n",
       "            -2.1372e-01, -4.8722e-02],\n",
       "           [ 3.4329e-01,  3.0959e-01, -2.5944e-01,  ...,  1.9678e+00,\n",
       "             1.1195e+00,  7.4174e-03],\n",
       "           [ 2.4672e-01,  2.4282e-01, -3.0494e-01,  ...,  3.3992e+00,\n",
       "             7.2278e-01, -5.7827e-01],\n",
       "           ...,\n",
       "           [ 2.2138e+00, -4.6816e-01, -7.0044e-02,  ...,  2.0001e+00,\n",
       "            -2.4597e-01, -1.7057e+00],\n",
       "           [-2.8770e-01, -7.4415e-02,  5.8400e-02,  ...,  2.0551e+00,\n",
       "            -2.6120e-01, -1.5841e+00],\n",
       "           [-2.6382e+00,  3.6629e-01,  1.5678e-01,  ...,  2.0823e+00,\n",
       "            -2.9211e-01, -1.7051e+00]]]]),\n",
       " tensor([[[[ 4.6913e-03, -1.9296e-02, -2.2183e-03,  ...,  7.7910e-02,\n",
       "            -1.3041e-01,  2.5092e-02],\n",
       "           [ 3.2954e-01,  2.1048e-01,  5.8683e-01,  ...,  1.0125e+00,\n",
       "            -6.7532e-01, -9.5839e-01],\n",
       "           [ 5.4008e-01,  5.8529e-01, -5.9721e-02,  ...,  4.0177e-01,\n",
       "            -5.3365e-01, -1.6867e+00],\n",
       "           ...,\n",
       "           [ 4.5608e+00,  6.1484e-03,  8.9124e-01,  ...,  2.2298e-01,\n",
       "            -5.8676e-01,  2.7828e-01],\n",
       "           [ 2.3653e+00,  1.0670e+00,  4.2096e-01,  ...,  1.2160e-01,\n",
       "            -5.4812e-01,  3.2966e-01],\n",
       "           [-1.8675e+00,  1.5353e+00, -1.8479e-01,  ...,  2.1354e-01,\n",
       "            -6.2751e-01,  3.0556e-01]],\n",
       " \n",
       "          [[ 2.0988e-02,  5.5956e-05,  2.1840e-02,  ...,  2.4119e-01,\n",
       "             1.7708e-01,  2.4415e-01],\n",
       "           [ 3.1938e-01,  2.1474e-01, -2.4567e-02,  ..., -6.8485e-01,\n",
       "             6.5206e-01, -1.7355e+00],\n",
       "           [ 4.2922e-01,  5.8598e-03, -4.0944e-01,  ..., -6.9768e-01,\n",
       "             1.6076e+00, -1.7084e+00],\n",
       "           ...,\n",
       "           [-8.6278e-01, -2.1017e-01, -3.4632e-01,  ..., -1.1070e-01,\n",
       "             3.5589e-01,  1.2044e+00],\n",
       "           [-3.0566e+00,  3.3060e-02, -8.5982e-02,  ..., -1.0037e-01,\n",
       "             4.2139e-01,  1.1831e+00],\n",
       "           [-2.4558e+00,  4.9958e-01,  2.7687e-01,  ..., -5.9594e-02,\n",
       "             3.4062e-01,  1.1556e+00]],\n",
       " \n",
       "          [[-2.6043e-02,  6.2351e-03,  1.5740e-02,  ..., -1.0152e+00,\n",
       "            -1.9740e-01, -2.0400e+00],\n",
       "           [-2.4464e-01,  2.4185e-01, -2.3361e-01,  ...,  1.5245e+00,\n",
       "            -3.8915e-01,  2.3985e+00],\n",
       "           [ 1.6890e-01, -1.6050e-01, -1.2641e-01,  ...,  3.1577e+00,\n",
       "            -1.4449e+00,  4.2603e+00],\n",
       "           ...,\n",
       "           [ 1.7728e+00,  5.1665e-01,  1.8091e-01,  ..., -2.1956e+00,\n",
       "            -4.0322e+00,  1.2136e+00],\n",
       "           [ 7.5562e-01,  7.6143e-01,  4.2888e-01,  ..., -1.7442e+00,\n",
       "            -3.8621e+00,  1.0964e+00],\n",
       "           [-9.1689e-01,  4.1119e-01,  4.3110e-01,  ..., -2.0304e+00,\n",
       "            -3.9203e+00,  1.0504e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.6406e-02,  1.6571e-02, -1.1450e-02,  ...,  1.8994e-01,\n",
       "             4.8301e-02,  1.2363e-01],\n",
       "           [-4.0200e-01, -1.3162e-02, -4.5757e-01,  ..., -1.3339e+00,\n",
       "            -1.1936e+00, -1.4135e+00],\n",
       "           [-6.4590e-01, -2.1493e-01, -5.1064e-01,  ..., -1.2577e+00,\n",
       "            -5.0847e-01, -7.3201e-01],\n",
       "           ...,\n",
       "           [-5.0123e-01,  4.3520e-01, -3.5802e-02,  ...,  1.6507e+00,\n",
       "             2.5857e-02, -1.3185e+00],\n",
       "           [ 2.4046e+00,  3.1734e-01,  3.7023e-01,  ...,  1.6003e+00,\n",
       "             1.7556e-03, -1.3444e+00],\n",
       "           [ 3.3531e+00, -3.2201e-02,  6.0933e-01,  ...,  1.5844e+00,\n",
       "             6.3324e-02, -1.2707e+00]],\n",
       " \n",
       "          [[-1.8737e-02, -5.4450e-03,  4.4979e-03,  ...,  2.5055e-01,\n",
       "             2.6450e+00,  3.2591e-02],\n",
       "           [-5.3490e-01,  1.9737e-01,  2.7690e-01,  ...,  2.1390e-01,\n",
       "            -2.7804e+00,  1.5163e+00],\n",
       "           [ 1.5743e-01,  7.2055e-01, -3.9169e-01,  ...,  2.2367e+00,\n",
       "            -3.2540e+00,  1.2497e+00],\n",
       "           ...,\n",
       "           [-1.4928e+00,  8.2123e-03,  2.4123e-01,  ..., -2.6825e-01,\n",
       "             2.1270e+00, -4.1382e-01],\n",
       "           [-2.9208e+00, -7.2414e-01,  8.6209e-01,  ..., -2.5563e-01,\n",
       "             1.9646e+00, -5.3136e-01],\n",
       "           [-1.9751e+00, -9.9506e-01,  1.0104e+00,  ..., -3.5476e-01,\n",
       "             2.1339e+00, -4.0058e-01]],\n",
       " \n",
       "          [[ 4.8063e-03, -4.6860e-04,  2.3758e-02,  ..., -1.5648e-01,\n",
       "            -1.2258e-01,  1.9944e-01],\n",
       "           [-3.9287e-01,  4.1577e-01, -5.7342e-01,  ..., -1.0171e+00,\n",
       "            -1.4306e+00,  3.9873e-01],\n",
       "           [-1.3479e-01, -7.5563e-01, -5.1382e-01,  ..., -7.0813e-01,\n",
       "            -1.2413e+00,  4.0547e-01],\n",
       "           ...,\n",
       "           [ 2.5662e-01,  1.4834e+00, -3.6013e-01,  ..., -7.1408e-01,\n",
       "             2.3692e-01, -4.8216e-01],\n",
       "           [-1.9203e+00,  1.0462e+00, -7.7758e-01,  ..., -6.9106e-01,\n",
       "             7.0070e-02, -4.0161e-01],\n",
       "           [-2.3638e+00, -1.7845e-02, -8.6075e-01,  ..., -6.4789e-01,\n",
       "             2.5739e-01, -4.6533e-01]]]]),\n",
       " tensor([[[[ 4.8826e-02,  4.4519e-02, -1.5939e-02,  ...,  1.9612e-02,\n",
       "             1.1377e-01, -1.2273e-01],\n",
       "           [-1.8439e-02,  3.7384e-01, -6.1843e-01,  ..., -1.0761e+00,\n",
       "            -6.5269e-01, -8.7066e-01],\n",
       "           [ 1.1470e-01, -8.6414e-03, -5.9384e-01,  ...,  6.6786e-01,\n",
       "            -4.8920e-02, -8.6468e-01],\n",
       "           ...,\n",
       "           [-2.2492e+00,  9.2866e-01, -9.1149e-01,  ...,  2.3441e+00,\n",
       "            -1.2724e-01,  1.7623e-01],\n",
       "           [ 8.5250e-01,  3.1777e-01, -1.4306e+00,  ...,  2.3086e+00,\n",
       "            -7.2466e-02,  3.9126e-02],\n",
       "           [ 3.3209e+00, -2.0158e-01, -1.2801e+00,  ...,  2.3280e+00,\n",
       "            -6.6314e-02,  1.6022e-01]],\n",
       " \n",
       "          [[ 1.9743e-02, -2.4085e-04,  3.7744e-03,  ...,  6.3897e-01,\n",
       "            -3.2047e-01,  4.6999e-01],\n",
       "           [-3.8461e-01, -2.2286e-01,  5.6056e-01,  ...,  1.0405e+00,\n",
       "             4.3542e-01, -1.8022e+00],\n",
       "           [-2.9544e-01,  2.0627e-01, -1.0570e+00,  ..., -1.6121e-01,\n",
       "             1.5740e+00, -5.7584e-01],\n",
       "           ...,\n",
       "           [ 9.4952e-02, -9.9699e-01,  1.1967e-01,  ..., -2.5939e+00,\n",
       "             7.5034e-01, -2.4468e+00],\n",
       "           [ 3.9532e+00, -1.0808e+00, -5.3533e-02,  ..., -2.5689e+00,\n",
       "             8.8102e-01, -2.2778e+00],\n",
       "           [ 4.4839e+00, -3.7996e-01, -2.8090e-01,  ..., -2.5859e+00,\n",
       "             8.3154e-01, -2.4001e+00]],\n",
       " \n",
       "          [[ 1.1162e-02, -2.4412e-04, -3.6409e-03,  ...,  6.9358e-02,\n",
       "             8.6253e-01, -5.4250e-01],\n",
       "           [ 7.4245e-02,  1.3871e-01,  1.2457e-01,  ..., -7.8723e-01,\n",
       "             4.3961e-01,  3.2159e+00],\n",
       "           [-3.6226e-01,  7.9822e-02,  1.6583e-01,  ...,  6.6770e-01,\n",
       "            -9.0485e-01,  1.6436e+00],\n",
       "           ...,\n",
       "           [ 3.9714e-01,  7.8287e-01,  3.5509e-01,  ...,  2.4861e+00,\n",
       "            -2.0540e+00,  2.6652e+00],\n",
       "           [ 1.1713e+00,  1.9765e-01,  2.9740e-01,  ...,  2.1965e+00,\n",
       "            -2.0987e+00,  2.3753e+00],\n",
       "           [ 8.4811e-01, -5.0860e-01,  1.7261e-02,  ...,  2.3918e+00,\n",
       "            -1.9397e+00,  2.7703e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.6422e-02,  2.4394e-03,  1.2689e-02,  ...,  4.9469e-01,\n",
       "             1.1877e+00,  4.8977e-01],\n",
       "           [-4.3970e-01,  1.0482e-01, -2.8200e-01,  ..., -8.5352e-01,\n",
       "            -6.2041e-01, -4.0969e-01],\n",
       "           [ 6.1240e-01, -1.4893e-01, -4.6562e-01,  ..., -1.4451e+00,\n",
       "            -7.8523e-01, -1.2730e+00],\n",
       "           ...,\n",
       "           [ 2.7836e+00,  1.3915e+00,  8.5017e-01,  ..., -6.2542e-01,\n",
       "             1.7841e+00,  3.3059e-02],\n",
       "           [-3.2110e+00,  1.0879e+00,  8.8214e-01,  ..., -6.7274e-01,\n",
       "             1.5174e+00, -1.4463e-01],\n",
       "           [-6.4740e+00,  2.2723e-01,  5.3600e-01,  ..., -6.7215e-01,\n",
       "             1.8014e+00, -8.0711e-02]],\n",
       " \n",
       "          [[ 2.4104e-02, -4.8039e-02,  4.9229e-02,  ...,  2.8988e-01,\n",
       "             2.8245e-01, -2.1747e+00],\n",
       "           [-2.6157e-01,  1.7389e-01, -3.2079e-01,  ..., -3.1988e-01,\n",
       "             5.9613e-01,  3.4950e+00],\n",
       "           [-2.2352e-01, -1.2996e-01, -6.6984e-03,  ...,  1.5416e+00,\n",
       "             4.0982e+00,  3.9833e+00],\n",
       "           ...,\n",
       "           [-1.1125e+00, -8.4978e-01,  1.1301e-01,  ...,  8.6473e-02,\n",
       "             2.1624e+00,  1.5599e+00],\n",
       "           [ 1.3411e+00,  6.8696e-01,  3.3846e-01,  ...,  3.2488e-02,\n",
       "             2.1235e+00,  1.5554e+00],\n",
       "           [ 2.7830e+00,  1.7702e+00,  4.5663e-01,  ...,  1.1380e-01,\n",
       "             2.1639e+00,  1.6120e+00]],\n",
       " \n",
       "          [[-2.8626e-02,  2.2180e-02, -5.9311e-04,  ...,  3.0851e-01,\n",
       "            -2.0940e-01, -8.5071e-01],\n",
       "           [-1.2710e-01, -3.3456e-01,  8.8869e-01,  ..., -7.6291e-01,\n",
       "             5.4269e-01,  8.0276e-01],\n",
       "           [-4.7811e-01, -4.5261e-01, -1.4822e-01,  ...,  8.0609e-01,\n",
       "            -5.3486e-01,  1.6706e+00],\n",
       "           ...,\n",
       "           [-5.8754e+00,  1.1513e+00, -8.6852e-01,  ...,  8.9242e-02,\n",
       "             4.8694e-01,  6.3680e-01],\n",
       "           [-6.5619e+00, -6.7456e-01, -5.1741e-01,  ...,  3.7069e-03,\n",
       "             3.3898e-01,  7.2660e-01],\n",
       "           [-1.6107e+00, -1.9372e+00,  7.3129e-02,  ...,  1.0244e-01,\n",
       "             4.5642e-01,  6.1074e-01]]]]),\n",
       " tensor([[[[ 2.7900e-02, -1.9844e-02,  6.2463e-04,  ..., -2.6199e-01,\n",
       "            -3.5938e-02,  3.2320e-01],\n",
       "           [ 2.0174e-01, -5.7454e-01, -2.7626e-01,  ...,  5.1634e-02,\n",
       "             7.5766e-01, -2.2469e+00],\n",
       "           [-1.7633e-01, -5.1125e-01,  5.0133e-01,  ..., -4.9724e-01,\n",
       "            -2.0970e-01, -2.8099e+00],\n",
       "           ...,\n",
       "           [-4.4153e+00,  5.6787e-02, -9.4671e-02,  ..., -1.9485e+00,\n",
       "             2.2243e+00,  3.9386e-01],\n",
       "           [-8.5097e-01, -3.5554e-01,  3.6053e-01,  ..., -1.7636e+00,\n",
       "             2.0935e+00,  2.9320e-01],\n",
       "           [ 3.4884e+00, -4.0841e-01,  7.6014e-01,  ..., -1.9423e+00,\n",
       "             2.2363e+00,  4.8091e-01]],\n",
       " \n",
       "          [[ 1.1376e-02, -9.3102e-03, -5.9994e-03,  ..., -9.7079e-02,\n",
       "            -2.3406e-02, -9.6038e-02],\n",
       "           [ 3.8799e-01,  1.9244e-01,  2.9009e-02,  ...,  7.0687e-01,\n",
       "            -1.1835e-01,  1.3612e+00],\n",
       "           [-2.6167e-01, -8.0595e-01,  3.4418e-03,  ...,  1.2669e+00,\n",
       "            -7.8171e-01,  1.0084e+00],\n",
       "           ...,\n",
       "           [ 2.6015e+00,  1.9454e-01, -1.8082e-01,  ..., -1.1456e+00,\n",
       "             7.0305e-01,  1.1781e+00],\n",
       "           [ 3.9965e+00, -2.1076e-01, -9.1873e-02,  ..., -1.0328e+00,\n",
       "             5.8033e-01,  1.1717e+00],\n",
       "           [ 1.9854e+00, -5.5388e-01,  8.6239e-03,  ..., -1.1683e+00,\n",
       "             6.9496e-01,  1.1931e+00]],\n",
       " \n",
       "          [[ 1.1648e-02, -1.0477e-02, -8.4498e-03,  ..., -6.3018e-02,\n",
       "             6.9851e-03, -1.0035e+00],\n",
       "           [-7.3477e-04,  2.9102e-01, -6.3726e-01,  ...,  1.3400e+00,\n",
       "            -5.4306e-01, -2.0111e+00],\n",
       "           [-4.4453e-01, -4.6882e-01,  3.4516e-01,  ...,  9.4532e-01,\n",
       "            -2.7266e+00, -3.4720e+00],\n",
       "           ...,\n",
       "           [ 1.1263e+00,  1.3120e-01,  1.1352e-01,  ..., -4.2831e-01,\n",
       "             6.9593e-01,  1.1380e-01],\n",
       "           [ 1.7559e+00,  1.0959e-01,  9.0620e-02,  ..., -3.8750e-01,\n",
       "             6.4945e-01,  1.2560e-01],\n",
       "           [ 8.5223e-01,  1.2842e-01, -5.3411e-02,  ..., -4.8383e-01,\n",
       "             6.2854e-01,  1.5308e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.3915e-02, -1.5501e-02,  1.3795e-02,  ..., -1.9204e-01,\n",
       "            -2.3410e-01, -7.7863e-01],\n",
       "           [ 3.1600e-01, -4.6670e-01, -1.9342e-01,  ...,  6.7171e-01,\n",
       "            -3.9824e-01,  6.1608e-01],\n",
       "           [ 6.6467e-02, -5.8653e-01, -6.1605e-01,  ...,  3.0507e-01,\n",
       "             1.6149e+00,  1.0029e+00],\n",
       "           ...,\n",
       "           [ 1.4232e+00,  2.3380e-01, -4.8313e-03,  ...,  9.6047e-01,\n",
       "             3.5807e-01,  2.1889e+00],\n",
       "           [ 4.9272e+00,  2.9905e-01, -2.4120e-01,  ...,  9.5990e-01,\n",
       "             3.5938e-01,  1.9850e+00],\n",
       "           [ 4.2362e+00,  1.2209e-01, -3.3629e-01,  ...,  9.8860e-01,\n",
       "             3.8979e-01,  2.1191e+00]],\n",
       " \n",
       "          [[-2.0394e-02,  1.5862e-02, -3.0523e-02,  ...,  2.5980e-01,\n",
       "            -8.7595e-01, -5.3170e-01],\n",
       "           [ 7.4647e-03, -5.4181e-01,  8.1411e-02,  ..., -1.8970e+00,\n",
       "             1.1525e-02, -5.3113e-01],\n",
       "           [ 1.7542e-02,  3.7593e-01,  8.4684e-01,  ..., -1.2999e+00,\n",
       "            -1.0124e+00, -1.6096e+00],\n",
       "           ...,\n",
       "           [ 1.1720e+00, -1.5466e-01,  5.0381e-01,  ...,  7.2553e-01,\n",
       "             1.2449e+00, -2.0798e-01],\n",
       "           [-6.9978e-01, -2.5815e-02,  1.0437e-01,  ...,  6.9909e-01,\n",
       "             1.2930e+00, -2.9493e-01],\n",
       "           [-2.0658e+00,  9.6978e-02, -2.3267e-01,  ...,  6.2818e-01,\n",
       "             1.3033e+00, -2.3521e-01]],\n",
       " \n",
       "          [[-1.1900e-02, -1.8850e-02, -2.4731e-03,  ..., -1.4490e-02,\n",
       "             1.7323e-01, -3.3513e-01],\n",
       "           [ 4.2479e-01,  3.0829e-02,  2.6159e-01,  ...,  4.8108e-01,\n",
       "            -5.8085e-01, -1.4185e+00],\n",
       "           [-3.7794e-01, -1.5356e-01,  3.4122e-01,  ...,  1.0491e+00,\n",
       "             6.2023e-01, -2.0217e+00],\n",
       "           ...,\n",
       "           [ 9.7000e-01,  1.1893e+00,  6.1871e-03,  ..., -6.7921e-01,\n",
       "            -4.8857e-01, -1.4147e+00],\n",
       "           [ 1.7206e+00,  5.2178e-01, -8.1994e-01,  ..., -7.5619e-01,\n",
       "            -4.9338e-01, -1.3019e+00],\n",
       "           [ 1.0153e+00, -4.7536e-01, -1.0811e+00,  ..., -6.5422e-01,\n",
       "            -4.5510e-01, -1.4258e+00]]]]),\n",
       " tensor([[[[ 9.2457e-03,  1.6773e-02,  4.9353e-03,  ..., -2.2445e-01,\n",
       "             1.5013e-01, -1.0513e-01],\n",
       "           [ 6.9415e-01, -5.1054e-01, -1.1312e-01,  ..., -1.7303e+00,\n",
       "            -1.6476e-01,  9.9399e-01],\n",
       "           [-8.3382e-02, -1.3120e-01, -7.6331e-01,  ..., -2.3546e+00,\n",
       "             1.2053e+00,  3.2547e-01],\n",
       "           ...,\n",
       "           [-3.3084e+00,  1.6277e+00,  2.0992e-01,  ..., -1.2161e-01,\n",
       "            -1.1513e-01,  1.0476e-01],\n",
       "           [-8.6540e-01,  6.7559e-01, -4.1798e-01,  ..., -1.6365e-01,\n",
       "            -5.4103e-02,  1.8407e-01],\n",
       "           [ 2.2892e+00, -9.1618e-01, -7.8268e-01,  ..., -1.3057e-01,\n",
       "            -1.1511e-01,  7.4018e-02]],\n",
       " \n",
       "          [[-2.0914e-02,  9.9188e-03,  1.7280e-03,  ..., -2.1834e+00,\n",
       "             2.5192e-01,  4.7671e-01],\n",
       "           [ 8.9811e-02,  2.8955e-01,  1.1006e-01,  ...,  2.5406e+00,\n",
       "            -2.9340e+00, -9.2876e-01],\n",
       "           [-8.4921e-02,  1.0690e-01, -7.4648e-02,  ...,  3.1844e+00,\n",
       "            -3.1292e+00, -5.0738e-01],\n",
       "           ...,\n",
       "           [-4.0527e+00,  6.3532e-02, -6.1538e-01,  ..., -5.9297e-01,\n",
       "            -9.1683e-01, -2.3018e+00],\n",
       "           [-2.7145e+00, -1.7769e-02, -5.4225e-02,  ..., -5.9040e-01,\n",
       "            -8.8430e-01, -2.3069e+00],\n",
       "           [ 9.8748e-01, -9.5939e-02,  5.0828e-01,  ..., -5.2129e-01,\n",
       "            -1.0190e+00, -2.3524e+00]],\n",
       " \n",
       "          [[-6.9201e-03, -2.0457e-03,  3.2233e-02,  ...,  7.3060e-02,\n",
       "            -6.1512e-02, -2.0396e-01],\n",
       "           [-1.5669e-01, -1.5923e-01,  4.3504e-01,  ...,  7.3363e-01,\n",
       "            -1.1367e+00,  6.1418e-01],\n",
       "           [-5.4705e-01, -2.7776e-01, -3.4977e-01,  ...,  1.5581e+00,\n",
       "            -7.5831e-01,  4.7430e-01],\n",
       "           ...,\n",
       "           [-3.0999e+00, -7.2513e-01,  4.7716e-02,  ..., -1.1587e+00,\n",
       "            -1.8057e-01, -1.4615e-01],\n",
       "           [ 9.6078e-01, -1.0654e+00,  4.1467e-01,  ..., -1.0664e+00,\n",
       "            -1.1619e-01, -2.1987e-01],\n",
       "           [ 4.1579e+00, -5.7048e-01,  4.7186e-01,  ..., -1.0901e+00,\n",
       "            -1.8297e-01, -1.8268e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.9886e-02, -9.3577e-03, -1.6391e-02,  ..., -1.8185e+00,\n",
       "             2.2022e+00, -6.0541e-01],\n",
       "           [ 1.1233e-01, -7.1609e-01,  3.2145e-01,  ...,  2.8636e+00,\n",
       "            -1.1981e+00,  1.5654e+00],\n",
       "           [ 2.8087e-01, -3.2976e-01,  9.7706e-02,  ...,  2.5424e+00,\n",
       "             1.8808e-02,  2.9840e+00],\n",
       "           ...,\n",
       "           [ 2.1423e+00, -2.6613e-01, -8.9440e-01,  ..., -1.7389e+00,\n",
       "             1.8990e+00,  3.2887e-01],\n",
       "           [ 2.3904e+00, -3.3307e-01, -5.4726e-01,  ..., -1.7078e+00,\n",
       "             1.9003e+00,  2.9667e-01],\n",
       "           [ 5.9479e-01,  1.5106e-02,  7.8411e-02,  ..., -1.6500e+00,\n",
       "             1.8702e+00,  3.2333e-01]],\n",
       " \n",
       "          [[-2.9719e-02,  1.5929e-02, -1.0110e-02,  ..., -1.3306e-01,\n",
       "             1.7535e-01, -7.2597e-01],\n",
       "           [ 3.5843e-01,  1.2327e-01, -3.5879e-01,  ...,  1.4252e+00,\n",
       "             8.1828e-01,  1.9624e+00],\n",
       "           [-2.6868e-01, -4.0412e-01,  1.3005e-01,  ...,  4.8588e-01,\n",
       "            -2.0855e-01,  1.0426e+00],\n",
       "           ...,\n",
       "           [-3.2776e+00,  7.4658e-02,  4.8989e-01,  ..., -2.1326e-01,\n",
       "             4.5273e-02, -6.5142e-01],\n",
       "           [-1.1821e+00,  6.1966e-01,  6.1347e-01,  ..., -2.0685e-01,\n",
       "             8.7771e-02, -6.0937e-01],\n",
       "           [ 1.9391e+00,  7.1840e-01,  2.6169e-01,  ..., -2.0332e-01,\n",
       "             1.8476e-02, -6.6571e-01]],\n",
       " \n",
       "          [[ 6.2230e-03,  4.3810e-03,  9.5825e-03,  ..., -2.4659e-02,\n",
       "            -1.4532e-01,  4.9595e-02],\n",
       "           [-2.7199e-01, -1.1269e+00, -1.2911e+00,  ...,  7.6198e-01,\n",
       "             5.1451e-01, -4.0560e-01],\n",
       "           [ 1.8754e+00,  5.1915e-01, -1.9192e-01,  ...,  4.7315e-01,\n",
       "             1.3925e-01, -2.0380e+00],\n",
       "           ...,\n",
       "           [-1.2562e+00,  4.3831e-01,  4.2467e-02,  ..., -1.0297e+00,\n",
       "            -5.5280e-01,  1.8786e+00],\n",
       "           [-1.8851e+00, -2.2888e-01, -1.5363e-01,  ..., -9.7125e-01,\n",
       "            -6.0167e-01,  1.8064e+00],\n",
       "           [-7.2166e-01, -6.8429e-01, -3.3024e-01,  ..., -1.0432e+00,\n",
       "            -5.3132e-01,  1.8466e+00]]]]),\n",
       " tensor([[[[-1.5076e-02, -6.7433e-03, -1.7615e-02,  ..., -1.2483e-01,\n",
       "            -6.5044e-02, -8.5470e-02],\n",
       "           [ 4.6154e-01, -2.3711e-01, -6.0223e-02,  ..., -1.3637e+00,\n",
       "            -1.3239e+00,  1.9713e+00],\n",
       "           [-3.8258e-01,  3.4239e-01, -2.6798e-01,  ..., -1.5045e+00,\n",
       "            -3.6869e-01,  8.4380e-02],\n",
       "           ...,\n",
       "           [-2.5759e-01,  4.2013e-02, -8.4241e-03,  ...,  9.4724e-02,\n",
       "            -1.2832e-01,  2.4120e-01],\n",
       "           [ 1.0664e+00, -4.0443e-01,  2.3625e-01,  ...,  1.4442e-01,\n",
       "            -1.8645e-01,  2.9958e-01],\n",
       "           [ 1.3912e+00, -5.5037e-01,  4.5874e-01,  ...,  1.5136e-01,\n",
       "            -1.5903e-01,  2.1165e-01]],\n",
       " \n",
       "          [[ 2.5421e-02, -4.4638e-03,  6.4012e-04,  ..., -6.7852e-02,\n",
       "            -5.0976e-02, -1.4779e-01],\n",
       "           [-1.6156e-01,  1.7553e-01,  2.4920e-01,  ..., -1.4372e+00,\n",
       "             1.0419e+00, -6.9846e-01],\n",
       "           [ 2.3559e-01,  3.4500e-01, -2.6603e-01,  ..., -1.6494e+00,\n",
       "             2.4272e+00, -1.2710e+00],\n",
       "           ...,\n",
       "           [-1.5203e+00,  6.4389e-02, -5.8001e-01,  ...,  7.0525e-02,\n",
       "            -1.0935e+00, -3.8887e+00],\n",
       "           [ 9.0972e-02,  1.0379e+00, -4.2752e-01,  ...,  9.3164e-02,\n",
       "            -9.1570e-01, -3.7059e+00],\n",
       "           [ 1.5965e+00,  1.3529e+00, -1.8494e-01,  ...,  2.4387e-02,\n",
       "            -1.1375e+00, -3.8378e+00]],\n",
       " \n",
       "          [[ 3.1712e-02,  5.5990e-02,  2.0664e-03,  ...,  3.9221e-01,\n",
       "            -5.5046e-01, -5.2660e-02],\n",
       "           [-1.9122e-01, -9.8041e-03, -2.3944e-01,  ..., -3.4987e-02,\n",
       "             1.8183e+00,  8.8764e-01],\n",
       "           [-2.6458e-02,  5.4952e-01, -7.3683e-01,  ..., -1.2034e+00,\n",
       "             1.1772e+00,  5.5949e-02],\n",
       "           ...,\n",
       "           [-1.8931e+00, -1.7699e-01,  2.3686e-01,  ..., -5.0915e-01,\n",
       "             5.4774e-01,  1.8563e-01],\n",
       "           [-5.6824e+00,  4.8471e-01,  2.5447e-01,  ..., -3.9589e-01,\n",
       "             4.9795e-01,  1.6589e-01],\n",
       "           [-4.5455e+00,  7.9459e-01,  1.1753e-01,  ..., -5.4239e-01,\n",
       "             5.0239e-01,  2.3768e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.2963e-02, -1.7150e-02, -1.7549e-02,  ...,  4.8282e-02,\n",
       "            -4.1922e-01,  1.8905e-01],\n",
       "           [ 2.5939e-02,  1.1702e-01, -1.3933e-01,  ...,  1.5457e+00,\n",
       "             1.7190e+00, -6.7714e-01],\n",
       "           [-4.1062e-02, -3.8225e-01,  3.8781e-01,  ...,  2.6555e+00,\n",
       "             2.4488e+00, -4.9130e-01],\n",
       "           ...,\n",
       "           [-1.0554e+00, -2.2660e+00, -4.7237e-01,  ...,  1.1122e+00,\n",
       "             3.3056e+00, -1.0963e-01],\n",
       "           [ 9.4692e-01, -7.4187e-01, -2.4591e-01,  ...,  1.0792e+00,\n",
       "             3.1241e+00, -5.5738e-02],\n",
       "           [ 2.0101e+00,  1.2237e+00,  4.2135e-02,  ...,  1.0338e+00,\n",
       "             3.2642e+00, -8.5323e-02]],\n",
       " \n",
       "          [[-3.1092e-03,  1.9775e-02,  2.6061e-02,  ...,  7.2759e-01,\n",
       "            -2.0308e-01, -2.4779e+00],\n",
       "           [ 5.8298e-01,  3.0876e-03,  1.9133e-01,  ..., -1.8114e+00,\n",
       "            -3.4731e-01,  8.8537e+00],\n",
       "           [ 3.0128e-01,  4.7834e-02, -7.3330e-02,  ..., -2.6453e+00,\n",
       "             3.6798e-01,  8.5479e+00],\n",
       "           ...,\n",
       "           [ 3.1765e+00,  4.4952e-02,  3.1064e-02,  ...,  1.2897e+00,\n",
       "            -3.1379e-01,  4.4924e+00],\n",
       "           [-9.8986e-01, -2.8129e-01, -4.3761e-01,  ...,  1.1820e+00,\n",
       "            -3.4507e-01,  4.2250e+00],\n",
       "           [-4.2370e+00, -3.3828e-01, -7.0935e-01,  ...,  1.2929e+00,\n",
       "            -3.4493e-01,  4.4170e+00]],\n",
       " \n",
       "          [[-3.2396e-02,  2.9319e-02, -1.0061e-02,  ..., -2.9537e-01,\n",
       "            -1.6209e-01,  2.5595e-01],\n",
       "           [-5.3479e-02,  4.4427e-01, -5.8566e-02,  ...,  3.5000e+00,\n",
       "             1.9166e+00, -5.4618e-01],\n",
       "           [-2.1053e-01, -1.6416e-02,  5.5102e-03,  ...,  3.0492e+00,\n",
       "             1.2715e+00, -6.6036e-01],\n",
       "           ...,\n",
       "           [ 1.7588e-01,  4.5149e-02,  1.7662e-01,  ...,  3.9906e-02,\n",
       "             1.6890e+00,  3.0922e-01],\n",
       "           [-2.8892e+00,  1.0973e-01,  5.7870e-01,  ...,  1.8338e-01,\n",
       "             1.6010e+00,  3.5613e-01],\n",
       "           [-3.5329e+00,  1.1185e-01,  7.8093e-01,  ...,  6.8015e-02,\n",
       "             1.5924e+00,  3.2279e-01]]]]),\n",
       " tensor([[[[ 3.6724e-02, -1.1237e-02,  1.5775e-02,  ..., -3.9597e-01,\n",
       "             2.6697e-01, -1.6921e-01],\n",
       "           [-8.5194e-01, -9.6361e-02, -3.3993e-02,  ..., -2.0267e-01,\n",
       "            -7.5762e-01,  1.5762e+00],\n",
       "           [-9.1737e-02,  6.0148e-01,  4.1115e-01,  ..., -7.6097e-01,\n",
       "            -1.8238e+00,  6.3323e-01],\n",
       "           ...,\n",
       "           [-4.4315e-01,  2.2906e-01,  1.7183e-01,  ..., -5.7253e-01,\n",
       "            -9.9064e-01, -2.5994e-01],\n",
       "           [ 1.4860e+00,  5.6260e-01,  1.2818e-03,  ..., -4.8912e-01,\n",
       "            -9.9365e-01, -2.4411e-01],\n",
       "           [ 2.0391e+00,  5.3238e-01,  1.0332e-01,  ..., -6.3753e-01,\n",
       "            -1.0824e+00, -2.4416e-01]],\n",
       " \n",
       "          [[-3.6383e-02,  2.4027e-03,  1.0866e-03,  ..., -5.8778e-01,\n",
       "            -7.4254e-01, -2.3934e-01],\n",
       "           [-4.1616e-02, -2.3738e-01, -3.9302e-01,  ..., -6.5867e-01,\n",
       "             1.4944e+00,  7.5408e-01],\n",
       "           [-3.1454e-01,  1.0731e-01,  1.3220e-01,  ..., -9.6820e-01,\n",
       "             3.8214e-01,  8.4439e-01],\n",
       "           ...,\n",
       "           [-1.5423e+00, -1.0510e+00, -4.0267e-02,  ...,  2.5605e+00,\n",
       "             4.5476e+00, -3.1472e+00],\n",
       "           [-3.8903e+00, -6.4269e-01,  3.6191e-01,  ...,  2.4513e+00,\n",
       "             4.2919e+00, -3.1908e+00],\n",
       "           [-2.8388e+00,  1.4710e-01,  7.6316e-01,  ...,  2.5528e+00,\n",
       "             4.5245e+00, -3.0389e+00]],\n",
       " \n",
       "          [[ 2.9042e-02,  4.0256e-03,  1.7169e-02,  ..., -1.2107e-01,\n",
       "             8.0932e-02,  3.5685e-01],\n",
       "           [-2.8338e-01, -4.6311e-01, -8.1943e-01,  ..., -1.2504e+00,\n",
       "             1.7237e+00, -1.2551e-01],\n",
       "           [-2.9777e-01, -1.7026e-01, -1.0775e-01,  ..., -6.4654e-01,\n",
       "             4.4196e-02,  3.1983e-01],\n",
       "           ...,\n",
       "           [ 3.1460e+00, -5.4395e-01,  3.8510e-02,  ..., -2.6656e-01,\n",
       "             1.1244e+00,  6.8931e-02],\n",
       "           [ 4.9042e+00, -1.3378e+00,  9.5110e-02,  ..., -2.9668e-01,\n",
       "             1.1336e+00,  1.6424e-01],\n",
       "           [ 2.5494e+00, -1.0016e+00,  1.4495e-01,  ..., -2.6916e-01,\n",
       "             1.1199e+00,  8.1627e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.5267e-02, -3.6412e-03, -1.7395e-02,  ..., -4.3826e-01,\n",
       "            -7.5074e-02,  3.8283e-01],\n",
       "           [ 5.8079e-02,  6.9630e-01,  4.0510e-01,  ..., -1.0541e-01,\n",
       "            -1.0714e+00, -3.7284e+00],\n",
       "           [-2.7073e-01,  1.6428e-01, -2.3313e-01,  ...,  5.6979e-01,\n",
       "            -8.2249e-01, -3.2804e+00],\n",
       "           ...,\n",
       "           [-2.7105e+00,  6.0798e-01,  8.4145e-01,  ..., -2.1984e+00,\n",
       "            -4.8389e-01, -1.6808e+00],\n",
       "           [ 2.1805e+00,  1.0607e+00,  8.9268e-01,  ..., -2.0544e+00,\n",
       "            -4.5376e-01, -1.5105e+00],\n",
       "           [ 5.1585e+00,  7.9516e-01,  5.1443e-01,  ..., -2.2264e+00,\n",
       "            -4.6232e-01, -1.6764e+00]],\n",
       " \n",
       "          [[-6.2080e-03,  1.3659e-02,  4.8717e-02,  ...,  7.2891e-01,\n",
       "             3.9029e-01, -2.3494e+00],\n",
       "           [ 2.7667e-01,  1.2669e-01, -3.5284e-01,  ..., -6.7629e-01,\n",
       "            -1.6971e+00,  1.8967e+00],\n",
       "           [ 1.4463e-01, -3.2468e-01,  5.4689e-02,  ..., -1.9070e+00,\n",
       "            -1.2375e+00,  3.9265e+00],\n",
       "           ...,\n",
       "           [-8.9438e-01, -3.4152e-01, -1.6882e-01,  ..., -8.5548e-01,\n",
       "             2.3419e+00, -4.6213e+00],\n",
       "           [-2.5269e+00, -7.0260e-01, -1.4414e-01,  ..., -7.7895e-01,\n",
       "             2.3911e+00, -4.3946e+00],\n",
       "           [-1.9031e+00, -6.1176e-01, -3.3485e-02,  ..., -8.6138e-01,\n",
       "             2.2940e+00, -4.5663e+00]],\n",
       " \n",
       "          [[-3.7299e-04,  1.2361e-02,  9.3307e-03,  ...,  2.9193e-01,\n",
       "            -1.7553e-01, -1.1688e-01],\n",
       "           [ 4.2295e-01, -1.0027e+00,  6.9776e-01,  ..., -1.1565e+00,\n",
       "            -4.1186e-01, -3.5745e-01],\n",
       "           [ 5.4456e-01, -3.6362e-01,  2.8894e-01,  ..., -4.7909e-01,\n",
       "            -3.6643e+00, -2.7975e+00],\n",
       "           ...,\n",
       "           [-3.7516e+00,  1.5024e+00,  2.3660e-01,  ...,  5.6558e-01,\n",
       "            -2.8094e-01,  1.8487e+00],\n",
       "           [-3.5989e+00,  1.0634e+00,  3.7393e-01,  ...,  5.0221e-01,\n",
       "            -2.3569e-01,  1.8095e+00],\n",
       "           [-2.9438e-01, -9.4063e-02,  3.2029e-01,  ...,  6.1634e-01,\n",
       "            -2.9452e-01,  1.8420e+00]]]]),\n",
       " tensor([[[[-2.0132e-02,  9.6959e-03, -1.1386e-02,  ...,  5.8096e-01,\n",
       "            -3.5126e-01, -6.7623e-01],\n",
       "           [-1.6150e-01,  1.0554e-01, -2.3847e-01,  ...,  1.4187e-01,\n",
       "             2.9902e-01,  1.2256e+00],\n",
       "           [-4.4632e-02,  2.8295e-01, -4.9882e-01,  ...,  2.1567e+00,\n",
       "             1.1519e+00, -6.5854e-02],\n",
       "           ...,\n",
       "           [ 7.8288e-01, -3.3735e-01,  1.5195e-01,  ...,  7.0910e-02,\n",
       "            -3.7540e-01, -7.3759e-01],\n",
       "           [ 9.4699e-01, -4.7339e-01, -2.6438e-01,  ...,  4.9866e-02,\n",
       "            -4.0902e-01, -7.7147e-01],\n",
       "           [ 1.4180e-01, -3.2002e-01, -4.4483e-01,  ...,  5.2202e-02,\n",
       "            -2.6317e-01, -7.8096e-01]],\n",
       " \n",
       "          [[-2.4656e-03, -1.1937e-02, -2.4166e-02,  ...,  6.3369e-02,\n",
       "            -1.1401e-01, -7.9834e-02],\n",
       "           [ 1.0894e+00,  8.6521e-01,  9.1472e-01,  ...,  8.8168e-01,\n",
       "             1.2801e+00, -3.4365e-01],\n",
       "           [ 5.5971e-01,  5.2313e-01, -5.3336e-01,  ..., -1.5178e-01,\n",
       "             2.3425e+00, -1.5573e+00],\n",
       "           ...,\n",
       "           [-1.4181e+00,  7.9734e-01, -7.3485e-01,  ...,  9.0083e-02,\n",
       "            -7.3758e-01,  4.7629e-01],\n",
       "           [-7.8070e-01,  1.1099e+00, -1.0447e+00,  ...,  1.4912e-01,\n",
       "            -6.7831e-01,  4.9917e-01],\n",
       "           [ 5.4273e-01,  3.3035e-01, -8.7225e-01,  ...,  1.0388e-01,\n",
       "            -7.1692e-01,  4.9664e-01]],\n",
       " \n",
       "          [[ 1.4078e-02,  4.0531e-02, -1.0923e-02,  ..., -5.6042e-01,\n",
       "             1.1569e-01,  2.3972e-01],\n",
       "           [-1.2054e-01, -2.4526e-02,  4.2254e-01,  ...,  4.3499e+00,\n",
       "            -6.4794e-01, -4.7036e-01],\n",
       "           [ 1.1780e-01, -3.2295e-01,  1.2718e-01,  ...,  3.3332e+00,\n",
       "             1.2932e+00,  8.1235e-02],\n",
       "           ...,\n",
       "           [ 1.4157e+00,  1.4221e-01, -5.6348e-01,  ...,  2.4874e-02,\n",
       "            -1.8323e+00,  2.8079e-01],\n",
       "           [-2.5598e+00, -1.8011e-01, -3.7052e-01,  ..., -1.4988e-01,\n",
       "            -1.7892e+00,  4.3050e-01],\n",
       "           [-4.3768e+00, -3.2083e-01, -1.1112e-02,  ...,  1.1756e-01,\n",
       "            -1.9296e+00,  2.0048e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.8526e-03,  3.2650e-03,  1.3155e-03,  ...,  7.7529e-02,\n",
       "            -2.6241e-02, -6.3594e-02],\n",
       "           [ 6.8750e-01, -1.7492e+00, -2.2537e-01,  ..., -1.7102e-01,\n",
       "             1.4249e+00,  6.4174e-01],\n",
       "           [ 1.5748e+00,  2.3629e-01, -1.0968e+00,  ..., -2.6234e-01,\n",
       "             6.9884e-02,  1.2269e+00],\n",
       "           ...,\n",
       "           [-2.2337e+00,  1.1822e+00, -1.9742e-01,  ...,  7.1515e-01,\n",
       "            -1.0735e+00,  7.2392e-01],\n",
       "           [-9.1930e-01,  5.4979e-01, -6.6285e-01,  ...,  7.2356e-01,\n",
       "            -9.6909e-01,  7.4218e-01],\n",
       "           [ 1.2044e+00, -7.0761e-01, -7.9061e-01,  ...,  7.0888e-01,\n",
       "            -9.5190e-01,  7.6690e-01]],\n",
       " \n",
       "          [[ 4.3781e-02,  1.4369e-02, -1.3573e-02,  ...,  1.7637e-01,\n",
       "            -1.1240e-01,  5.7555e-02],\n",
       "           [ 6.4597e-01,  9.8006e-02,  7.5390e-01,  ..., -1.3021e+00,\n",
       "             1.8348e+00,  2.2214e-01],\n",
       "           [-1.1153e+00,  3.2102e-01,  1.6451e-01,  ...,  8.6648e-01,\n",
       "             1.0562e+00, -1.1844e+00],\n",
       "           ...,\n",
       "           [-1.6539e+00, -1.5236e+00, -8.9543e-02,  ...,  1.6350e+00,\n",
       "             2.9141e+00,  2.3888e-01],\n",
       "           [ 1.2958e+00, -1.1551e+00,  2.6377e-02,  ...,  1.5813e+00,\n",
       "             2.9031e+00,  3.3104e-01],\n",
       "           [ 3.2145e+00, -3.7586e-02,  1.7441e-01,  ...,  1.6126e+00,\n",
       "             2.9405e+00,  3.1608e-01]],\n",
       " \n",
       "          [[ 1.1534e-02,  3.4897e-03, -2.9119e-02,  ...,  2.2718e-01,\n",
       "            -1.2694e+00,  2.2459e-01],\n",
       "           [ 2.4089e-04,  4.8048e-01, -1.2058e-01,  ..., -2.0800e+00,\n",
       "             7.6035e-01, -2.4250e+00],\n",
       "           [ 1.2730e-01,  2.4633e-01,  3.4328e-02,  ..., -5.4318e-01,\n",
       "             6.8881e-02, -1.7374e+00],\n",
       "           ...,\n",
       "           [-2.2013e+00, -3.2152e-01, -2.9511e-01,  ...,  4.2131e-01,\n",
       "             1.1921e+00, -2.7549e+00],\n",
       "           [-2.7929e+00,  1.8834e-01,  1.6412e-01,  ...,  4.1750e-01,\n",
       "             1.1259e+00, -2.6044e+00],\n",
       "           [-8.4079e-01,  5.8477e-01,  5.2529e-01,  ...,  3.8119e-01,\n",
       "             1.2962e+00, -2.7104e+00]]]]),\n",
       " tensor([[[[-3.6750e-02,  2.9522e-02, -2.7210e-02,  ...,  3.3773e-01,\n",
       "             6.6609e-02,  1.4824e-01],\n",
       "           [-8.9484e-02, -1.2560e+00,  4.7763e-01,  ...,  6.2696e-01,\n",
       "            -1.1451e+00, -1.3683e+00],\n",
       "           [-1.2051e-02, -4.9277e-02,  2.6891e-01,  ...,  3.6975e-01,\n",
       "             2.3758e-01, -1.6621e+00],\n",
       "           ...,\n",
       "           [ 6.4052e-01,  8.3707e-02, -6.0071e-01,  ..., -1.1587e+00,\n",
       "             6.1363e-01,  1.5776e+00],\n",
       "           [-3.0636e+00,  1.1814e-01, -1.1430e+00,  ..., -1.0110e+00,\n",
       "             3.9872e-01,  1.5760e+00],\n",
       "           [-4.0227e+00,  4.9362e-02, -1.0770e+00,  ..., -1.1127e+00,\n",
       "             6.3895e-01,  1.6031e+00]],\n",
       " \n",
       "          [[-2.2363e-02,  8.7363e-03, -1.2856e-02,  ...,  3.4406e-01,\n",
       "             4.0961e-01,  6.1588e-01],\n",
       "           [-1.2315e-01, -1.1045e-02, -7.4610e-02,  ..., -5.7482e-02,\n",
       "            -3.9292e-01, -3.5493e+00],\n",
       "           [-2.6996e-01, -2.9450e-01, -4.3080e-01,  ...,  5.6983e-02,\n",
       "            -9.9707e-01, -3.3922e+00],\n",
       "           ...,\n",
       "           [ 6.4126e-01, -6.6977e-01,  2.1383e-01,  ..., -3.2611e+00,\n",
       "             6.3087e-01, -1.8187e+00],\n",
       "           [-6.3709e-01, -4.3407e-01,  6.7753e-01,  ..., -3.2525e+00,\n",
       "             6.9997e-01, -1.7500e+00],\n",
       "           [-1.3092e+00,  1.4975e-01,  8.0457e-01,  ..., -3.2277e+00,\n",
       "             6.0486e-01, -1.8072e+00]],\n",
       " \n",
       "          [[-4.2689e-03,  2.7469e-02,  1.7777e-02,  ..., -2.2325e-02,\n",
       "             1.7380e-01,  2.9250e-01],\n",
       "           [ 1.8685e-01,  5.6124e-02,  9.7050e-01,  ...,  9.3331e-03,\n",
       "             4.9610e-01,  7.8342e-01],\n",
       "           [-8.9727e-01, -1.0938e-01,  6.3702e-01,  ...,  2.2304e+00,\n",
       "             6.7024e-01,  1.3058e+00],\n",
       "           ...,\n",
       "           [ 2.3837e+00, -1.1940e+00,  1.3165e+00,  ...,  4.3083e-01,\n",
       "            -1.1564e+00, -6.3575e-02],\n",
       "           [ 6.4879e+00, -1.3822e+00,  1.2582e+00,  ...,  3.4760e-01,\n",
       "            -1.2131e+00,  1.0368e-01],\n",
       "           [ 4.9206e+00, -1.2609e+00,  1.0034e+00,  ...,  4.7051e-01,\n",
       "            -1.0571e+00, -6.4950e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.3012e-02,  2.4690e-03, -3.4982e-04,  ..., -2.8240e-02,\n",
       "            -2.0571e-01, -4.7630e-01],\n",
       "           [ 3.9525e-01, -4.0151e-01, -9.9023e-01,  ...,  2.3909e-01,\n",
       "             3.8744e-01,  8.9293e-01],\n",
       "           [ 6.1928e-01, -1.3501e+00, -1.3429e+00,  ...,  1.0405e+00,\n",
       "             8.4202e-02,  2.1740e+00],\n",
       "           ...,\n",
       "           [-1.2145e+00,  1.3893e-01, -5.6719e-01,  ...,  1.5241e+00,\n",
       "             3.1668e-01, -1.8549e+00],\n",
       "           [-1.4084e+00,  3.7692e-01, -1.0303e+00,  ...,  1.4870e+00,\n",
       "             2.5187e-01, -1.6576e+00],\n",
       "           [-4.8699e-01,  2.6018e-01, -9.0388e-01,  ...,  1.4948e+00,\n",
       "             2.8556e-01, -1.8250e+00]],\n",
       " \n",
       "          [[ 1.9290e-02, -2.7251e-02, -3.2269e-02,  ...,  5.6106e-02,\n",
       "            -1.6367e+00,  3.3755e-02],\n",
       "           [ 2.1246e-01,  2.6734e-01,  1.9453e-01,  ..., -1.2639e+00,\n",
       "             3.3802e+00,  9.1215e-01],\n",
       "           [ 1.9030e-01,  7.0491e-02, -1.6194e-01,  ...,  4.3588e-01,\n",
       "             5.4397e+00,  4.7448e-01],\n",
       "           ...,\n",
       "           [ 6.4560e-01,  3.0084e-01,  1.1807e+00,  ..., -2.7500e+00,\n",
       "            -1.5054e+00,  1.0741e+00],\n",
       "           [-2.2258e+00, -6.7623e-01,  1.0347e+00,  ..., -2.7704e+00,\n",
       "            -1.6060e+00,  1.0455e+00],\n",
       "           [-3.0037e+00, -1.1604e+00,  2.3894e-01,  ..., -2.7341e+00,\n",
       "            -1.5309e+00,  1.0416e+00]],\n",
       " \n",
       "          [[ 6.5238e-03,  1.9731e-02,  8.4626e-03,  ...,  2.1061e-01,\n",
       "             5.7128e-01,  6.7259e-03],\n",
       "           [ 1.4020e+00,  2.5798e-01, -1.1074e+00,  ..., -9.0743e-01,\n",
       "            -7.0022e-01, -2.6250e+00],\n",
       "           [-2.7248e-02,  4.8936e-01,  5.9031e-01,  ..., -1.6533e+00,\n",
       "             7.2163e-01, -3.5612e+00],\n",
       "           ...,\n",
       "           [-1.8321e+00,  6.9936e-02,  4.3539e-01,  ..., -1.4894e+00,\n",
       "             3.1733e-01, -2.1416e+00],\n",
       "           [-4.8725e-01,  4.7812e-02, -1.8271e-01,  ..., -1.3988e+00,\n",
       "             5.3328e-01, -1.9943e+00],\n",
       "           [ 1.3652e+00,  4.9771e-02, -6.9268e-01,  ..., -1.4606e+00,\n",
       "             3.9694e-01, -2.0945e+00]]]]),\n",
       " tensor([[[[-2.4536e-02, -8.5128e-03,  1.5896e-02,  ..., -2.1799e-01,\n",
       "             4.9007e-01,  7.2436e-01],\n",
       "           [-2.8946e-01, -2.5146e-01,  4.6802e-01,  ...,  1.2153e+00,\n",
       "            -1.9048e+00, -1.5045e+00],\n",
       "           [ 4.6765e-01,  3.9591e-01,  2.8891e-02,  ...,  1.2206e+00,\n",
       "            -1.3286e+00, -1.3064e+00],\n",
       "           ...,\n",
       "           [ 5.5848e+00, -9.5800e-03, -4.9082e-01,  ..., -3.4221e+00,\n",
       "            -1.8330e-01, -2.7424e+00],\n",
       "           [ 1.0813e+00, -3.6265e-01, -4.0931e-01,  ..., -3.1882e+00,\n",
       "            -2.8751e-01, -2.7595e+00],\n",
       "           [-4.3315e+00, -3.8562e-01, -2.0960e-01,  ..., -3.3551e+00,\n",
       "            -2.0484e-01, -2.8014e+00]],\n",
       " \n",
       "          [[-3.9669e-03,  3.8336e-06, -1.9146e-02,  ..., -2.8221e-01,\n",
       "             5.0634e-02,  1.7189e-02],\n",
       "           [-3.1375e-02, -1.2749e+00, -2.6941e-01,  ..., -3.2499e-01,\n",
       "            -3.1828e-01,  7.5949e-01],\n",
       "           [-2.5593e-01, -5.4006e-01, -1.9180e-01,  ..., -9.0388e-01,\n",
       "            -2.0473e+00,  1.4529e+00],\n",
       "           ...,\n",
       "           [-2.6255e+00,  3.5712e-01, -5.0882e-01,  ..., -3.9826e-02,\n",
       "            -1.5507e+00, -5.3009e-01],\n",
       "           [-7.0515e-01,  6.1758e-01, -4.8617e-01,  ..., -1.5125e-01,\n",
       "            -1.5272e+00, -6.5558e-01],\n",
       "           [ 1.7306e+00,  3.2595e-01, -2.7111e-01,  ..., -6.7581e-02,\n",
       "            -1.5447e+00, -5.4453e-01]],\n",
       " \n",
       "          [[-3.6309e-03, -7.6247e-03, -1.3731e-02,  ..., -6.0649e-02,\n",
       "             1.8558e-01, -2.0800e-02],\n",
       "           [-3.3465e-01,  1.5394e+00, -2.7484e-01,  ...,  2.2882e-01,\n",
       "             2.7442e-01,  1.7910e-01],\n",
       "           [ 5.5242e-01,  8.9911e-01,  2.0837e-01,  ..., -6.5767e-01,\n",
       "             8.2383e-01,  7.5053e-01],\n",
       "           ...,\n",
       "           [ 5.1471e+00,  2.0260e+00, -1.2815e-01,  ..., -3.4215e-01,\n",
       "            -5.9153e-01, -1.6840e+00],\n",
       "           [ 2.5946e+00,  1.8126e+00, -8.5490e-01,  ..., -1.6754e-01,\n",
       "            -7.5014e-01, -1.6497e+00],\n",
       "           [-2.2203e+00,  3.7227e-01, -1.2964e+00,  ..., -3.7042e-01,\n",
       "            -6.4431e-01, -1.6583e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.4821e-02, -5.7367e-03,  9.1645e-03,  ..., -1.3660e-01,\n",
       "            -4.8759e-02,  3.4934e-02],\n",
       "           [ 2.5964e-01,  5.6081e-01,  1.0898e+00,  ...,  1.5065e+00,\n",
       "             9.0173e-01, -1.6408e-01],\n",
       "           [ 7.9283e-01, -6.6654e-02,  1.3680e+00,  ..., -8.4605e-01,\n",
       "             9.1666e-01,  4.1627e-01],\n",
       "           ...,\n",
       "           [-9.9440e-02, -1.0352e+00, -1.5879e-01,  ..., -5.6778e-01,\n",
       "             7.7149e-01,  1.8605e-01],\n",
       "           [-9.6437e-01, -6.0335e-01,  6.0333e-02,  ..., -7.6974e-01,\n",
       "             6.9193e-01,  3.6872e-01],\n",
       "           [-1.0293e+00,  3.0258e-01,  3.0438e-01,  ..., -6.2620e-01,\n",
       "             7.8155e-01,  3.0790e-01]],\n",
       " \n",
       "          [[-5.8958e-03, -4.4043e-03, -1.2055e-02,  ...,  8.1563e-02,\n",
       "            -1.8148e-01, -2.5827e-01],\n",
       "           [ 2.0626e-01, -4.5803e-01, -5.5755e-01,  ..., -1.1423e+00,\n",
       "             7.1724e-01,  2.2343e-01],\n",
       "           [-5.7313e-02, -2.5590e-01,  3.8142e-01,  ..., -2.6143e-01,\n",
       "             1.2464e+00,  4.8791e-01],\n",
       "           ...,\n",
       "           [-1.5555e+00,  1.9036e-01, -5.6203e-01,  ..., -2.5018e+00,\n",
       "            -6.2815e-01, -2.2210e-01],\n",
       "           [-9.2775e-01,  5.0711e-01, -1.0760e+00,  ..., -2.3385e+00,\n",
       "            -7.9091e-01, -4.2878e-01],\n",
       "           [ 5.8716e-01,  5.7337e-01, -1.1233e+00,  ..., -2.5213e+00,\n",
       "            -5.8096e-01, -2.6567e-01]],\n",
       " \n",
       "          [[-5.7476e-03,  4.3757e-03, -1.8615e-02,  ...,  2.2560e-01,\n",
       "             1.9241e-01,  8.2647e-02],\n",
       "           [-1.1673e+00,  6.4580e-02, -7.4505e-01,  ..., -1.4837e+00,\n",
       "            -2.3198e+00,  1.0148e+00],\n",
       "           [-3.8191e-02,  5.5381e-01, -2.1886e-01,  ..., -2.1193e-01,\n",
       "            -2.6750e+00, -8.4639e-01],\n",
       "           ...,\n",
       "           [-2.2524e+00, -1.0570e-01,  1.0172e-01,  ...,  7.1596e-01,\n",
       "            -2.4909e+00, -1.9980e+00],\n",
       "           [-4.7700e+00, -3.5473e-01,  6.6355e-01,  ...,  6.8875e-01,\n",
       "            -2.3183e+00, -1.8765e+00],\n",
       "           [-3.0692e+00, -4.2590e-01,  9.8631e-01,  ...,  7.2686e-01,\n",
       "            -2.4602e+00, -2.0022e+00]]]]),\n",
       " tensor([[[[-6.7820e-02,  2.6616e-02,  2.9034e-03,  ..., -1.4483e-02,\n",
       "             2.0984e-01, -1.3582e+00],\n",
       "           [ 4.3988e-01, -1.5821e-01,  1.4284e-01,  ...,  2.3801e+00,\n",
       "            -8.0677e-01,  5.9204e+00],\n",
       "           [ 1.9956e-02, -4.1467e-01,  3.8944e-01,  ...,  2.4646e+00,\n",
       "             2.1691e+00,  6.4453e+00],\n",
       "           ...,\n",
       "           [-3.8795e-01,  1.3511e+00,  2.9571e-01,  ...,  1.1364e+00,\n",
       "            -1.9833e+00,  1.6703e+00],\n",
       "           [ 1.4437e-01,  7.8564e-01, -6.6743e-02,  ...,  1.2698e+00,\n",
       "            -1.9526e+00,  1.5254e+00],\n",
       "           [ 5.9932e-01, -3.1875e-01, -5.1012e-01,  ...,  1.1144e+00,\n",
       "            -1.9531e+00,  1.7067e+00]],\n",
       " \n",
       "          [[ 4.3497e-02, -3.3137e-03,  3.0218e-02,  ..., -1.4608e-01,\n",
       "             1.8199e+00, -1.0199e-01],\n",
       "           [ 6.2842e-02,  2.2535e-01,  1.5802e-01,  ...,  1.5265e+00,\n",
       "            -4.0612e+00,  3.7337e-01],\n",
       "           [ 3.0921e-01,  3.4403e-01,  2.5244e-01,  ...,  7.5216e-02,\n",
       "            -4.7188e+00, -5.4983e-01],\n",
       "           ...,\n",
       "           [ 5.7225e-01, -2.2397e-01, -6.3794e-02,  ..., -2.2758e+00,\n",
       "             3.2378e+00,  3.4006e-01],\n",
       "           [ 3.8776e-01, -4.0812e-01, -3.1902e-01,  ..., -2.2379e+00,\n",
       "             3.3751e+00,  8.1230e-02],\n",
       "           [-1.5181e-01, -2.6751e-01, -3.4501e-01,  ..., -2.2541e+00,\n",
       "             3.2179e+00,  2.9656e-01]],\n",
       " \n",
       "          [[-8.2058e-03, -1.2317e-02,  7.7815e-03,  ...,  8.3198e-02,\n",
       "            -5.2154e-01,  7.3687e-01],\n",
       "           [-1.0245e+00,  5.0751e-01,  1.9344e-01,  ...,  1.4656e+00,\n",
       "             2.2765e+00, -2.4717e+00],\n",
       "           [-2.1080e-02,  3.7665e-01, -4.2401e-01,  ...,  1.3006e+00,\n",
       "             1.1286e+00, -1.7520e+00],\n",
       "           ...,\n",
       "           [ 2.2605e+00,  1.7649e+00, -7.0278e-01,  ..., -1.6639e+00,\n",
       "             5.8316e-01,  6.8526e-01],\n",
       "           [ 3.1372e+00,  9.2643e-01, -4.1159e-01,  ..., -1.6478e+00,\n",
       "             5.5407e-01,  7.9107e-01],\n",
       "           [ 1.2850e+00, -5.9831e-01,  1.3366e-02,  ..., -1.6262e+00,\n",
       "             4.9570e-01,  7.9530e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.1843e-02, -3.7463e-02,  2.5695e-03,  ..., -5.8324e-01,\n",
       "            -5.6754e-02, -3.1774e-01],\n",
       "           [ 9.3535e-02, -3.1812e-01, -2.8939e-01,  ...,  2.2707e+00,\n",
       "            -3.4785e+00, -1.3967e+00],\n",
       "           [ 4.7783e-01, -2.8074e-01, -1.0171e-01,  ...,  3.2339e+00,\n",
       "            -4.8389e+00, -1.1423e+00],\n",
       "           ...,\n",
       "           [-5.5221e-01,  4.8375e-02,  1.4860e-01,  ...,  3.3084e+00,\n",
       "            -3.3173e+00, -4.1251e+00],\n",
       "           [-1.0809e+00,  2.0237e-01,  2.4048e-01,  ...,  3.3551e+00,\n",
       "            -3.3801e+00, -4.2508e+00],\n",
       "           [-5.8998e-01,  2.2020e-01,  1.9924e-01,  ...,  3.2475e+00,\n",
       "            -3.2804e+00, -4.0756e+00]],\n",
       " \n",
       "          [[ 3.2756e-02,  6.9122e-03, -1.4937e-02,  ...,  3.9522e-01,\n",
       "            -1.1302e+00,  8.8268e-01],\n",
       "           [ 7.8437e-02, -9.9971e-03, -1.1646e-01,  ..., -4.3146e+00,\n",
       "             1.2444e+00, -4.3886e+00],\n",
       "           [ 9.5594e-02, -2.9055e-01, -1.9096e-01,  ..., -5.1588e+00,\n",
       "             7.9984e-01, -4.9268e+00],\n",
       "           ...,\n",
       "           [-2.8277e+00, -1.0935e+00, -5.1295e-01,  ..., -1.8874e+00,\n",
       "             1.6578e+00,  3.0302e+00],\n",
       "           [-1.1919e+00, -9.5882e-01, -3.7035e-01,  ..., -1.8867e+00,\n",
       "             1.7121e+00,  2.8872e+00],\n",
       "           [ 1.5267e+00, -3.0096e-01,  4.7922e-02,  ..., -1.8737e+00,\n",
       "             1.6657e+00,  2.9599e+00]],\n",
       " \n",
       "          [[-1.0296e-02, -6.7003e-03, -6.4359e-03,  ..., -1.8880e-01,\n",
       "            -4.6910e-01,  1.1893e+00],\n",
       "           [ 3.8648e-02, -3.5686e-01, -6.5582e-01,  ...,  1.4544e+00,\n",
       "            -1.6771e+00, -3.8326e+00],\n",
       "           [-1.9592e-01,  1.2830e-01, -9.1432e-01,  ...,  1.9798e+00,\n",
       "            -9.6185e-01, -5.0582e+00],\n",
       "           ...,\n",
       "           [ 3.7850e+00, -3.8496e-01,  4.4883e-01,  ..., -4.3395e+00,\n",
       "            -3.1056e+00, -1.5532e+00],\n",
       "           [ 5.1383e+00, -7.3912e-01,  4.8830e-01,  ..., -4.3538e+00,\n",
       "            -3.0202e+00, -1.5899e+00],\n",
       "           [ 1.9019e+00, -5.7126e-01,  3.2045e-01,  ..., -4.3012e+00,\n",
       "            -3.0569e+00, -1.5628e+00]]]]),\n",
       " tensor([[[[-7.8783e-03, -1.1347e-02, -9.3454e-03,  ...,  2.0468e-01,\n",
       "            -1.0969e-01, -2.0600e-01],\n",
       "           [ 7.7569e-02, -1.6574e-01, -1.9245e-01,  ..., -2.0121e+00,\n",
       "            -1.0223e+00,  7.9550e-01],\n",
       "           [ 8.0953e-02, -1.3011e-02, -4.0947e-01,  ..., -2.5561e+00,\n",
       "             8.4890e-02,  1.2778e+00],\n",
       "           ...,\n",
       "           [-4.7075e+00, -6.1588e-01,  6.0551e-01,  ...,  2.4633e+00,\n",
       "            -1.6963e-02, -2.8743e-01],\n",
       "           [-9.6667e-01, -4.9640e-01,  5.6665e-01,  ...,  2.4314e+00,\n",
       "             2.4700e-02, -3.9036e-01],\n",
       "           [ 3.6902e+00, -1.5992e-02,  3.4215e-01,  ...,  2.4843e+00,\n",
       "             6.4600e-02, -3.6413e-01]],\n",
       " \n",
       "          [[ 2.7341e-02, -6.2517e-03,  5.0272e-03,  ...,  3.0722e-01,\n",
       "             1.0646e+00, -3.4757e-01],\n",
       "           [ 3.2484e-01, -5.0697e-02,  1.2145e-01,  ..., -2.7064e+00,\n",
       "            -3.1908e+00,  1.9261e-01],\n",
       "           [ 9.6647e-03, -2.7580e-02,  9.7851e-02,  ..., -1.8321e+00,\n",
       "            -4.0167e+00,  5.0798e-01],\n",
       "           ...,\n",
       "           [-1.3918e-01, -2.0703e-01,  1.0699e-02,  ..., -1.9473e+00,\n",
       "            -1.1842e+00,  1.2929e+00],\n",
       "           [-2.7066e-01, -1.4792e-01,  8.9142e-02,  ..., -2.0695e+00,\n",
       "            -8.6480e-01,  1.2994e+00],\n",
       "           [-1.5202e-01,  1.2938e-02,  5.0078e-02,  ..., -2.0011e+00,\n",
       "            -1.1163e+00,  1.2802e+00]],\n",
       " \n",
       "          [[-1.4308e-02, -1.2643e-02, -4.2790e-03,  ...,  4.3089e-01,\n",
       "             3.4001e-01, -3.6874e-01],\n",
       "           [ 4.5724e-01, -8.0516e-01,  6.4028e-01,  ..., -2.7957e+00,\n",
       "            -6.8815e-01, -8.5740e-01],\n",
       "           [ 2.7264e-01,  7.0927e-01,  1.7930e-01,  ..., -2.7745e+00,\n",
       "             8.2353e-01, -7.6971e-01],\n",
       "           ...,\n",
       "           [-8.1400e-01, -5.8350e-02, -6.2846e-01,  ...,  2.7957e+00,\n",
       "            -1.0484e+00,  3.6857e-01],\n",
       "           [-3.4742e-01,  6.0969e-01, -3.0675e-01,  ...,  2.7411e+00,\n",
       "            -9.8264e-01,  3.2214e-01],\n",
       "           [ 4.1014e-01,  8.8245e-01,  6.0474e-02,  ...,  2.7902e+00,\n",
       "            -1.0804e+00,  4.0996e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.4420e-02,  1.4650e-02,  6.2908e-03,  ...,  1.7013e+00,\n",
       "            -2.7372e-01, -3.5429e-01],\n",
       "           [ 5.8577e-02,  1.8056e-01, -1.7824e-01,  ..., -5.4427e+00,\n",
       "            -1.1622e-01,  1.6590e+00],\n",
       "           [-8.1555e-01,  2.0531e-01,  2.7700e-01,  ..., -6.9225e+00,\n",
       "            -2.3945e+00,  2.5544e+00],\n",
       "           ...,\n",
       "           [ 3.4821e-01,  3.5240e-01,  3.2423e-01,  ..., -2.9135e+00,\n",
       "            -1.2345e-01,  9.3081e-01],\n",
       "           [ 5.0291e-01,  3.4106e-02,  2.8246e-01,  ..., -2.9773e+00,\n",
       "            -1.5842e-01,  9.6272e-01],\n",
       "           [ 2.0808e-01, -2.9505e-01,  1.2489e-01,  ..., -2.9697e+00,\n",
       "            -1.0239e-01,  9.8448e-01]],\n",
       " \n",
       "          [[ 2.2111e-02, -1.7597e-02, -1.1252e-03,  ..., -9.4120e-01,\n",
       "            -3.0739e-01, -1.1467e-01],\n",
       "           [-3.4040e-01,  3.1294e-01, -1.3837e-01,  ...,  6.3833e+00,\n",
       "             4.9678e-01,  6.8304e-01],\n",
       "           [-3.9494e-01,  1.9767e-01,  1.8600e-01,  ...,  7.2134e+00,\n",
       "             6.1814e-01, -1.0196e+00],\n",
       "           ...,\n",
       "           [ 2.8125e-01, -1.2692e-01,  3.1321e-01,  ...,  3.3360e+00,\n",
       "            -5.8375e-01,  2.9399e+00],\n",
       "           [ 1.3160e+00, -3.7734e-02,  4.7122e-01,  ...,  3.8357e+00,\n",
       "            -5.9960e-01,  2.9452e+00],\n",
       "           [ 1.2612e+00,  1.3107e-01,  3.7122e-01,  ...,  3.1656e+00,\n",
       "            -6.9441e-01,  3.0024e+00]],\n",
       " \n",
       "          [[ 3.0833e-02,  5.9534e-03,  1.5584e-02,  ...,  1.0280e-02,\n",
       "             1.1223e+00, -1.7223e-02],\n",
       "           [ 3.8857e-02, -4.0456e-01,  9.5778e-01,  ...,  1.0757e+00,\n",
       "            -2.3386e+00,  1.2648e+00],\n",
       "           [ 1.9547e-01, -6.2207e-01,  7.2484e-01,  ...,  1.5755e+00,\n",
       "            -4.1632e+00,  1.4707e+00],\n",
       "           ...,\n",
       "           [-1.1986e-01,  7.0827e-01, -1.5010e-01,  ..., -3.6448e+00,\n",
       "             6.9029e-01,  4.7456e+00],\n",
       "           [ 1.3566e+00, -1.0126e+00, -4.5578e-01,  ..., -3.4112e+00,\n",
       "             5.9016e-01,  5.0975e+00],\n",
       "           [ 1.5627e+00, -2.0612e+00, -5.9705e-01,  ..., -3.6759e+00,\n",
       "             6.5266e-01,  4.7462e+00]]]]),\n",
       " tensor([[[[-5.4806e-03, -2.8862e-03,  1.1900e-02,  ..., -2.6344e-01,\n",
       "             4.7764e-02, -4.6585e-01],\n",
       "           [-5.1010e-01,  1.1675e+00,  2.6839e-01,  ...,  5.1765e-01,\n",
       "            -1.7792e-01,  1.8549e+00],\n",
       "           [-1.2758e-01, -3.3502e-01,  3.9655e-01,  ..., -7.6753e-01,\n",
       "             2.0097e-01,  9.1636e-01],\n",
       "           ...,\n",
       "           [-9.0772e-02,  4.2152e-01,  1.8008e+00,  ..., -5.0319e-01,\n",
       "            -3.9680e-01,  3.8718e+00],\n",
       "           [ 4.8479e-02,  1.1367e-01,  1.2119e+00,  ..., -5.9541e-01,\n",
       "            -4.4716e-01,  3.8527e+00],\n",
       "           [ 1.2564e-01, -3.2236e-01, -1.6430e-02,  ..., -5.4668e-01,\n",
       "            -4.3325e-01,  3.8875e+00]],\n",
       " \n",
       "          [[-2.2594e-02, -5.2468e-03,  1.4412e-02,  ...,  4.1589e-02,\n",
       "             2.8992e-01, -4.1328e-02],\n",
       "           [ 3.8300e-01, -4.0192e-01, -2.5072e-01,  ..., -1.2156e+00,\n",
       "             1.0777e-01, -2.9446e+00],\n",
       "           [ 5.2504e-01, -5.7947e-01, -2.3449e-01,  ..., -9.1796e-01,\n",
       "             1.4496e+00, -1.1230e+00],\n",
       "           ...,\n",
       "           [-3.7016e-02, -4.6742e-01, -4.4206e-01,  ..., -3.4193e+00,\n",
       "            -7.7922e-01,  1.4883e+00],\n",
       "           [ 6.6475e-01,  5.0425e-01, -1.4226e+00,  ..., -3.3002e+00,\n",
       "            -7.0902e-01,  1.3835e+00],\n",
       "           [ 7.5772e-01,  1.0967e+00, -1.5066e+00,  ..., -3.4325e+00,\n",
       "            -8.3312e-01,  1.4890e+00]],\n",
       " \n",
       "          [[-1.8411e-03,  1.1019e-02, -9.0737e-04,  ...,  3.0035e-02,\n",
       "             1.4324e+00, -5.5865e-02],\n",
       "           [ 5.9034e-01, -5.3395e-01,  8.8925e-01,  ...,  4.1597e-01,\n",
       "            -3.6843e+00, -3.4919e-01],\n",
       "           [ 7.2266e-01, -9.8720e-01,  4.7481e-01,  ...,  3.4094e-01,\n",
       "            -4.0091e+00, -2.3147e-01],\n",
       "           ...,\n",
       "           [-2.4929e+00, -6.6826e-01,  1.4845e+00,  ..., -7.9409e-01,\n",
       "            -2.0497e+00, -5.3523e-01],\n",
       "           [-1.5438e+00, -8.6634e-01,  1.1497e+00,  ..., -8.5772e-01,\n",
       "            -2.1164e+00, -4.3033e-01],\n",
       "           [ 6.6882e-01, -5.6037e-01, -4.9623e-02,  ..., -8.5964e-01,\n",
       "            -2.0641e+00, -5.7624e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.0345e-03,  3.1777e-02,  1.3987e-02,  ...,  3.9070e-01,\n",
       "            -1.3817e-01, -3.5923e-01],\n",
       "           [-3.7118e-01,  6.7011e-02, -7.1360e-01,  ..., -4.4582e-01,\n",
       "             1.0748e+00,  2.2009e-01],\n",
       "           [-4.1559e-01, -4.6275e-01,  3.3475e-01,  ..., -4.7981e-01,\n",
       "             6.7481e-01, -7.5355e-01],\n",
       "           ...,\n",
       "           [-5.6391e-01, -8.5229e-01,  2.1230e-01,  ...,  1.5670e+00,\n",
       "             6.6161e-01,  1.5223e+00],\n",
       "           [ 3.7671e-01, -2.8696e-01,  1.5071e-01,  ...,  1.6309e+00,\n",
       "             9.7172e-01,  1.4824e+00],\n",
       "           [ 8.9451e-01,  4.8936e-01,  1.0506e-01,  ...,  1.6424e+00,\n",
       "             7.8821e-01,  1.5757e+00]],\n",
       " \n",
       "          [[-7.8705e-03,  3.2336e-02,  1.8891e-02,  ...,  2.0578e+00,\n",
       "             2.3117e-01,  5.1667e-01],\n",
       "           [ 6.8333e-01,  1.2740e-01, -2.5664e-01,  ..., -3.8682e+00,\n",
       "            -1.0899e-01, -3.8292e+00],\n",
       "           [ 5.3208e-02, -3.4840e-01,  7.2754e-01,  ..., -4.1330e+00,\n",
       "            -1.3220e+00, -2.5460e+00],\n",
       "           ...,\n",
       "           [-1.6546e+00,  2.0862e-02,  3.9044e-01,  ...,  3.9612e+00,\n",
       "            -5.3331e+00,  1.5511e+00],\n",
       "           [-3.5118e-01,  7.8819e-01,  2.9795e-01,  ...,  3.9313e+00,\n",
       "            -5.4074e+00,  1.5718e+00],\n",
       "           [ 1.2738e+00,  8.7284e-01,  1.1480e-01,  ...,  4.0829e+00,\n",
       "            -5.3891e+00,  1.6048e+00]],\n",
       " \n",
       "          [[ 3.3552e-02,  4.7515e-02, -2.6335e-02,  ..., -1.1330e+00,\n",
       "            -1.9343e-01,  2.8831e-01],\n",
       "           [ 3.3380e-01,  2.5421e-02, -3.8170e-02,  ...,  1.8530e+00,\n",
       "            -2.4242e-02,  1.1148e+00],\n",
       "           [ 1.4851e-01, -1.2637e-01,  4.0589e-01,  ...,  2.4979e+00,\n",
       "             1.6758e-01, -4.4110e-01],\n",
       "           ...,\n",
       "           [ 4.1396e-01, -6.9558e-01, -4.9233e-01,  ..., -7.3814e+00,\n",
       "            -2.8507e+00, -9.0575e-02],\n",
       "           [ 4.4847e-01, -5.9947e-01, -7.2243e-01,  ..., -7.4256e+00,\n",
       "            -3.0303e+00,  5.0284e-02],\n",
       "           [ 9.0501e-02, -2.1890e-01, -5.1353e-01,  ..., -7.2892e+00,\n",
       "            -2.8339e+00,  1.1925e-02]]]]),\n",
       " tensor([[[[-1.9058e-03,  7.0061e-03,  1.4043e-02,  ..., -1.7608e-01,\n",
       "            -6.9880e-03, -7.7678e-01],\n",
       "           [-2.8005e-01,  8.0395e-03, -1.3702e-01,  ..., -5.7673e-02,\n",
       "             2.4886e-01,  1.4580e+00],\n",
       "           [-3.0793e-01,  2.5866e-01, -1.5269e-01,  ..., -1.6405e+00,\n",
       "             1.8535e+00,  3.4411e+00],\n",
       "           ...,\n",
       "           [ 9.8024e-02, -8.9947e-02, -2.0932e-01,  ...,  1.3756e+00,\n",
       "            -9.3755e-01, -9.5424e-01],\n",
       "           [-3.0796e-01, -1.7535e-01, -3.2079e-02,  ...,  1.0270e+00,\n",
       "            -1.1077e+00, -4.8094e-01],\n",
       "           [-4.2184e-01, -1.0162e-01,  1.8605e-01,  ...,  1.5537e+00,\n",
       "            -8.8550e-01, -9.1338e-01]],\n",
       " \n",
       "          [[-2.1825e-02, -1.7948e-02,  1.9615e-02,  ...,  3.2536e-01,\n",
       "             4.2397e-01,  3.1498e-01],\n",
       "           [-5.4070e-01,  1.7173e-02, -5.4590e-02,  ...,  1.8402e+00,\n",
       "             1.0369e+00, -4.9276e-01],\n",
       "           [ 1.0128e+00, -3.0219e-01,  6.1168e-01,  ..., -9.2334e-01,\n",
       "            -1.7434e-01, -1.7056e-01],\n",
       "           ...,\n",
       "           [ 6.6505e+00, -6.3433e-01,  6.2306e-01,  ...,  1.5423e+00,\n",
       "            -2.2325e+00,  2.9070e+00],\n",
       "           [ 6.3561e+00, -1.2300e+00,  9.6872e-01,  ...,  1.4486e+00,\n",
       "            -2.1055e+00,  2.7590e+00],\n",
       "           [ 2.3224e-01, -7.0959e-01,  8.7782e-01,  ...,  1.4953e+00,\n",
       "            -2.2095e+00,  2.9181e+00]],\n",
       " \n",
       "          [[-9.2057e-03, -8.2000e-05, -7.5335e-03,  ..., -3.1109e-01,\n",
       "             4.5022e-02,  1.8849e-01],\n",
       "           [-9.2263e-01, -1.1212e-01, -3.3895e-01,  ...,  2.0684e+00,\n",
       "             2.9427e-01, -1.5989e+00],\n",
       "           [-9.3412e-01,  5.9193e-03, -1.2112e+00,  ...,  1.2805e+00,\n",
       "             1.0446e-01, -9.5796e-01],\n",
       "           ...,\n",
       "           [ 1.6180e+00,  2.3597e-01, -6.6927e-01,  ...,  4.9168e-01,\n",
       "            -1.6833e+00,  1.0754e+00],\n",
       "           [ 1.1084e+00,  3.2869e-01, -8.1203e-01,  ...,  6.5238e-01,\n",
       "            -1.7188e+00,  1.0165e+00],\n",
       "           [-3.2564e-01,  1.7261e-01, -3.6554e-01,  ...,  3.8123e-01,\n",
       "            -1.7404e+00,  1.0529e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.8752e-02,  2.5437e-03,  1.0155e-02,  ...,  7.5652e-02,\n",
       "             2.6500e-01, -3.1139e-02],\n",
       "           [ 1.5862e-01, -4.7131e-01,  3.2563e-01,  ...,  2.6483e-02,\n",
       "            -8.7781e-01, -3.2097e-01],\n",
       "           [ 9.0631e-01, -9.1196e-01,  3.3496e-01,  ...,  2.5937e-01,\n",
       "            -5.2316e-01, -1.1228e+00],\n",
       "           ...,\n",
       "           [-4.4667e-02, -9.5242e-02, -7.0558e-01,  ..., -5.3693e-01,\n",
       "             6.0902e+00, -3.6876e-01],\n",
       "           [ 1.4790e+00,  3.0014e-01, -1.2211e+00,  ..., -3.6281e-01,\n",
       "             6.2540e+00, -4.5604e-01],\n",
       "           [ 1.5927e+00,  3.1358e-01, -1.1155e+00,  ..., -4.1687e-01,\n",
       "             6.1904e+00, -3.4899e-01]],\n",
       " \n",
       "          [[-2.0972e-02, -7.2020e-03,  2.7656e-02,  ..., -1.8033e-01,\n",
       "             5.0050e-01,  1.8552e-02],\n",
       "           [ 2.8789e-02, -4.8643e-01,  4.3771e-02,  ...,  1.4846e+00,\n",
       "            -1.0046e+00, -1.7482e+00],\n",
       "           [-3.9696e-01, -7.2516e-02, -4.4296e-01,  ...,  5.5237e-01,\n",
       "            -1.3937e+00, -5.3293e-01],\n",
       "           ...,\n",
       "           [-6.2969e-01,  7.4236e-02, -6.3961e-01,  ...,  1.4007e-01,\n",
       "            -3.6626e+00,  4.5651e-01],\n",
       "           [-2.8202e-01,  2.8044e-01, -4.7007e-01,  ...,  1.3906e-01,\n",
       "            -3.6167e+00,  6.3853e-01],\n",
       "           [ 4.1936e-01,  3.0304e-01,  5.8459e-02,  ...,  3.9550e-02,\n",
       "            -3.7366e+00,  4.9956e-01]],\n",
       " \n",
       "          [[ 8.7371e-03, -6.8403e-03,  3.3726e-02,  ...,  3.2597e-01,\n",
       "             1.0222e+00,  4.1459e-01],\n",
       "           [-4.5785e-02,  1.0219e-01,  4.2940e-01,  ..., -7.7051e-01,\n",
       "            -1.6297e+00,  1.6739e+00],\n",
       "           [-1.6493e-01,  1.5042e-01,  3.9759e-01,  ..., -4.3333e-01,\n",
       "            -1.7092e+00,  1.3391e+00],\n",
       "           ...,\n",
       "           [ 1.4405e-01,  8.7437e-03, -2.2342e-01,  ...,  2.7459e-01,\n",
       "             3.2393e+00, -8.0986e+00],\n",
       "           [-3.0698e-01, -1.9627e-01, -1.0995e-01,  ...,  6.3545e-01,\n",
       "             3.2769e+00, -8.1962e+00],\n",
       "           [-4.5395e-01, -2.7627e-01, -2.3254e-02,  ...,  4.1484e-01,\n",
       "             3.2731e+00, -8.1785e+00]]]]),\n",
       " tensor([[[[-3.7546e-03,  9.3752e-03,  1.0426e-02,  ...,  2.2017e-02,\n",
       "            -3.8001e-03, -1.1436e-01],\n",
       "           [-3.5154e-01, -6.5660e-01, -3.5239e-02,  ...,  7.6217e-01,\n",
       "             9.4632e-01,  2.3627e+00],\n",
       "           [ 7.1126e-01,  4.8811e-01,  4.9372e-03,  ...,  7.9048e-01,\n",
       "             1.5060e+00,  2.2849e-01],\n",
       "           ...,\n",
       "           [ 4.1695e-01, -1.6932e-01,  3.0300e-01,  ..., -8.6673e-01,\n",
       "            -1.4038e-01,  1.5070e+00],\n",
       "           [ 6.3494e-01,  3.5343e-01, -2.3219e-01,  ..., -7.7923e-01,\n",
       "            -3.1968e-02,  1.5186e+00],\n",
       "           [ 3.2624e-01,  6.1595e-01, -6.7773e-01,  ..., -8.9358e-01,\n",
       "            -1.0262e-01,  1.4913e+00]],\n",
       " \n",
       "          [[ 1.8192e-03, -2.5358e-03,  2.0529e-02,  ..., -2.3054e-01,\n",
       "            -1.9550e-01, -6.3666e-01],\n",
       "           [-1.9176e+00, -1.1811e+00,  1.4783e-01,  ...,  2.4745e+00,\n",
       "             1.0485e+00,  1.8881e+00],\n",
       "           [-7.0518e-01, -1.6736e+00, -2.3567e-01,  ...,  2.6774e+00,\n",
       "             5.6302e-01,  2.2326e+00],\n",
       "           ...,\n",
       "           [ 4.6802e+00, -9.3471e-02,  1.2928e+00,  ..., -1.4394e+00,\n",
       "            -1.6777e+00,  2.9707e+00],\n",
       "           [ 5.1893e+00, -5.2728e-01, -3.0898e-01,  ..., -1.6108e+00,\n",
       "            -1.6506e+00,  2.8516e+00],\n",
       "           [ 1.4326e+00, -4.5381e-02, -2.0153e+00,  ..., -1.4482e+00,\n",
       "            -1.5634e+00,  2.9689e+00]],\n",
       " \n",
       "          [[-1.2456e-02, -8.7512e-03,  3.7461e-02,  ..., -3.6235e-01,\n",
       "             3.9938e-02,  1.5685e-01],\n",
       "           [ 1.9510e-01, -7.5067e-01, -1.6819e-01,  ...,  6.9028e-01,\n",
       "             6.1446e-01,  4.7368e-01],\n",
       "           [-1.7112e-01,  1.9054e-01,  4.4778e-02,  ...,  4.8201e-01,\n",
       "            -4.8810e-01, -2.5457e-01],\n",
       "           ...,\n",
       "           [ 4.9420e-01, -7.0675e-01, -9.1987e-01,  ..., -8.3608e-01,\n",
       "            -6.6147e-01, -1.5735e+00],\n",
       "           [ 1.1415e+00, -1.8160e-01, -1.4053e+00,  ..., -9.8265e-01,\n",
       "            -5.1588e-01, -1.5028e+00],\n",
       "           [ 7.2170e-01,  3.6004e-01, -1.0050e+00,  ..., -7.7810e-01,\n",
       "            -6.5136e-01, -1.5516e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.8837e-02,  2.0955e-02,  2.4288e-02,  ...,  1.8866e+00,\n",
       "             2.5296e-01, -1.0497e+00],\n",
       "           [-1.8281e-01, -2.8760e-01,  5.8805e-02,  ..., -2.8776e+00,\n",
       "             4.6406e-01,  1.1481e+00],\n",
       "           [-1.2765e-01, -7.8207e-02, -5.9748e-02,  ..., -3.0292e+00,\n",
       "            -1.6424e+00, -6.5333e-01],\n",
       "           ...,\n",
       "           [ 3.8579e-01,  6.7260e-01,  2.1775e-01,  ...,  6.6579e+00,\n",
       "             3.1368e+00, -6.7128e+00],\n",
       "           [ 1.2054e+00,  6.3523e-01, -8.6049e-02,  ...,  6.4708e+00,\n",
       "             3.0531e+00, -7.1438e+00],\n",
       "           [ 9.8811e-01,  3.7106e-01, -2.3008e-01,  ...,  6.9067e+00,\n",
       "             2.9776e+00, -6.8941e+00]],\n",
       " \n",
       "          [[-3.7139e-02, -4.4086e-03, -1.9901e-02,  ..., -6.6920e-01,\n",
       "             8.9188e-02,  3.8952e-01],\n",
       "           [-2.9188e-01, -1.8817e-01, -4.0057e-02,  ...,  4.2751e+00,\n",
       "            -2.0451e-02, -2.1225e+00],\n",
       "           [-2.2775e-01, -4.3327e-01, -7.0352e-02,  ...,  3.5865e+00,\n",
       "             7.9764e-01,  1.7290e-01],\n",
       "           ...,\n",
       "           [ 1.4832e+00, -1.2562e+00,  2.4968e-01,  ...,  4.3691e-02,\n",
       "             6.6947e-01, -5.4723e+00],\n",
       "           [ 1.1836e+00, -7.8896e-01,  6.9897e-01,  ...,  5.6473e-02,\n",
       "             5.9047e-01, -5.4999e+00],\n",
       "           [-2.5410e-01,  2.1735e-01,  6.3548e-01,  ...,  4.2788e-02,\n",
       "             6.5286e-01, -5.5175e+00]],\n",
       " \n",
       "          [[-1.0259e-02,  8.8390e-03,  5.0331e-05,  ...,  1.5483e-01,\n",
       "            -8.7930e-02,  8.4904e-02],\n",
       "           [ 7.5606e-01, -1.3532e+00,  4.2196e-01,  ...,  4.4926e-03,\n",
       "             6.2325e-01,  7.6118e-01],\n",
       "           [ 6.0741e-01,  1.9213e-01, -9.1123e-01,  ..., -5.9689e-01,\n",
       "             1.3782e+00,  1.0892e+00],\n",
       "           ...,\n",
       "           [ 1.1469e+00,  3.4095e-01, -4.8467e-01,  ...,  1.4038e+00,\n",
       "             1.5040e+00,  1.1824e-01],\n",
       "           [ 8.3458e-01,  3.7233e-01, -5.4017e-01,  ...,  1.4918e+00,\n",
       "             1.6016e+00,  9.4012e-02],\n",
       "           [-3.5094e-01,  7.9180e-03, -4.3270e-01,  ...,  1.4716e+00,\n",
       "             1.3853e+00, -7.0664e-03]]]]),\n",
       " tensor([[[[ 3.3390e-02,  1.6915e-03, -5.6843e-03,  ...,  1.3512e-01,\n",
       "            -2.5432e-01, -1.2459e+00],\n",
       "           [-2.7295e-01,  4.2862e-02,  2.6604e-02,  ..., -3.9934e-01,\n",
       "             1.3020e+00,  7.1980e+00],\n",
       "           [-2.9597e-01,  2.1689e-01, -4.4797e-01,  ..., -1.2150e+00,\n",
       "             2.7480e+00,  7.6816e+00],\n",
       "           ...,\n",
       "           [-2.5922e-01, -2.7462e-01, -1.0047e-01,  ..., -3.5958e+00,\n",
       "             2.5660e+00,  7.5902e+00],\n",
       "           [ 3.7452e-01, -6.6669e-01,  1.5748e-01,  ..., -3.8903e+00,\n",
       "             2.5859e+00,  7.7639e+00],\n",
       "           [ 6.7821e-01, -3.3014e-01, -1.3748e-02,  ..., -3.4738e+00,\n",
       "             2.5547e+00,  7.5376e+00]],\n",
       " \n",
       "          [[-1.8427e-02,  3.2279e-02,  7.4789e-03,  ..., -2.2302e-01,\n",
       "             5.5928e-02,  3.2939e-01],\n",
       "           [-6.2599e-02, -2.7228e-01, -1.4864e-01,  ...,  1.5925e+00,\n",
       "             3.2136e-01,  6.5269e-01],\n",
       "           [-5.6867e-01,  1.7906e-01, -8.8489e-01,  ..., -4.5432e-01,\n",
       "             2.0569e-01, -3.0283e-01],\n",
       "           ...,\n",
       "           [-6.0472e-02,  8.2207e-04, -5.6757e-01,  ..., -1.0189e+00,\n",
       "            -2.4512e+00, -1.0438e+00],\n",
       "           [ 1.1735e+00, -1.6508e+00, -2.0406e-01,  ..., -1.2493e+00,\n",
       "            -2.2005e+00, -1.2719e+00],\n",
       "           [ 1.1349e+00, -1.7397e+00,  3.7946e-01,  ..., -1.0980e+00,\n",
       "            -2.4438e+00, -9.4283e-01]],\n",
       " \n",
       "          [[ 1.2735e-02, -5.8030e-03,  1.0595e-02,  ...,  4.5622e-03,\n",
       "            -6.6845e-02, -8.1869e-02],\n",
       "           [-5.3256e-01, -8.0393e-01,  7.4907e-01,  ...,  1.1894e+00,\n",
       "             1.4800e+00,  1.6640e+00],\n",
       "           [-3.5383e-01, -2.2656e-01,  1.1497e+00,  ..., -6.5643e-01,\n",
       "             1.4405e-01,  1.4232e+00],\n",
       "           ...,\n",
       "           [ 9.9399e-01, -1.0098e-01, -4.2061e-01,  ..., -5.9376e-01,\n",
       "            -1.3442e+00, -3.2812e-01],\n",
       "           [ 5.8503e-01,  1.3498e+00, -8.5097e-02,  ..., -5.0160e-01,\n",
       "            -1.2470e+00, -2.9019e-01],\n",
       "           [-2.7171e-01,  1.5410e+00,  2.0254e-01,  ..., -6.7635e-01,\n",
       "            -1.3086e+00, -3.5090e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.6787e-02, -1.0119e-02,  1.2283e-02,  ..., -6.3830e-02,\n",
       "             1.2401e-01,  9.5480e-02],\n",
       "           [ 2.3605e+00, -6.8681e-01,  4.0697e-01,  ...,  7.2972e-01,\n",
       "             6.6107e-01, -6.8191e-01],\n",
       "           [ 5.7791e-01, -9.9859e-01, -4.4724e-01,  ..., -3.3867e-01,\n",
       "             1.0629e+00, -3.2530e-01],\n",
       "           ...,\n",
       "           [-1.2914e+00, -9.8641e-01,  1.7255e-01,  ...,  2.8491e-02,\n",
       "             2.8141e-01,  2.1896e+00],\n",
       "           [-1.8704e-01, -1.7139e+00, -1.2812e+00,  ...,  1.5665e-01,\n",
       "             3.5652e-01,  2.2153e+00],\n",
       "           [ 1.1218e+00, -1.4552e+00, -1.9457e+00,  ..., -1.7188e-02,\n",
       "             3.2442e-01,  2.2111e+00]],\n",
       " \n",
       "          [[-1.8456e-02,  1.2453e-02, -1.1515e-02,  ...,  1.4165e-01,\n",
       "            -1.1210e-01, -1.0983e-01],\n",
       "           [-2.7772e-01, -5.5002e-01, -3.8485e-02,  ..., -1.0896e+00,\n",
       "             1.0838e+00,  1.7219e+00],\n",
       "           [-8.4631e-01,  3.7420e-01, -2.1143e-01,  ..., -4.3370e-01,\n",
       "            -7.5095e-01,  2.8091e+00],\n",
       "           ...,\n",
       "           [ 2.4977e-01,  7.5939e-01, -2.2385e-01,  ..., -8.3307e-01,\n",
       "             2.6269e-02,  1.4078e-01],\n",
       "           [ 4.4251e-01,  1.0631e+00, -4.9608e-01,  ..., -7.6154e-01,\n",
       "            -1.3605e-01,  8.5536e-02],\n",
       "           [ 4.1755e-01,  4.4469e-01, -2.7295e-01,  ..., -8.0272e-01,\n",
       "            -7.0217e-02,  9.1585e-02]],\n",
       " \n",
       "          [[ 1.4467e-02,  4.8605e-02, -2.2641e-02,  ..., -9.6437e-03,\n",
       "            -2.9106e-01,  2.0912e-01],\n",
       "           [ 7.1936e-01,  7.7685e-01,  2.7561e-01,  ..., -5.4155e-01,\n",
       "            -1.4872e-01, -1.5016e+00],\n",
       "           [ 1.3360e-01,  1.1783e-01, -1.4245e+00,  ..., -1.2159e-01,\n",
       "             3.6460e-01, -2.5981e+00],\n",
       "           ...,\n",
       "           [ 3.1182e-01, -1.1794e-01,  1.3314e+00,  ...,  5.4210e-01,\n",
       "             1.8115e+00, -3.0983e+00],\n",
       "           [ 8.1711e-01, -9.7270e-01,  2.5515e-01,  ...,  4.9382e-01,\n",
       "             1.2804e+00, -2.7580e+00],\n",
       "           [ 6.3002e-01, -8.4925e-01, -8.1823e-01,  ...,  4.8853e-01,\n",
       "             1.9546e+00, -3.1702e+00]]]]),\n",
       " tensor([[[[-8.1117e-04, -4.9985e-02, -3.8868e-04,  ..., -8.2974e-02,\n",
       "             1.8183e-01, -1.1992e-01],\n",
       "           [ 2.4137e-01,  6.9998e-01, -5.4794e-01,  ..., -1.1796e+00,\n",
       "            -2.9950e-01, -1.8394e+00],\n",
       "           [ 4.9356e-01,  2.8519e-01,  9.3686e-01,  ..., -5.5457e-01,\n",
       "            -9.1265e-01, -1.0500e+00],\n",
       "           ...,\n",
       "           [ 9.8377e-01,  7.3560e-01, -1.5285e-01,  ...,  3.4342e-02,\n",
       "            -4.9933e-02, -1.0783e+00],\n",
       "           [-1.9925e-01,  7.6214e-01,  4.5106e-03,  ...,  3.5472e-01,\n",
       "            -4.8231e-01, -8.9228e-01],\n",
       "           [-1.2403e+00,  1.2918e-01, -4.6235e-02,  ..., -8.0943e-03,\n",
       "             7.1628e-02, -1.1678e+00]],\n",
       " \n",
       "          [[-2.7767e-04, -1.7105e-02, -2.1964e-02,  ..., -1.6346e-01,\n",
       "             7.8606e-02, -2.5008e-01],\n",
       "           [ 7.3997e-01,  7.8962e-01, -5.5502e-01,  ..., -5.4676e-01,\n",
       "            -3.1524e-01,  4.5854e-02],\n",
       "           [-8.9947e-02, -7.8753e-01, -4.1990e-01,  ..., -1.1511e-01,\n",
       "             8.1813e-01,  6.5542e-01],\n",
       "           ...,\n",
       "           [ 3.4980e-01,  4.8296e-01,  7.8634e-01,  ..., -2.1675e+00,\n",
       "             1.0820e+00,  4.4949e-01],\n",
       "           [-2.4941e-01,  1.2657e+00,  9.2169e-01,  ..., -2.2014e+00,\n",
       "             8.5769e-01,  3.5315e-01],\n",
       "           [-4.0411e-01,  1.0222e+00,  6.3312e-01,  ..., -2.1268e+00,\n",
       "             1.1212e+00,  4.5810e-01]],\n",
       " \n",
       "          [[-2.6166e-03,  1.8204e-02, -6.7580e-03,  ...,  5.8065e-02,\n",
       "             2.3749e-01, -6.9118e-02],\n",
       "           [ 3.8929e-01, -1.8151e-01, -4.0893e-01,  ...,  1.4084e+00,\n",
       "             1.5738e+00,  1.4595e-01],\n",
       "           [ 3.2307e-01,  2.5340e-01, -3.5005e-01,  ...,  3.6581e-01,\n",
       "             6.1618e-01,  9.1365e-01],\n",
       "           ...,\n",
       "           [ 7.9411e-01, -1.5482e-01,  1.2641e-01,  ...,  1.0514e+00,\n",
       "            -8.4875e-01, -8.6020e-01],\n",
       "           [ 1.5710e+00, -1.6617e-01,  1.9275e-01,  ...,  8.9766e-01,\n",
       "            -7.5491e-01, -1.0909e+00],\n",
       "           [ 7.8515e-01,  1.8208e-02,  1.4370e-01,  ...,  9.6475e-01,\n",
       "            -8.4014e-01, -9.9377e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.7453e-02, -3.5067e-02, -7.2517e-03,  ...,  1.4005e-01,\n",
       "             1.1768e-01, -6.2136e-01],\n",
       "           [ 1.1742e-01, -2.6067e-01,  4.8816e-01,  ..., -1.0233e+00,\n",
       "            -2.3257e+00,  7.0155e-01],\n",
       "           [-1.1673e-01, -9.2754e-02, -8.4850e-02,  ..., -9.1525e-01,\n",
       "            -1.5166e+00, -1.0816e-01],\n",
       "           ...,\n",
       "           [-3.3674e+00,  7.2523e-01, -4.3918e-01,  ..., -1.0995e+00,\n",
       "             1.1406e-01,  2.5442e+00],\n",
       "           [-1.3004e+00, -3.0718e-01, -1.4917e-01,  ..., -1.1215e+00,\n",
       "             4.8232e-01,  2.3380e+00],\n",
       "           [ 1.8195e+00, -1.0287e+00,  1.6614e-01,  ..., -1.0989e+00,\n",
       "             5.4939e-02,  2.4805e+00]],\n",
       " \n",
       "          [[-1.4941e-02,  4.9702e-02, -2.3526e-02,  ...,  2.5200e+00,\n",
       "            -2.9906e-01, -4.5789e-01],\n",
       "           [ 1.8293e+00, -1.9062e+00,  1.9457e+00,  ..., -1.0719e+01,\n",
       "             1.0921e-01, -2.3601e+00],\n",
       "           [ 1.3958e+00, -2.1732e+00, -5.2268e-01,  ..., -1.0641e+01,\n",
       "            -5.2532e-01, -3.2396e-01],\n",
       "           ...,\n",
       "           [ 4.8107e+00,  2.6510e-01,  4.2590e-01,  ..., -5.2796e+00,\n",
       "            -2.0471e+00,  2.8963e+00],\n",
       "           [-1.9419e+00,  1.0710e-01,  1.1171e+00,  ..., -5.0802e+00,\n",
       "            -1.8330e+00,  3.7108e+00],\n",
       "           [-7.0939e+00,  8.1008e-01,  1.1426e+00,  ..., -5.3470e+00,\n",
       "            -1.9783e+00,  2.7728e+00]],\n",
       " \n",
       "          [[-8.8953e-03, -8.6255e-03,  3.6452e-03,  ..., -1.8615e+00,\n",
       "            -2.6595e-02,  4.6819e-01],\n",
       "           [ 5.0125e-01,  1.2264e-02,  3.2695e-01,  ...,  9.6562e+00,\n",
       "            -5.7996e-01, -7.7081e-01],\n",
       "           [-8.3070e-01, -2.4649e-01,  2.1338e-01,  ...,  9.9910e+00,\n",
       "            -1.4591e+00, -1.2760e+00],\n",
       "           ...,\n",
       "           [ 6.5726e-01, -1.9027e-01,  4.2483e-01,  ...,  2.9296e+00,\n",
       "            -4.8558e-01,  1.2306e+00],\n",
       "           [ 7.4398e-01, -1.4435e-01, -7.8824e-01,  ...,  2.7120e+00,\n",
       "            -2.6293e-01,  1.3514e+00],\n",
       "           [ 1.3727e-01,  1.0423e-01, -1.5194e+00,  ...,  2.9939e+00,\n",
       "            -4.6956e-01,  1.2254e+00]]]]),\n",
       " tensor([[[[-4.5671e-03, -1.2270e-02, -1.3034e-02,  ...,  1.0338e-01,\n",
       "            -7.7146e-02, -5.9231e-02],\n",
       "           [-4.9950e-01,  9.6098e-01, -1.0441e+00,  ...,  7.7628e-01,\n",
       "             2.5668e-01, -4.4451e-01],\n",
       "           [-1.0590e+00,  6.6606e-01, -7.4479e-01,  ...,  1.7660e-01,\n",
       "             5.7954e-01, -1.9593e+00],\n",
       "           ...,\n",
       "           [ 7.8794e-01, -6.1758e-02, -7.2243e-01,  ..., -1.3276e+00,\n",
       "             1.5887e+00,  1.6305e+00],\n",
       "           [ 1.6695e+00,  5.5880e-01, -5.7132e-01,  ..., -1.4667e+00,\n",
       "             1.6027e+00,  1.6976e+00],\n",
       "           [ 8.1675e-01,  7.1919e-01, -3.9438e-01,  ..., -1.3301e+00,\n",
       "             1.6707e+00,  1.5985e+00]],\n",
       " \n",
       "          [[ 3.1519e-03,  5.1290e-03,  2.6378e-03,  ..., -7.9593e-03,\n",
       "             2.4130e-01, -2.4949e-01],\n",
       "           [ 9.0112e-01,  2.8742e-02, -1.6289e-01,  ...,  1.7425e+00,\n",
       "            -1.5351e+00,  1.8051e+00],\n",
       "           [ 6.3305e-01, -7.7210e-03, -1.0517e+00,  ...,  6.6346e-01,\n",
       "            -9.1471e-02, -4.1469e-01],\n",
       "           ...,\n",
       "           [ 2.1382e-01,  1.7673e-01,  5.8156e-01,  ...,  1.6705e+00,\n",
       "             1.0616e+00, -1.2250e+00],\n",
       "           [-7.9721e-01, -1.2906e-01,  2.5092e-01,  ...,  1.5152e+00,\n",
       "             9.9570e-01, -1.0742e+00],\n",
       "           [-1.2348e+00, -3.1781e-01, -1.3701e-01,  ...,  1.8042e+00,\n",
       "             1.0879e+00, -1.2502e+00]],\n",
       " \n",
       "          [[-7.3597e-03, -2.4577e-03, -1.6064e-02,  ...,  4.4651e-02,\n",
       "             2.7832e-01, -7.7946e-01],\n",
       "           [-1.2138e+00, -3.5698e-01,  2.3816e-01,  ..., -5.9122e-01,\n",
       "             8.7535e-01, -3.6513e-01],\n",
       "           [-2.1883e-02, -1.6394e+00,  9.2285e-01,  ...,  1.0075e+00,\n",
       "            -1.2999e+00,  1.7852e-01],\n",
       "           ...,\n",
       "           [ 4.2608e+00,  1.1763e+00,  1.1560e+00,  ...,  7.0441e-01,\n",
       "             2.2377e+00,  3.5909e+00],\n",
       "           [ 2.6420e+00,  6.5235e-01,  6.5728e-01,  ...,  7.9564e-01,\n",
       "             2.2837e+00,  3.6162e+00],\n",
       "           [-1.0102e+00, -3.1158e-01, -2.8498e-01,  ...,  6.9679e-01,\n",
       "             2.2534e+00,  3.5887e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.6137e-02,  1.2672e-02, -2.5038e-02,  ...,  2.4246e-01,\n",
       "             1.0253e-01, -3.8904e-02],\n",
       "           [ 1.2687e-01, -2.2642e-01, -1.2247e-01,  ..., -1.3297e+00,\n",
       "            -9.6397e-01,  3.8423e-01],\n",
       "           [ 3.6659e-01, -9.0329e-01, -9.4375e-02,  ...,  1.1127e+00,\n",
       "            -1.3646e+00, -8.6681e-01],\n",
       "           ...,\n",
       "           [ 4.5277e-01, -6.8618e-02, -5.5429e-02,  ..., -4.5890e-01,\n",
       "             1.5333e+00,  9.3108e-01],\n",
       "           [ 2.8368e+00,  7.1383e-02,  1.6400e-01,  ..., -2.2732e-01,\n",
       "             1.6776e+00,  1.0239e+00],\n",
       "           [ 2.9048e+00,  5.1060e-02,  3.5794e-01,  ..., -4.9169e-01,\n",
       "             1.5868e+00,  7.9375e-01]],\n",
       " \n",
       "          [[-9.5692e-03, -2.0498e-02,  5.7025e-03,  ...,  2.7823e-02,\n",
       "             2.6155e-01,  4.5776e-02],\n",
       "           [-3.6366e-02,  3.6846e-01, -7.1065e-02,  ...,  7.3452e-01,\n",
       "            -1.2406e+00, -1.2414e-01],\n",
       "           [-5.7175e-02, -9.8028e-02,  7.5048e-01,  ...,  8.2829e-01,\n",
       "            -1.1063e+00, -1.7372e+00],\n",
       "           ...,\n",
       "           [-1.2944e+00, -1.0906e+00,  1.3830e-01,  ...,  6.6462e-01,\n",
       "            -2.7214e+00,  7.6673e-01],\n",
       "           [-7.3778e-01, -6.2933e-01,  6.1946e-01,  ...,  6.4868e-01,\n",
       "            -2.6456e+00,  7.8295e-01],\n",
       "           [ 2.8162e-01, -3.3883e-02,  7.0737e-01,  ...,  7.3389e-01,\n",
       "            -2.6559e+00,  8.9399e-01]],\n",
       " \n",
       "          [[ 1.8496e-02, -1.0455e-02,  3.0169e-02,  ...,  1.4455e-01,\n",
       "             3.3595e-01, -1.8254e+00],\n",
       "           [ 7.4812e-01, -4.3633e-01, -3.6885e-01,  ..., -3.8960e-02,\n",
       "            -9.9063e-01,  6.7103e+00],\n",
       "           [-1.5218e-02, -4.1694e-01,  4.6484e-01,  ..., -2.3816e-01,\n",
       "            -2.8108e+00,  7.4835e+00],\n",
       "           ...,\n",
       "           [-1.2473e+00, -7.2850e-01, -5.4250e-01,  ...,  1.5986e-01,\n",
       "             9.1691e-01,  9.1092e+00],\n",
       "           [-1.0385e+00, -5.1389e-01, -4.6467e-01,  ...,  1.6781e-01,\n",
       "             8.6210e-01,  9.0912e+00],\n",
       "           [ 2.3653e-02,  2.9023e-01,  1.7058e-01,  ...,  9.6346e-02,\n",
       "             8.9662e-01,  9.2530e+00]]]]),\n",
       " tensor([[[[ 1.1988e-02,  6.8194e-03, -1.5535e-03,  ...,  1.0806e-01,\n",
       "             4.3306e-01, -2.0788e-01],\n",
       "           [-6.6467e-01,  4.8264e-01,  1.0053e+00,  ..., -1.2682e-02,\n",
       "            -2.5905e+00,  3.8890e-02],\n",
       "           [ 1.1436e+00, -2.8139e-01,  5.9653e-01,  ..., -5.2395e-02,\n",
       "            -2.2061e+00, -7.1882e-01],\n",
       "           ...,\n",
       "           [-5.9479e-01, -7.9856e-02,  1.7610e-01,  ...,  1.2869e+00,\n",
       "             2.8763e-01,  1.2695e+00],\n",
       "           [-7.2604e-01,  7.1080e-01,  6.4191e-01,  ...,  1.5229e+00,\n",
       "             3.9908e-01,  1.0951e+00],\n",
       "           [-9.1945e-02,  9.9139e-01,  7.0409e-01,  ...,  1.2483e+00,\n",
       "             2.6839e-01,  1.2634e+00]],\n",
       " \n",
       "          [[-2.5157e-02,  8.7278e-04,  6.0443e-03,  ..., -1.2411e-01,\n",
       "             8.0369e-02, -1.0235e-01],\n",
       "           [ 5.4539e-01,  9.1700e-02, -3.1214e-02,  ..., -1.1097e-01,\n",
       "             2.6012e-01, -5.7264e-02],\n",
       "           [-2.1249e-02, -9.5263e-02, -4.9477e-01,  ..., -1.3490e+00,\n",
       "            -1.1967e-02, -1.1628e+00],\n",
       "           ...,\n",
       "           [ 2.2061e-01,  3.2908e-01, -9.0425e-02,  ...,  7.6823e-01,\n",
       "             1.3240e+00,  6.3334e-01],\n",
       "           [-6.0177e-01,  7.6486e-02,  3.9596e-02,  ...,  6.7785e-01,\n",
       "             1.0733e+00,  8.6872e-01],\n",
       "           [-7.1105e-01, -3.1088e-02, -5.8592e-02,  ...,  7.3503e-01,\n",
       "             1.3864e+00,  6.7693e-01]],\n",
       " \n",
       "          [[ 6.5683e-04, -1.5097e-02,  1.9735e-03,  ...,  4.1443e-01,\n",
       "             2.5224e-01, -1.7893e-01],\n",
       "           [-4.4647e-02,  9.7400e-01, -4.4056e-01,  ..., -1.5491e+00,\n",
       "            -2.5756e-01,  1.0097e+00],\n",
       "           [ 3.5626e-01,  1.5721e+00, -1.1908e+00,  ..., -9.6387e-01,\n",
       "             1.5887e-01, -7.2382e-02],\n",
       "           ...,\n",
       "           [ 7.7731e-01, -5.5791e-01,  7.2689e-01,  ..., -3.5156e-01,\n",
       "            -1.3963e+00,  4.8161e-02],\n",
       "           [ 3.1492e-01, -5.3807e-01, -8.4986e-01,  ..., -6.9135e-02,\n",
       "            -1.1500e+00,  4.8901e-02],\n",
       "           [-5.6014e-01, -1.6929e-01, -2.1047e+00,  ..., -2.9878e-01,\n",
       "            -1.3329e+00,  8.7317e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.8998e-02,  1.0144e-02,  5.0974e-03,  ..., -1.5109e+00,\n",
       "             1.6999e-01, -5.0782e-01],\n",
       "           [ 2.8054e-01,  6.4358e-01, -7.2816e-01,  ...,  3.4822e+00,\n",
       "            -1.5279e+00, -1.3033e+00],\n",
       "           [-1.4242e+00,  1.4252e+00, -1.1225e+00,  ...,  4.3641e+00,\n",
       "            -2.3331e+00, -9.1825e-01],\n",
       "           ...,\n",
       "           [-4.1158e-01,  7.9769e-01, -1.3341e+00,  ..., -3.2417e+00,\n",
       "            -1.5004e-01, -8.3683e+00],\n",
       "           [ 2.7787e-01,  1.0210e+00, -5.8987e-01,  ..., -3.3231e+00,\n",
       "            -2.7051e-01, -8.0066e+00],\n",
       "           [ 6.2420e-01,  5.4451e-01,  4.7787e-01,  ..., -3.3577e+00,\n",
       "            -2.4419e-01, -8.5503e+00]],\n",
       " \n",
       "          [[-1.6434e-02, -9.4743e-03, -1.1695e-02,  ..., -1.7287e-01,\n",
       "             6.4271e-02,  3.6552e-03],\n",
       "           [ 3.3956e-01,  2.1051e-01,  7.2630e-01,  ..., -2.1915e-02,\n",
       "             4.7817e-01,  6.8513e-01],\n",
       "           [ 6.1851e-01,  2.1747e-01,  1.8532e-01,  ...,  5.1591e-01,\n",
       "            -9.8686e-01, -1.1938e+00],\n",
       "           ...,\n",
       "           [ 9.5958e-01, -7.2849e-01, -2.1816e-01,  ...,  9.9400e-02,\n",
       "            -3.6060e-01, -1.0901e+00],\n",
       "           [-2.3462e-01, -1.5077e+00,  5.7035e-02,  ..., -3.9342e-02,\n",
       "            -2.4553e-01, -9.8144e-01],\n",
       "           [-6.9897e-01, -1.0541e+00,  3.2737e-01,  ...,  8.6206e-02,\n",
       "            -3.8762e-01, -1.1130e+00]],\n",
       " \n",
       "          [[-2.0261e-02,  1.4136e-02,  1.8468e-03,  ..., -2.9581e-01,\n",
       "            -6.9971e-02, -1.4973e-01],\n",
       "           [ 2.4755e+00, -2.8411e+00,  2.5012e+00,  ...,  4.4955e-01,\n",
       "            -3.9248e-01,  9.7623e-01],\n",
       "           [ 1.5034e+00, -2.5061e+00,  1.0311e+00,  ...,  3.0180e-01,\n",
       "             8.0454e-01,  8.5277e-01],\n",
       "           ...,\n",
       "           [-2.4947e+00,  1.4764e+00,  1.7229e+00,  ...,  2.0071e+00,\n",
       "            -1.2504e+00,  7.2014e-01],\n",
       "           [-7.6572e-01,  4.7051e-01,  6.4904e-01,  ...,  2.1328e+00,\n",
       "            -1.3094e+00,  9.5837e-01],\n",
       "           [ 1.6128e+00, -5.8711e-01, -8.7192e-01,  ...,  2.0855e+00,\n",
       "            -1.3133e+00,  6.2565e-01]]]]),\n",
       " tensor([[[[-5.5994e-03,  3.0430e-02, -6.2770e-03,  ...,  2.8681e-01,\n",
       "             6.2385e-02, -1.9648e-01],\n",
       "           [-4.5377e-01, -6.1979e-01, -5.9934e-01,  ..., -1.8128e+00,\n",
       "            -7.6693e-01,  1.0337e+00],\n",
       "           [-3.7387e-01,  3.9514e-01, -4.3515e-01,  ..., -2.3425e+00,\n",
       "            -6.5380e-01,  1.8454e-02],\n",
       "           ...,\n",
       "           [ 7.1157e-01,  4.4017e-01, -8.8634e-01,  ...,  1.2973e+00,\n",
       "            -1.9634e+00, -1.6445e-01],\n",
       "           [ 9.1551e-01,  6.8281e-01, -7.3208e-01,  ...,  1.0111e+00,\n",
       "            -1.8228e+00,  6.3782e-02],\n",
       "           [ 4.9055e-01,  3.4031e-01, -2.6249e-01,  ...,  1.3148e+00,\n",
       "            -1.9267e+00, -1.4431e-01]],\n",
       " \n",
       "          [[ 5.0155e-03,  9.2425e-03,  1.3157e-02,  ...,  6.2159e-01,\n",
       "             2.8919e-01,  5.0871e-01],\n",
       "           [ 1.8424e+00, -1.4693e-01,  5.2295e-02,  ..., -1.9170e+00,\n",
       "             5.4788e-02, -2.2082e+00],\n",
       "           [ 1.4217e+00, -4.1878e-01,  3.3976e-01,  ..., -1.9623e+00,\n",
       "             6.8134e-02, -2.4391e+00],\n",
       "           ...,\n",
       "           [ 1.8973e+00, -6.1394e-01, -1.0359e+00,  ..., -4.2508e-01,\n",
       "             2.8343e-01, -1.6397e+00],\n",
       "           [ 2.8343e+00,  1.0426e-01, -8.7324e-01,  ..., -3.0402e-01,\n",
       "             6.8554e-01, -1.5800e+00],\n",
       "           [ 9.0708e-01,  4.4577e-01, -4.3077e-01,  ..., -5.3973e-01,\n",
       "             2.0827e-01, -1.7036e+00]],\n",
       " \n",
       "          [[ 1.4090e-02,  1.7858e-02,  3.3792e-03,  ...,  5.1979e-02,\n",
       "             1.2304e+00, -1.2302e+00],\n",
       "           [ 4.7532e-02, -6.4259e-02, -6.3940e-03,  ..., -1.7713e+00,\n",
       "            -1.7076e+00,  2.5874e+00],\n",
       "           [-1.8811e-03,  8.4855e-02,  2.6323e-01,  ..., -7.4486e-01,\n",
       "            -2.0871e+00,  4.1086e+00],\n",
       "           ...,\n",
       "           [ 3.3010e-01,  8.8401e-02,  2.8208e-01,  ...,  1.6294e+00,\n",
       "             2.7540e+00,  1.6273e+00],\n",
       "           [ 3.6298e-01, -9.0918e-01,  4.5410e-01,  ...,  1.2257e+00,\n",
       "             2.0389e+00,  1.3347e+00],\n",
       "           [-3.5707e-02, -1.2866e+00,  5.3088e-01,  ...,  1.6395e+00,\n",
       "             2.8940e+00,  1.7268e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 8.9686e-03,  5.6570e-03, -3.2342e-03,  ..., -1.6854e-01,\n",
       "            -2.4639e-01, -1.3454e-01],\n",
       "           [ 9.3073e-01,  5.9738e-01, -6.3083e-01,  ..., -9.4302e-01,\n",
       "             2.3086e-01, -1.1532e+00],\n",
       "           [-1.3452e+00, -1.5994e-01, -4.1085e-01,  ...,  4.1694e-01,\n",
       "             6.2505e-01, -7.5094e-01],\n",
       "           ...,\n",
       "           [-5.8639e-01, -1.9438e-01, -1.3245e-01,  ...,  5.6736e-02,\n",
       "             9.9477e-01,  1.9194e+00],\n",
       "           [-1.0903e+00, -1.4966e+00,  3.1249e-01,  ..., -1.8880e-01,\n",
       "             7.5694e-01,  1.9469e+00],\n",
       "           [-5.1961e-01, -1.9185e+00,  7.1631e-01,  ..., -1.1330e-02,\n",
       "             9.7500e-01,  1.8775e+00]],\n",
       " \n",
       "          [[-1.8720e-02,  7.1289e-03, -2.0854e-02,  ...,  9.6459e-01,\n",
       "            -1.2275e-01,  4.1718e-01],\n",
       "           [-1.5549e-01, -2.4709e-01,  1.5742e-01,  ..., -3.0697e+00,\n",
       "             2.4185e+00,  1.3410e-01],\n",
       "           [-1.7127e-01,  3.4014e-01,  5.7808e-01,  ..., -3.1245e+00,\n",
       "             1.1896e+00,  1.2541e-01],\n",
       "           ...,\n",
       "           [-3.4843e+00, -1.4809e+00, -6.2534e-01,  ...,  7.2304e+00,\n",
       "            -2.9832e-01, -1.7782e+00],\n",
       "           [-2.0484e+00, -1.1235e+00, -4.3124e-01,  ...,  5.4918e+00,\n",
       "            -2.9385e-03, -1.5923e+00],\n",
       "           [ 1.1564e+00,  1.6267e-01,  2.4873e-01,  ...,  7.8763e+00,\n",
       "            -3.5708e-01, -1.8464e+00]],\n",
       " \n",
       "          [[ 1.4501e-02,  5.8138e-04,  1.5472e-02,  ...,  2.2637e-01,\n",
       "            -2.9873e-01, -7.4865e-01],\n",
       "           [ 5.9695e-02,  4.6134e-02, -5.8120e-01,  ...,  4.1668e-01,\n",
       "             1.9146e+00,  4.5444e+00],\n",
       "           [-5.3970e-01,  7.3320e-01, -3.4361e-01,  ...,  6.7778e-01,\n",
       "             2.0867e+00,  3.7452e+00],\n",
       "           ...,\n",
       "           [-3.2436e-01, -1.5963e-01, -2.3438e-01,  ...,  1.4599e+00,\n",
       "             1.3539e+00,  3.6277e+00],\n",
       "           [ 8.7277e-02, -1.8504e-01,  2.1023e-02,  ...,  1.5892e+00,\n",
       "             1.1045e+00,  2.8557e+00],\n",
       "           [ 2.2340e-01, -2.1851e-02,  2.5352e-01,  ...,  1.4593e+00,\n",
       "             1.4361e+00,  3.9399e+00]]]]),\n",
       " tensor([[[[ 1.7099e-03, -7.0966e-03, -3.4683e-02,  ..., -2.7068e-02,\n",
       "            -1.1403e-01, -4.3077e-03],\n",
       "           [-4.8145e-01,  3.2835e-02,  9.2934e-01,  ...,  6.1299e-01,\n",
       "             7.1181e-01, -2.9840e+00],\n",
       "           [-3.8395e-01,  8.0403e-02,  1.2361e+00,  ...,  1.2095e-01,\n",
       "             8.0921e-01,  3.2326e-01],\n",
       "           ...,\n",
       "           [ 1.6230e+00, -2.0884e-01, -4.3307e-01,  ...,  1.7664e+00,\n",
       "             8.9861e-01, -1.8201e-01],\n",
       "           [ 1.1450e+00, -4.9241e-01, -4.0064e-01,  ...,  1.9657e+00,\n",
       "             1.0804e+00, -1.6554e-01],\n",
       "           [-3.5344e-01, -3.8859e-01, -2.0686e-01,  ...,  1.7381e+00,\n",
       "             1.0107e+00, -2.0395e-01]],\n",
       " \n",
       "          [[-9.3858e-03, -6.2947e-03, -5.5885e-03,  ..., -1.4554e-02,\n",
       "             2.3962e-02, -2.6448e-01],\n",
       "           [ 5.5683e-01, -7.9215e-02, -5.0485e-01,  ..., -2.2124e+00,\n",
       "             1.9296e+00,  8.5557e-03],\n",
       "           [ 3.0336e-01,  7.1663e-01,  6.1506e-01,  ..., -1.0369e+00,\n",
       "             2.7060e+00,  8.4077e-01],\n",
       "           ...,\n",
       "           [-9.3463e-01, -1.4267e-01,  2.2709e-01,  ..., -9.6153e-01,\n",
       "            -1.6403e-01,  2.7237e+00],\n",
       "           [-6.1759e-01, -6.0201e-01,  2.5155e-01,  ..., -9.0833e-01,\n",
       "             3.7238e-02,  2.7843e+00],\n",
       "           [ 2.6470e-01, -6.1478e-01,  8.4492e-02,  ..., -9.7196e-01,\n",
       "            -1.9383e-01,  2.7270e+00]],\n",
       " \n",
       "          [[ 1.0543e-02, -1.3764e-03,  1.5236e-02,  ..., -2.9054e-02,\n",
       "             3.1313e-02,  2.1517e-01],\n",
       "           [ 1.7660e+00,  4.9785e-01,  1.0164e+00,  ..., -1.0474e-01,\n",
       "            -4.6726e-01, -9.2808e-01],\n",
       "           [ 7.8903e-01,  1.0466e+00, -3.8199e-02,  ..., -4.4549e-02,\n",
       "            -2.3958e+00,  2.4152e-01],\n",
       "           ...,\n",
       "           [-6.5925e-01, -3.5797e-01, -9.8247e-01,  ...,  1.5140e+00,\n",
       "            -6.9347e-02,  1.7901e+00],\n",
       "           [-1.3529e+00,  7.1996e-01, -3.3435e-01,  ...,  1.4766e+00,\n",
       "            -2.4593e-02,  1.8640e+00],\n",
       "           [-8.9867e-01,  1.3001e+00,  4.5473e-01,  ...,  1.4918e+00,\n",
       "            -1.2505e-01,  1.7116e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.6197e-02, -8.2137e-03, -1.6645e-02,  ..., -3.9038e-01,\n",
       "            -5.6940e-01,  5.2322e-02],\n",
       "           [ 5.7361e-02, -8.5774e-01,  1.2852e-01,  ..., -2.9551e+00,\n",
       "            -1.2386e+00, -1.1850e-01],\n",
       "           [-3.0132e-01, -1.3785e+00,  5.6729e-01,  ..., -2.0430e+00,\n",
       "            -1.1890e+00, -7.6568e-01],\n",
       "           ...,\n",
       "           [-8.2192e-01,  7.6039e-01,  6.5767e-01,  ...,  6.1247e-01,\n",
       "             5.8605e+00, -9.1710e+00],\n",
       "           [-4.6789e-01,  8.8864e-01,  4.9067e-01,  ...,  6.2460e-01,\n",
       "             5.7466e+00, -9.1411e+00],\n",
       "           [ 3.0926e-01,  5.1703e-01,  2.3872e-01,  ...,  6.0587e-01,\n",
       "             5.8751e+00, -9.0768e+00]],\n",
       " \n",
       "          [[-1.4438e-03, -1.1795e-02, -2.5977e-02,  ..., -1.8052e-02,\n",
       "             3.0287e-02,  1.4621e-01],\n",
       "           [-1.4887e+00,  9.3785e-02, -7.4922e-01,  ...,  2.1418e+00,\n",
       "            -1.1741e+00, -2.0399e+00],\n",
       "           [-1.3755e-01, -6.9058e-01,  1.3884e-01,  ...,  1.0255e+00,\n",
       "            -3.0747e-01, -3.4008e-01],\n",
       "           ...,\n",
       "           [-5.1129e-01, -5.6926e-01, -7.2972e-01,  ...,  2.2326e+00,\n",
       "            -4.6576e-01, -1.5766e+00],\n",
       "           [-1.7817e-01, -3.0057e-01, -9.5500e-02,  ...,  2.3246e+00,\n",
       "            -5.2111e-01, -1.4736e+00],\n",
       "           [ 3.1465e-01,  1.5772e-01,  4.7024e-01,  ...,  2.1711e+00,\n",
       "            -3.7561e-01, -1.6465e+00]],\n",
       " \n",
       "          [[-8.6781e-03,  5.7958e-03, -8.7408e-03,  ...,  1.1581e-01,\n",
       "             2.3304e-02,  2.4817e-01],\n",
       "           [ 8.8511e-01, -2.6205e+00,  1.0181e-02,  ..., -1.2031e+00,\n",
       "             1.9655e+00,  7.3809e-01],\n",
       "           [ 1.0694e+00, -1.2972e+00, -1.9754e+00,  ..., -7.8371e-01,\n",
       "             1.8825e+00,  3.1647e+00],\n",
       "           ...,\n",
       "           [ 2.2185e-01, -1.3894e+00,  5.2774e-01,  ..., -4.7712e+00,\n",
       "            -1.3278e+00, -3.1640e+00],\n",
       "           [-2.9077e-01, -5.7226e-01,  1.8180e-02,  ..., -4.7532e+00,\n",
       "            -1.1468e+00, -3.0426e+00],\n",
       "           [-8.1911e-01,  5.7860e-01, -5.7863e-01,  ..., -4.7183e+00,\n",
       "            -1.3044e+00, -3.1704e+00]]]]),\n",
       " tensor([[[[-3.1937e-02, -8.8295e-03,  7.3780e-02,  ..., -8.6752e-01,\n",
       "            -8.4201e-02, -7.0821e-02],\n",
       "           [ 7.7665e-01,  1.8661e-02, -2.8128e-01,  ..., -3.1994e-02,\n",
       "             1.4260e+00, -1.0654e+00],\n",
       "           [ 2.9481e-01,  4.1350e-01,  1.9019e-01,  ...,  1.3084e+00,\n",
       "             6.6743e-01, -9.0929e-01],\n",
       "           ...,\n",
       "           [ 6.6442e-02, -9.0924e-02,  3.8749e-02,  ..., -3.5130e-01,\n",
       "             2.4177e+00, -5.7353e-01],\n",
       "           [ 6.0128e-02, -2.0503e-01,  5.7813e-02,  ..., -3.5060e-01,\n",
       "             2.4152e+00, -5.7265e-01],\n",
       "           [-1.3518e-03, -1.7631e-01,  4.6515e-02,  ..., -3.5276e-01,\n",
       "             2.4204e+00, -5.7454e-01]],\n",
       " \n",
       "          [[ 5.9258e-02, -1.2030e-01,  1.2843e-02,  ...,  5.7094e-02,\n",
       "            -3.2246e-01,  4.1472e-01],\n",
       "           [-6.8007e-01, -2.2559e-01,  9.5958e-01,  ..., -1.6668e+00,\n",
       "             1.5882e+00,  5.5227e-01],\n",
       "           [-8.3095e-01, -9.4782e-01,  6.4177e-01,  ..., -3.6168e-01,\n",
       "            -1.2744e+00,  1.9225e-01],\n",
       "           ...,\n",
       "           [ 6.7759e-02, -1.2358e-01,  1.3641e-01,  ...,  6.9616e-02,\n",
       "             2.2363e-01, -6.2741e-02],\n",
       "           [ 7.3796e-02, -1.4027e-02,  1.2765e-01,  ...,  6.9439e-02,\n",
       "             2.2325e-01, -6.2382e-02],\n",
       "           [ 1.4016e-02,  1.0515e-01,  5.1762e-02,  ...,  6.9733e-02,\n",
       "             2.2373e-01, -6.2841e-02]],\n",
       " \n",
       "          [[ 3.8748e-02,  2.8362e-02,  3.8982e-02,  ...,  2.4248e-02,\n",
       "             3.4127e-01, -1.6051e+00],\n",
       "           [-1.2734e-01, -6.4447e-03, -1.5488e-01,  ..., -4.8321e-01,\n",
       "            -1.0372e+00,  1.8934e+00],\n",
       "           [ 1.0383e-02,  1.2616e-01,  6.7888e-02,  ..., -8.3794e-01,\n",
       "            -1.1515e+00,  9.3217e-01],\n",
       "           ...,\n",
       "           [ 2.5002e-02, -1.1324e-01, -5.6887e-02,  ..., -1.0794e+00,\n",
       "             5.3091e-01, -2.9301e+00],\n",
       "           [ 2.1327e-02, -4.8356e-02, -4.2870e-02,  ..., -1.0790e+00,\n",
       "             5.3048e-01, -2.9289e+00],\n",
       "           [-1.7672e-03,  5.1223e-02, -6.0177e-03,  ..., -1.0799e+00,\n",
       "             5.3175e-01, -2.9324e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.0268e-02, -6.0059e-02,  2.9586e-02,  ...,  7.8957e-01,\n",
       "            -7.4411e-01,  1.4832e+00],\n",
       "           [-6.4064e-01, -2.2624e-01,  6.2840e-01,  ..., -5.4014e-01,\n",
       "             8.9799e-01, -8.5259e-01],\n",
       "           [ 5.6852e-01, -8.4187e-01,  1.0794e+00,  ..., -7.4421e-01,\n",
       "             2.5440e+00, -2.3671e+00],\n",
       "           ...,\n",
       "           [-1.7418e-01,  1.5142e-01, -1.4405e-01,  ..., -1.0837e+00,\n",
       "             2.2479e+00, -4.7704e-01],\n",
       "           [-6.4453e-02,  2.5549e-02, -1.2535e-01,  ..., -1.0840e+00,\n",
       "             2.2466e+00, -4.7818e-01],\n",
       "           [ 1.0381e-01, -1.1903e-01, -3.9095e-02,  ..., -1.0834e+00,\n",
       "             2.2494e+00, -4.7549e-01]],\n",
       " \n",
       "          [[ 5.9131e-02,  6.1968e-02, -4.2211e-02,  ..., -6.9190e-02,\n",
       "            -5.6339e-01,  1.8437e-01],\n",
       "           [ 7.0369e-01,  2.2736e-01,  8.2039e-01,  ...,  1.0520e+00,\n",
       "            -6.3220e-01, -1.3715e-01],\n",
       "           [ 1.1126e-01, -7.1917e-01, -7.5048e-01,  ...,  1.4380e+00,\n",
       "            -6.7015e-01,  3.3225e-01],\n",
       "           ...,\n",
       "           [ 2.4381e-03,  3.5310e-02, -3.4580e-02,  ...,  3.2324e+00,\n",
       "             2.5738e-01, -6.6795e-01],\n",
       "           [-1.1523e-02, -1.5692e-02, -7.2602e-02,  ...,  3.2307e+00,\n",
       "             2.5649e-01, -6.6720e-01],\n",
       "           [-1.4515e-02, -5.5813e-02, -7.1871e-02,  ...,  3.2342e+00,\n",
       "             2.5783e-01, -6.6872e-01]],\n",
       " \n",
       "          [[ 2.5758e-02,  1.8784e-02,  7.3905e-03,  ..., -4.2254e-01,\n",
       "             6.2659e-01, -7.3518e-02],\n",
       "           [-7.1581e-01,  1.7477e+00,  3.2028e-01,  ...,  2.2488e-01,\n",
       "            -3.6095e-01, -2.2553e+00],\n",
       "           [-1.2228e+00, -1.1374e+00,  2.5679e-01,  ..., -7.1917e-02,\n",
       "             4.3226e-01,  2.6965e-01],\n",
       "           ...,\n",
       "           [-5.2629e-02, -1.8811e-02,  1.0816e-02,  ...,  5.1484e-01,\n",
       "             6.9126e-01, -5.0432e-01],\n",
       "           [ 1.0283e-01, -1.8553e-02,  3.6218e-02,  ...,  5.1473e-01,\n",
       "             6.9147e-01, -5.0438e-01],\n",
       "           [ 1.6537e-01, -4.5549e-03,  4.2199e-02,  ...,  5.1479e-01,\n",
       "             6.9162e-01, -5.0415e-01]]]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['past_key_value'].key_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-3.0323e-03,  2.9014e-03, -2.5995e-03,  ...,  3.7256e-04,\n",
       "            -6.8101e-03,  2.4478e-03],\n",
       "           [ 4.9791e-03,  1.8254e-03,  4.7753e-04,  ..., -1.6144e-04,\n",
       "             6.5963e-04,  1.7619e-03],\n",
       "           [ 3.3913e-03, -4.6228e-03, -1.4544e-02,  ...,  4.5442e-03,\n",
       "             6.1396e-03, -7.7402e-03],\n",
       "           ...,\n",
       "           [ 5.6766e-03,  1.4015e-04,  1.0991e-03,  ..., -3.7779e-03,\n",
       "            -5.6904e-04,  1.0857e-02],\n",
       "           [-1.0003e-03, -7.0993e-03,  3.6075e-03,  ..., -5.5194e-04,\n",
       "             1.9539e-03,  4.3085e-03],\n",
       "           [-2.4467e-03,  1.1709e-02,  8.7955e-03,  ..., -1.6282e-03,\n",
       "            -8.2406e-03, -1.0132e-02]],\n",
       " \n",
       "          [[ 3.1156e-03,  3.2496e-04,  1.1357e-02,  ...,  1.8874e-03,\n",
       "             7.7895e-03, -5.8534e-03],\n",
       "           [-1.5936e-03, -3.9710e-03, -5.3878e-03,  ..., -7.5102e-04,\n",
       "            -3.9153e-03, -1.0004e-03],\n",
       "           [-3.3756e-03, -4.5582e-03, -1.3133e-02,  ...,  1.0132e-02,\n",
       "            -2.0331e-03, -7.9861e-03],\n",
       "           ...,\n",
       "           [-6.6557e-03,  1.0366e-02,  3.7189e-03,  ...,  6.1157e-03,\n",
       "            -4.6138e-03,  1.3597e-02],\n",
       "           [ 1.4996e-03,  1.4286e-03,  4.0420e-03,  ...,  2.2344e-03,\n",
       "            -1.5796e-03,  6.8868e-04],\n",
       "           [ 5.4075e-03, -2.5013e-04,  5.4033e-03,  ...,  1.2922e-03,\n",
       "            -2.9322e-03, -1.0213e-03]],\n",
       " \n",
       "          [[ 2.1070e-03, -2.3957e-03,  3.3266e-03,  ..., -2.8863e-03,\n",
       "            -4.5651e-03,  1.1155e-04],\n",
       "           [-1.3432e-03,  3.3929e-03, -9.0618e-03,  ...,  1.8220e-03,\n",
       "             1.7116e-03,  1.4508e-03],\n",
       "           [ 4.1897e-04,  3.6272e-03,  6.8833e-03,  ..., -5.5847e-03,\n",
       "            -5.2575e-05,  6.8830e-04],\n",
       "           ...,\n",
       "           [-6.0844e-04, -4.9919e-03, -5.0000e-03,  ...,  7.9449e-03,\n",
       "             3.7347e-03,  2.0766e-03],\n",
       "           [ 4.5617e-03,  4.2278e-03,  3.5906e-03,  ..., -1.5197e-03,\n",
       "             7.0741e-04, -3.9266e-03],\n",
       "           [ 5.7328e-03,  6.1627e-03, -5.1294e-03,  ...,  6.9376e-03,\n",
       "            -5.6965e-03, -1.5832e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.0154e-04,  2.5075e-02, -8.5972e-02,  ..., -6.0138e-02,\n",
       "            -1.6860e-02, -2.6598e-02],\n",
       "           [-1.0166e-02, -2.7523e-02, -2.2064e-02,  ...,  4.1538e-03,\n",
       "            -3.1007e-02, -1.8289e-02],\n",
       "           [-1.3264e-02,  2.4761e-02,  4.4840e-02,  ...,  5.5638e-02,\n",
       "            -1.3407e-04,  3.7564e-02],\n",
       "           ...,\n",
       "           [ 1.8129e-02, -8.6009e-03,  1.7941e-02,  ...,  5.1879e-02,\n",
       "            -5.0100e-02, -5.0492e-02],\n",
       "           [ 1.4700e-02, -3.5852e-02, -4.5829e-03,  ...,  1.1438e-02,\n",
       "            -2.8578e-02,  1.5462e-02],\n",
       "           [ 2.4287e-02,  4.3132e-02,  5.4399e-02,  ..., -5.4073e-02,\n",
       "            -2.1029e-02, -6.2641e-03]],\n",
       " \n",
       "          [[-2.4693e-03, -6.4147e-04,  2.8042e-03,  ...,  3.4349e-03,\n",
       "             1.4700e-02, -1.5671e-03],\n",
       "           [ 1.2120e-02,  4.4621e-03,  2.1716e-03,  ...,  2.5435e-03,\n",
       "            -1.5789e-04, -7.4070e-04],\n",
       "           [-1.1588e-02, -4.9534e-04, -4.0350e-03,  ...,  2.9895e-03,\n",
       "            -6.0112e-03,  7.0579e-04],\n",
       "           ...,\n",
       "           [-3.0789e-03,  4.2697e-03, -4.9317e-03,  ...,  4.9018e-03,\n",
       "             6.3829e-04,  4.8792e-03],\n",
       "           [-6.3183e-05,  1.1411e-03, -6.6466e-03,  ..., -3.1416e-03,\n",
       "            -1.2852e-04, -8.7685e-03],\n",
       "           [-7.8480e-03,  5.3208e-04, -5.2402e-03,  ..., -7.3224e-03,\n",
       "            -9.9813e-03,  3.0630e-03]],\n",
       " \n",
       "          [[ 2.5758e-03, -1.4309e-02,  3.6864e-03,  ...,  1.3476e-03,\n",
       "            -3.3641e-03,  1.2984e-03],\n",
       "           [ 7.8242e-03, -2.3447e-03, -3.0252e-04,  ...,  6.3087e-03,\n",
       "             7.5075e-03,  1.6743e-03],\n",
       "           [ 1.5930e-02,  1.6306e-02,  9.9532e-03,  ...,  3.3097e-03,\n",
       "             9.6144e-03,  6.1967e-03],\n",
       "           ...,\n",
       "           [ 6.0032e-03,  9.4029e-03, -2.1153e-03,  ..., -2.7855e-04,\n",
       "             1.2163e-02, -2.3104e-03],\n",
       "           [-2.3026e-03,  2.6669e-03,  3.3421e-03,  ...,  4.9243e-03,\n",
       "            -6.3231e-04, -8.8302e-04],\n",
       "           [ 7.0480e-03,  8.1127e-03,  4.4727e-03,  ...,  1.7349e-02,\n",
       "             4.3077e-03, -3.7690e-03]]]]),\n",
       " tensor([[[[ 7.7730e-02, -6.0227e-02,  4.9944e-02,  ...,  1.6385e-02,\n",
       "            -8.6901e-03, -7.7484e-02],\n",
       "           [-3.4800e-03, -2.7316e-02, -3.3351e-02,  ..., -4.9821e-02,\n",
       "             5.6361e-02,  1.0748e-01],\n",
       "           [ 2.3313e-02,  4.6236e-02, -2.6707e-02,  ...,  6.9382e-02,\n",
       "            -3.0329e-03,  5.1213e-02],\n",
       "           ...,\n",
       "           [ 7.8638e-03,  2.7654e-02, -6.5848e-02,  ...,  2.6332e-02,\n",
       "             1.3398e-02, -5.3510e-02],\n",
       "           [ 1.6600e-01,  4.8112e-03, -3.0680e-02,  ...,  1.6575e-02,\n",
       "            -1.1121e-02, -1.1076e-02],\n",
       "           [ 1.1035e-01,  2.9190e-03, -7.7504e-02,  ..., -3.2817e-02,\n",
       "             4.1963e-02, -3.4721e-02]],\n",
       " \n",
       "          [[-4.7699e-03, -1.4322e-02,  2.2159e-02,  ...,  1.6624e-02,\n",
       "             4.9097e-03,  5.6224e-03],\n",
       "           [ 6.3924e-03, -2.4167e-03, -1.9973e-03,  ..., -5.2901e-04,\n",
       "             8.0087e-03,  5.4953e-03],\n",
       "           [ 1.2182e-02, -3.5434e-03, -4.2626e-03,  ...,  2.9671e-03,\n",
       "             5.3482e-04, -3.0328e-03],\n",
       "           ...,\n",
       "           [-3.1165e-03,  1.9376e-02,  1.1831e-02,  ..., -6.6126e-03,\n",
       "             3.0810e-02,  8.4194e-03],\n",
       "           [-5.0185e-03,  1.6878e-02, -8.1980e-04,  ..., -2.6759e-04,\n",
       "             1.4624e-02,  2.2495e-02],\n",
       "           [-3.5498e-04,  2.6963e-02,  2.9100e-03,  ...,  8.7858e-03,\n",
       "             1.8860e-02,  4.5870e-03]],\n",
       " \n",
       "          [[ 1.1226e-02, -6.8756e-03, -7.8158e-03,  ...,  1.1116e-01,\n",
       "            -9.7392e-03, -1.9225e-02],\n",
       "           [-3.3597e-02,  1.5118e-02, -1.1927e-02,  ...,  7.2004e-02,\n",
       "             5.7736e-02,  5.8798e-03],\n",
       "           [ 6.8834e-02,  3.7252e-02,  8.2518e-02,  ..., -1.3869e-01,\n",
       "             1.0363e-01, -7.9371e-02],\n",
       "           ...,\n",
       "           [-8.3139e-02, -9.9622e-03,  8.3913e-02,  ...,  1.2460e-01,\n",
       "            -1.3935e-01,  7.4519e-01],\n",
       "           [-1.5047e-01,  4.2480e-04,  7.2226e-02,  ...,  2.9447e-01,\n",
       "            -1.4817e-01,  7.2897e-01],\n",
       "           [-1.3927e-01,  2.6938e-02,  1.1798e-01,  ...,  1.2139e-01,\n",
       "            -9.9527e-02,  9.0655e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.4029e-02,  1.2715e-02,  2.1529e-02,  ..., -1.4067e-02,\n",
       "             8.9380e-04, -2.2675e-02],\n",
       "           [-1.5905e-02,  3.4358e-04,  1.3354e-02,  ...,  1.0289e-02,\n",
       "            -2.4687e-02,  4.3720e-03],\n",
       "           [ 6.7030e-04,  4.2693e-03,  5.5501e-03,  ..., -5.1573e-03,\n",
       "            -2.1340e-03, -6.2594e-02],\n",
       "           ...,\n",
       "           [ 1.8995e-02, -1.4329e-02,  4.4011e-02,  ..., -4.5083e-02,\n",
       "            -1.2755e-02,  4.1506e-03],\n",
       "           [ 1.4892e-02, -1.6283e-02,  2.4251e-02,  ..., -5.6462e-02,\n",
       "            -4.3093e-02, -2.3432e-03],\n",
       "           [-6.8250e-03, -5.0113e-03,  1.7190e-02,  ..., -5.5842e-02,\n",
       "            -5.4923e-02, -2.0871e-02]],\n",
       " \n",
       "          [[-9.7888e-02, -1.1697e-01, -7.3323e-02,  ..., -2.7238e-03,\n",
       "            -6.6927e-03, -2.8958e-02],\n",
       "           [-3.3121e-02, -1.5378e-01, -3.2483e-02,  ..., -1.4580e-02,\n",
       "            -4.6367e-04, -1.2009e-02],\n",
       "           [ 2.1067e-02, -8.7934e-02,  1.6458e-01,  ..., -3.4422e-04,\n",
       "             9.4399e-03, -5.1322e-03],\n",
       "           ...,\n",
       "           [-9.9129e-03, -8.8602e-02,  1.4893e-01,  ...,  1.2160e-02,\n",
       "             4.4283e-02, -2.3410e-02],\n",
       "           [ 5.8641e-02, -1.0200e-02,  1.0964e-02,  ...,  3.2485e-02,\n",
       "             3.7697e-02, -4.1613e-02],\n",
       "           [-2.3060e-02, -6.3062e-02,  1.4391e-01,  ..., -3.8743e-03,\n",
       "             5.0043e-02, -4.7555e-02]],\n",
       " \n",
       "          [[ 7.1848e-04, -1.5223e-03,  1.2499e-02,  ...,  1.7696e-02,\n",
       "             5.8707e-03,  5.6291e-03],\n",
       "           [ 6.5946e-03,  1.2359e-02,  1.7758e-02,  ..., -9.8512e-04,\n",
       "            -1.3524e-02,  1.3068e-02],\n",
       "           [-2.6975e-02, -3.2681e-03,  8.5978e-03,  ..., -8.9387e-03,\n",
       "             3.1221e-03,  4.6462e-03],\n",
       "           ...,\n",
       "           [ 9.5779e-03,  2.5018e-03, -5.7692e-03,  ..., -4.3877e-02,\n",
       "             4.5467e-03,  2.2513e-02],\n",
       "           [ 1.0038e-02,  7.8742e-03,  4.5724e-03,  ..., -3.1198e-02,\n",
       "             1.2953e-02,  8.2535e-03],\n",
       "           [-4.9393e-03,  4.0716e-02, -1.3044e-02,  ..., -4.4283e-02,\n",
       "             2.3085e-03,  8.6437e-03]]]]),\n",
       " tensor([[[[ 2.9937e-03,  6.0837e-04,  3.9766e-03,  ...,  9.6533e-04,\n",
       "             1.5186e-03, -3.9546e-03],\n",
       "           [-7.4405e-02,  8.2919e-02, -1.1873e-01,  ...,  1.0965e-01,\n",
       "             1.3784e-02,  1.1671e-01],\n",
       "           [ 5.4494e-02, -6.8302e-02, -5.0704e-02,  ..., -4.8926e-02,\n",
       "             1.5280e-02,  2.4455e-01],\n",
       "           ...,\n",
       "           [-7.2163e-02,  1.0236e-01, -1.8742e-01,  ..., -1.6028e-02,\n",
       "             6.0316e-02, -1.3771e-01],\n",
       "           [-1.6749e-02, -2.0663e-02, -3.1188e-02,  ..., -3.4920e-02,\n",
       "             2.1400e-02, -1.9641e-02],\n",
       "           [ 5.4497e-02,  6.9653e-02, -5.7921e-02,  ..., -6.6243e-02,\n",
       "             2.2290e-02,  2.7222e-02]],\n",
       " \n",
       "          [[ 2.7299e-02,  1.5763e-03,  3.4701e-03,  ...,  7.9767e-04,\n",
       "            -2.7733e-03,  2.5038e-03],\n",
       "           [-7.5698e-01,  7.2822e-02, -4.0424e-01,  ...,  6.6272e-02,\n",
       "             3.3758e-02,  7.8538e-02],\n",
       "           [-8.3009e-01,  1.5694e-01,  2.3847e-01,  ...,  2.6806e-01,\n",
       "            -2.6324e-01,  1.8079e-01],\n",
       "           ...,\n",
       "           [ 1.5953e-01,  3.0888e-01,  1.9089e-01,  ..., -6.8298e-02,\n",
       "             4.1340e-02,  5.9200e-02],\n",
       "           [ 3.5481e-01,  1.5542e-01,  1.2506e-01,  ..., -3.3852e-03,\n",
       "             2.4200e-02,  6.7089e-02],\n",
       "           [ 6.2860e-02,  2.5949e-01,  2.0436e-02,  ..., -9.9057e-02,\n",
       "             1.0375e-01,  9.9421e-02]],\n",
       " \n",
       "          [[ 1.1508e-03, -2.6850e-03,  6.0345e-03,  ...,  2.5726e-03,\n",
       "             8.4672e-05, -2.1974e-03],\n",
       "           [ 2.3449e-01,  1.3986e-02, -3.3111e-02,  ...,  8.6282e-02,\n",
       "            -1.6395e-01,  7.4877e-02],\n",
       "           [-2.1445e-01, -1.3545e-01,  2.2491e-02,  ...,  8.9033e-03,\n",
       "             1.2612e-02,  4.8420e-02],\n",
       "           ...,\n",
       "           [-7.6205e-02,  2.1417e-02,  5.5694e-02,  ...,  1.7726e-02,\n",
       "            -2.9658e-02,  3.8202e-02],\n",
       "           [ 1.9729e-02,  5.5575e-02,  5.4778e-02,  ...,  3.2910e-02,\n",
       "             1.8072e-02,  8.4499e-02],\n",
       "           [-9.1507e-02,  1.1837e-01,  2.7273e-02,  ...,  1.0167e-01,\n",
       "             6.1593e-02, -4.7572e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.9245e-03,  2.2925e-02, -2.7883e-03,  ..., -6.9965e-03,\n",
       "            -7.2486e-04,  5.3087e-03],\n",
       "           [ 2.4721e-03, -1.8516e-01,  1.9016e-01,  ..., -1.2942e-01,\n",
       "             5.1007e-02, -5.7632e-02],\n",
       "           [-8.1280e-02, -5.2197e-01,  1.0394e-01,  ...,  9.5589e-02,\n",
       "             3.5848e-01, -2.0790e-01],\n",
       "           ...,\n",
       "           [-1.1828e-01, -4.3096e-01, -2.3508e-01,  ..., -1.1395e-01,\n",
       "            -8.1732e-02, -5.1930e-02],\n",
       "           [-3.3162e-02, -1.6671e-01, -1.4413e-01,  ..., -2.3733e-02,\n",
       "             5.6598e-02, -8.4917e-02],\n",
       "           [-2.8163e-02, -7.4112e-02, -1.9224e-01,  ..., -4.2482e-02,\n",
       "             1.1801e-01, -9.4258e-02]],\n",
       " \n",
       "          [[ 2.6441e-03, -5.0837e-03,  7.5769e-07,  ..., -2.0616e-04,\n",
       "            -1.7116e-03, -2.5110e-03],\n",
       "           [-3.6021e-01, -2.7121e-01, -2.2140e-01,  ..., -1.8486e-01,\n",
       "            -2.1964e-01,  3.6070e-01],\n",
       "           [-1.3137e-01,  5.5031e-01,  1.8138e-01,  ...,  1.8983e-02,\n",
       "             9.7102e-02, -2.7524e-01],\n",
       "           ...,\n",
       "           [ 7.0007e-02,  3.0116e-01, -9.2658e-02,  ..., -7.4688e-02,\n",
       "             8.4000e-02,  1.4554e-01],\n",
       "           [ 3.7844e-03,  1.3580e-02, -7.4214e-02,  ..., -2.1057e-02,\n",
       "            -1.0639e-01,  1.2201e-01],\n",
       "           [-2.1872e-02,  6.6159e-02, -9.4855e-02,  ..., -7.5479e-02,\n",
       "             8.0383e-02,  1.0216e-01]],\n",
       " \n",
       "          [[ 2.2947e-04, -4.4499e-03,  1.0349e-04,  ...,  1.8613e-02,\n",
       "            -5.7361e-04, -1.8639e-03],\n",
       "           [ 1.0493e-02, -1.7612e-01, -4.6191e-01,  ..., -4.4339e-01,\n",
       "             4.1776e-01,  1.0290e-01],\n",
       "           [-3.2538e-02, -5.3345e-02, -1.8195e-02,  ...,  6.0616e-02,\n",
       "            -2.0210e-01,  3.2833e-02],\n",
       "           ...,\n",
       "           [-1.1560e-01, -5.1478e-02, -2.2639e-01,  ..., -1.3900e-01,\n",
       "            -4.3134e-02, -5.5988e-03],\n",
       "           [ 3.2293e-02, -3.1920e-02, -6.7697e-02,  ..., -1.1581e-01,\n",
       "             1.1246e-03, -4.4000e-03],\n",
       "           [ 7.2279e-02, -1.2883e-02, -1.1611e-01,  ..., -6.8430e-02,\n",
       "            -7.1558e-02, -3.7450e-02]]]]),\n",
       " tensor([[[[ 9.4409e-04, -4.6045e-03,  2.7355e-03,  ..., -1.6416e-03,\n",
       "            -1.4138e-03,  1.6567e-03],\n",
       "           [-3.1923e-01,  9.9795e-02, -1.1970e-01,  ...,  2.3465e-01,\n",
       "             9.4030e-02, -1.9383e-01],\n",
       "           [ 5.9234e-02,  2.5971e-01,  7.7999e-02,  ..., -2.9367e-01,\n",
       "             2.6073e-01,  2.9891e-02],\n",
       "           ...,\n",
       "           [-5.1101e-02, -2.3058e-02,  1.5415e-01,  ...,  1.0260e-03,\n",
       "             1.4565e-01,  2.8272e-02],\n",
       "           [-1.4992e-01, -2.7066e-02,  8.7077e-02,  ...,  1.5718e-02,\n",
       "             7.8173e-02,  9.0961e-02],\n",
       "           [-1.8378e-01, -4.7525e-02,  1.3970e-01,  ...,  8.8141e-02,\n",
       "             1.1179e-01,  9.0205e-02]],\n",
       " \n",
       "          [[-1.0391e-03, -2.0116e-03, -5.7705e-04,  ..., -4.0749e-03,\n",
       "             3.8443e-03, -6.5721e-04],\n",
       "           [ 7.3250e-02, -5.6508e-02, -8.3610e-03,  ...,  1.2726e-01,\n",
       "             3.3006e-02,  8.9215e-02],\n",
       "           [ 1.0170e-01,  2.2167e-02, -2.7306e-01,  ...,  2.4536e-01,\n",
       "            -3.1770e-01, -3.4886e-02],\n",
       "           ...,\n",
       "           [-5.0563e-01, -2.8898e-01, -1.2450e-01,  ...,  1.2472e-02,\n",
       "             4.4325e-02, -2.9636e-01],\n",
       "           [-4.2033e-01, -1.8553e-01, -1.0335e-01,  ...,  1.1414e-01,\n",
       "             1.3063e-01, -2.8501e-01],\n",
       "           [-5.2514e-01, -1.3167e-01, -4.3388e-02,  ...,  6.8434e-02,\n",
       "             8.6889e-03, -4.2188e-01]],\n",
       " \n",
       "          [[-8.0107e-05,  6.4457e-03, -4.6802e-03,  ...,  6.3991e-03,\n",
       "            -6.3706e-03,  3.8655e-03],\n",
       "           [ 3.1437e-01,  3.2830e-01, -1.2661e-02,  ...,  4.2356e-01,\n",
       "            -7.9851e-03,  1.6598e-01],\n",
       "           [-2.6198e-01,  4.1643e-02,  9.0028e-03,  ..., -4.1467e-01,\n",
       "             7.8391e-01,  9.3206e-01],\n",
       "           ...,\n",
       "           [-2.7850e-01,  5.5949e-02,  2.1754e-01,  ..., -6.7231e-02,\n",
       "             3.6205e-02, -5.9358e-02],\n",
       "           [-6.7911e-02,  4.0377e-02,  1.8609e-02,  ..., -9.2501e-02,\n",
       "            -2.7320e-02, -2.4393e-02],\n",
       "           [-1.8799e-01, -7.3974e-02,  1.0331e-01,  ..., -4.4484e-03,\n",
       "             2.0929e-04, -3.7097e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.5974e-03,  4.3506e-03,  6.3973e-03,  ..., -2.6504e-03,\n",
       "            -3.7730e-03, -2.1116e-03],\n",
       "           [ 3.2005e-01, -4.7863e-01, -3.5589e-01,  ...,  5.8560e-02,\n",
       "            -7.9699e-02,  3.0792e-01],\n",
       "           [ 2.7675e-01, -6.1310e-02,  4.1555e-02,  ...,  2.2889e-01,\n",
       "            -4.4605e-01,  1.7797e-01],\n",
       "           ...,\n",
       "           [-1.1702e-02,  9.4952e-02, -1.1781e-01,  ..., -1.3546e-01,\n",
       "             2.5422e-01, -2.4960e-01],\n",
       "           [ 1.1301e-01, -1.9362e-01, -1.4474e-01,  ..., -3.5411e-02,\n",
       "             8.8322e-02, -1.6029e-01],\n",
       "           [ 2.0819e-02, -2.3047e-01, -1.3505e-01,  ..., -2.3630e-01,\n",
       "             2.0386e-01, -1.6871e-01]],\n",
       " \n",
       "          [[-4.9574e-03, -5.1742e-03,  6.9761e-03,  ...,  8.4153e-04,\n",
       "            -2.4693e-03,  3.5213e-03],\n",
       "           [ 4.6142e-01, -3.8797e-01,  6.0862e-01,  ..., -3.4310e-01,\n",
       "             2.4090e-01,  7.1186e-01],\n",
       "           [ 2.1828e-01, -1.9732e-01, -3.7708e-01,  ...,  2.0556e-01,\n",
       "             4.3758e-02,  2.6038e-01],\n",
       "           ...,\n",
       "           [-3.6436e-03, -1.0284e-01, -2.9143e-02,  ...,  1.0755e-01,\n",
       "            -3.8205e-01,  1.8516e-01],\n",
       "           [ 3.4613e-02, -2.0215e-01,  3.6998e-03,  ...,  8.2250e-02,\n",
       "            -3.2764e-01,  1.6238e-01],\n",
       "           [ 4.6222e-02, -1.9402e-01, -1.3372e-01,  ...,  1.5412e-01,\n",
       "            -4.5649e-01,  2.6525e-01]],\n",
       " \n",
       "          [[-3.5345e-03,  2.8708e-03,  5.2017e-03,  ..., -2.9421e-03,\n",
       "             1.4766e-03,  4.2614e-03],\n",
       "           [ 2.6098e-02, -2.2963e-01, -1.1780e-01,  ...,  1.1839e-01,\n",
       "            -1.9370e-01, -3.5210e-02],\n",
       "           [ 3.2474e-02, -1.1132e-01,  1.4315e-03,  ...,  1.0174e-01,\n",
       "            -9.3122e-02,  4.0931e-02],\n",
       "           ...,\n",
       "           [ 5.5515e-02,  4.4771e-01, -1.1454e-01,  ..., -1.0955e-01,\n",
       "             1.1450e-01, -1.2107e-01],\n",
       "           [ 4.4298e-02,  2.8327e-01, -1.6252e-01,  ..., -1.1431e-01,\n",
       "             3.6925e-02, -7.7635e-02],\n",
       "           [ 1.8908e-02,  2.8229e-01, -1.2275e-01,  ..., -7.3378e-02,\n",
       "             1.1885e-01, -7.7096e-02]]]]),\n",
       " tensor([[[[ 2.8362e-03, -4.2298e-03,  4.2532e-03,  ...,  5.9800e-04,\n",
       "            -1.8153e-04, -1.7141e-03],\n",
       "           [ 1.5063e-01,  1.1323e-01,  5.1670e-01,  ..., -3.9340e-01,\n",
       "            -2.0789e-01,  1.5858e-01],\n",
       "           [ 1.2061e-01, -3.9084e-02,  1.8745e-01,  ...,  1.4478e-01,\n",
       "            -1.6177e-01,  2.4497e-01],\n",
       "           ...,\n",
       "           [ 2.7560e-02,  1.9566e-01, -7.9134e-02,  ...,  4.0537e-01,\n",
       "             1.6805e-01,  3.6210e-02],\n",
       "           [ 7.8556e-02,  1.3391e-01, -4.9111e-02,  ...,  1.5325e-01,\n",
       "             1.0047e-01,  4.6589e-02],\n",
       "           [-3.8423e-02,  2.6143e-01, -1.9107e-02,  ...,  3.5896e-01,\n",
       "             1.7704e-01,  3.7264e-02]],\n",
       " \n",
       "          [[-4.6450e-03, -8.1833e-03,  5.7001e-03,  ..., -4.3283e-03,\n",
       "             5.8220e-03,  7.3407e-03],\n",
       "           [ 3.3208e-01, -1.6008e-01, -1.5417e-01,  ..., -5.4281e-01,\n",
       "             1.7861e-01,  3.1681e-02],\n",
       "           [ 2.4203e-01, -1.3825e-02, -3.7209e-01,  ...,  3.2624e-02,\n",
       "            -1.3619e-01,  2.5032e-01],\n",
       "           ...,\n",
       "           [-6.1800e-02, -2.9250e-02, -3.8873e-02,  ...,  1.0147e-01,\n",
       "            -2.0146e-01, -2.8057e-01],\n",
       "           [-6.9468e-02, -9.3410e-02,  3.5989e-02,  ...,  5.2115e-02,\n",
       "            -2.0676e-01, -2.3059e-01],\n",
       "           [-1.4128e-01,  3.2432e-02, -3.7151e-03,  ...,  9.7217e-03,\n",
       "            -1.8000e-01, -2.8405e-01]],\n",
       " \n",
       "          [[ 1.3702e-03,  4.2314e-03, -3.2091e-04,  ..., -3.4631e-03,\n",
       "            -5.8946e-03,  1.8627e-02],\n",
       "           [ 2.7290e-01, -9.5506e-03,  2.9770e-01,  ...,  2.9955e-01,\n",
       "             7.0549e-03, -1.5575e-01],\n",
       "           [ 1.1095e-01,  1.2242e-01,  2.9530e-01,  ..., -7.4928e-02,\n",
       "            -4.9823e-02, -6.4659e-01],\n",
       "           ...,\n",
       "           [-2.5084e-01,  2.8868e-01, -1.0483e-01,  ...,  5.0894e-01,\n",
       "             2.6561e-01, -3.9867e-01],\n",
       "           [-1.8378e-01,  1.5201e-01, -1.2274e-01,  ...,  4.2800e-01,\n",
       "             2.7275e-01, -3.2509e-01],\n",
       "           [-2.3463e-01,  2.1158e-01, -6.5029e-02,  ...,  4.2300e-01,\n",
       "             1.7613e-01, -3.2777e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 9.3471e-04, -1.9779e-03, -2.7518e-05,  ...,  4.5042e-03,\n",
       "             2.7590e-03, -4.0592e-03],\n",
       "           [ 3.1350e-01, -9.8757e-02, -2.6470e-01,  ...,  1.6641e-01,\n",
       "             4.8762e-01,  3.0833e-01],\n",
       "           [ 1.7181e-01,  6.3399e-01, -1.1475e-01,  ..., -3.7197e-01,\n",
       "             1.2946e-01,  3.5704e-01],\n",
       "           ...,\n",
       "           [-5.2995e-02,  1.7165e-01, -1.9027e-02,  ..., -1.1632e-01,\n",
       "             1.2254e-01,  1.1806e-02],\n",
       "           [-1.0851e-01,  1.4699e-01, -8.0106e-02,  ..., -6.2515e-02,\n",
       "             1.4390e-01, -8.0106e-02],\n",
       "           [-8.1461e-02,  1.9302e-01, -3.8566e-02,  ..., -2.3106e-02,\n",
       "             1.7942e-01, -5.5063e-02]],\n",
       " \n",
       "          [[-9.5062e-04,  2.1816e-03, -6.9847e-03,  ..., -2.1315e-02,\n",
       "            -3.0133e-04, -1.9834e-03],\n",
       "           [ 9.2097e-02, -1.7148e-01, -3.4102e-01,  ...,  1.8025e-01,\n",
       "            -4.5292e-02, -2.8273e-01],\n",
       "           [ 1.0426e-01, -1.3485e-01, -4.3141e-01,  ...,  8.7782e-02,\n",
       "             1.4182e-01, -2.4163e-01],\n",
       "           ...,\n",
       "           [-2.5194e-02, -1.2574e-02,  1.9407e-01,  ...,  1.7615e-01,\n",
       "             4.5208e-02, -1.6234e-01],\n",
       "           [-9.5286e-02, -1.4997e-02,  1.6043e-01,  ...,  1.4569e-01,\n",
       "             8.2441e-02, -4.1975e-02],\n",
       "           [-7.7311e-02,  3.6984e-02,  1.8703e-01,  ...,  1.7329e-01,\n",
       "             1.6038e-01, -1.8527e-01]],\n",
       " \n",
       "          [[ 7.1062e-03, -1.9440e-02, -3.4475e-03,  ...,  5.2779e-03,\n",
       "            -5.9189e-03, -4.1835e-03],\n",
       "           [ 1.7579e-01,  1.3260e-01,  3.4448e-01,  ...,  1.0811e-01,\n",
       "            -4.7558e-02, -2.1258e-01],\n",
       "           [-3.6251e-02,  2.2240e-01, -3.4914e-01,  ...,  1.4533e-01,\n",
       "             3.0888e-01, -2.8640e-01],\n",
       "           ...,\n",
       "           [-1.2919e-01, -1.0175e-01,  6.0291e-03,  ..., -1.1141e-02,\n",
       "             2.2459e-01, -1.1154e-02],\n",
       "           [-7.8940e-02, -7.6574e-02, -2.7617e-02,  ..., -8.6354e-02,\n",
       "             1.2049e-01, -1.0098e-03],\n",
       "           [-1.7241e-01,  3.2296e-02, -1.5075e-02,  ..., -9.4018e-02,\n",
       "             7.7757e-02, -1.7235e-02]]]]),\n",
       " tensor([[[[-3.9295e-03, -2.8766e-02, -3.7122e-03,  ...,  1.0005e-02,\n",
       "             1.2177e-02, -6.6925e-03],\n",
       "           [-2.4367e-02,  5.8400e-01,  2.2768e-01,  ..., -1.6566e-01,\n",
       "            -3.2637e-01,  1.3115e+00],\n",
       "           [-1.9001e-01,  8.5163e-01,  1.0137e-01,  ...,  1.4127e-01,\n",
       "            -4.3318e-01,  2.2273e-01],\n",
       "           ...,\n",
       "           [-2.7361e-01,  1.0839e+00,  7.0431e-01,  ..., -6.5778e-01,\n",
       "             8.9585e-03,  4.4075e-01],\n",
       "           [-3.5164e-01,  9.3116e-01,  7.3155e-01,  ..., -6.3093e-01,\n",
       "             5.2343e-03,  4.8607e-01],\n",
       "           [-2.2588e-01,  9.8598e-01,  8.2435e-01,  ..., -6.7478e-01,\n",
       "             1.2800e-02,  4.8377e-01]],\n",
       " \n",
       "          [[-3.7566e-03,  1.3950e-02,  2.5423e-03,  ..., -1.1378e-02,\n",
       "            -1.3801e-02, -1.4328e-03],\n",
       "           [-5.3473e-01,  3.7352e-01,  2.6463e-01,  ..., -2.7129e-01,\n",
       "            -8.8471e-02,  4.3183e-01],\n",
       "           [-6.8700e-02,  7.6546e-01, -1.4223e-01,  ...,  4.4203e-01,\n",
       "            -8.5788e-02,  4.4267e-01],\n",
       "           ...,\n",
       "           [-3.1944e-01, -9.6576e-02,  3.3692e-02,  ..., -2.9978e-01,\n",
       "             4.5387e-01, -9.6298e-02],\n",
       "           [-3.5978e-01, -9.8788e-02,  1.2784e-01,  ..., -4.1435e-01,\n",
       "             1.9601e-01, -1.8462e-01],\n",
       "           [-3.7423e-01, -1.3578e-01, -2.2442e-02,  ..., -3.4712e-01,\n",
       "             4.1086e-01, -1.0997e-01]],\n",
       " \n",
       "          [[ 2.3858e-03, -1.0193e-04, -1.4251e-02,  ...,  2.5152e-03,\n",
       "             1.9018e-02, -6.8937e-03],\n",
       "           [-5.8154e-02, -2.0516e-01, -6.4949e-02,  ..., -4.9652e-02,\n",
       "             2.1449e-01, -1.0823e-01],\n",
       "           [-2.1458e-01,  5.2511e-02,  8.1214e-02,  ...,  1.1185e-01,\n",
       "             9.8697e-02, -2.1135e-01],\n",
       "           ...,\n",
       "           [ 1.5574e-01, -2.8888e-03,  6.7050e-02,  ..., -2.7712e-01,\n",
       "            -5.7396e-02,  5.7750e-03],\n",
       "           [ 9.6688e-02, -2.9904e-02,  5.7531e-03,  ..., -2.2868e-01,\n",
       "            -2.3173e-01,  1.1003e-02],\n",
       "           [ 1.8538e-01,  4.5209e-02,  1.6398e-01,  ..., -2.6244e-01,\n",
       "            -1.5581e-01,  5.3834e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0446e-02, -2.1281e-03,  8.5213e-03,  ..., -4.6182e-03,\n",
       "            -8.3941e-03,  3.7427e-03],\n",
       "           [-6.4202e-03,  2.7011e-01, -6.8174e-01,  ...,  1.6608e-01,\n",
       "             5.2928e-01,  4.0588e-01],\n",
       "           [-1.1785e-01,  3.2297e-02, -4.1853e-01,  ..., -9.4867e-02,\n",
       "             1.0839e-01, -1.5241e-01],\n",
       "           ...,\n",
       "           [ 1.7363e-01,  5.1388e-02,  1.0254e-01,  ..., -2.0295e-01,\n",
       "             8.0905e-02,  7.8717e-02],\n",
       "           [ 1.9480e-01,  1.0415e-01,  4.1651e-02,  ..., -1.6696e-01,\n",
       "             6.6786e-02,  1.5139e-01],\n",
       "           [ 1.4745e-01, -2.8588e-02,  1.0468e-01,  ..., -1.6093e-01,\n",
       "             5.2175e-02,  1.1749e-01]],\n",
       " \n",
       "          [[-6.7502e-04,  1.7246e-03, -4.4187e-03,  ..., -2.1114e-03,\n",
       "            -5.2877e-04,  8.3484e-03],\n",
       "           [-1.0775e-01,  1.6968e-01,  3.6805e-01,  ..., -2.2811e-01,\n",
       "            -1.3842e-01,  2.9018e-01],\n",
       "           [ 1.8799e-01, -1.4084e-01,  3.5354e-01,  ...,  4.1203e-02,\n",
       "            -2.5733e-01,  5.1980e-02],\n",
       "           ...,\n",
       "           [ 3.1491e-02, -2.0308e-01, -3.1105e-02,  ...,  1.7928e-01,\n",
       "             1.7906e-01, -4.5014e-02],\n",
       "           [ 1.0714e-01, -1.0634e-01, -3.1040e-02,  ...,  1.0428e-01,\n",
       "             6.9623e-02, -2.2253e-02],\n",
       "           [ 1.9652e-02, -1.4853e-01,  2.2239e-03,  ...,  1.3089e-01,\n",
       "             1.4963e-01, -4.3030e-02]],\n",
       " \n",
       "          [[-4.9371e-03, -1.0654e-02,  1.0030e-02,  ...,  3.5218e-03,\n",
       "             9.9725e-05,  2.3159e-03],\n",
       "           [ 1.2546e-01,  4.6393e-01, -3.5580e-01,  ...,  2.7768e-01,\n",
       "             2.7162e-02, -3.3078e-01],\n",
       "           [ 3.8449e-01,  2.2896e-01, -1.7078e-01,  ...,  5.4487e-01,\n",
       "            -3.6922e-01, -4.1294e-01],\n",
       "           ...,\n",
       "           [-1.8669e-01, -5.7839e-02, -3.9200e-01,  ...,  1.0395e-01,\n",
       "             1.1352e-01,  3.3161e-02],\n",
       "           [-2.1107e-01, -6.8872e-02, -2.9840e-01,  ...,  8.2206e-02,\n",
       "             1.0665e-01, -1.8252e-02],\n",
       "           [-1.4520e-01, -1.2055e-01, -3.1896e-01,  ...,  1.1712e-01,\n",
       "             1.3112e-01,  2.2173e-02]]]]),\n",
       " tensor([[[[-5.9521e-03, -2.3344e-02, -3.8388e-03,  ...,  1.9411e-03,\n",
       "            -3.1233e-03, -7.4433e-03],\n",
       "           [-3.3449e-01,  2.6432e-01, -3.4723e-01,  ..., -4.4190e-01,\n",
       "             4.8221e-02,  5.1055e-02],\n",
       "           [-2.5891e-01,  1.0314e-01,  3.4768e-02,  ...,  3.0560e-01,\n",
       "            -4.7454e-01, -2.9038e-01],\n",
       "           ...,\n",
       "           [ 3.6657e-02, -1.4204e-01,  2.1549e-01,  ...,  7.2907e-01,\n",
       "             3.3593e-02, -3.9132e-02],\n",
       "           [-2.1854e-02, -2.0696e-01,  2.3303e-01,  ...,  6.5834e-01,\n",
       "            -8.3796e-02, -6.7850e-02],\n",
       "           [-1.1077e-02, -1.4991e-01,  2.3907e-01,  ...,  6.4158e-01,\n",
       "             2.2700e-03, -1.2250e-01]],\n",
       " \n",
       "          [[ 1.2121e-02,  4.2231e-04,  5.4500e-03,  ...,  1.2282e-02,\n",
       "             7.1089e-03, -1.4347e-03],\n",
       "           [-2.0701e-02,  4.9598e-02,  1.7673e-01,  ..., -7.3779e-02,\n",
       "            -8.4879e-02,  2.6004e-02],\n",
       "           [ 1.1458e-01, -4.0998e-01,  3.9705e-01,  ...,  1.8554e-01,\n",
       "            -4.7600e-01,  5.3958e-02],\n",
       "           ...,\n",
       "           [-1.5386e-02, -2.8741e-01, -8.6171e-02,  ..., -2.0480e-01,\n",
       "            -1.3033e-01,  2.6382e-01],\n",
       "           [-3.6803e-02, -2.2567e-01, -6.0920e-02,  ..., -1.2269e-01,\n",
       "            -3.7497e-02,  1.7296e-01],\n",
       "           [-5.2584e-02, -2.4967e-01, -5.5113e-02,  ..., -2.0115e-01,\n",
       "            -9.3068e-02,  2.6652e-01]],\n",
       " \n",
       "          [[-6.2248e-03, -2.1948e-02,  6.8697e-03,  ...,  1.0017e-01,\n",
       "             2.5410e-03, -6.2338e-03],\n",
       "           [ 3.3259e-01,  1.5481e-01, -3.3713e-02,  ..., -3.7700e-01,\n",
       "             3.8002e-01, -1.2439e-01],\n",
       "           [ 4.7112e-01,  6.5102e-01,  5.6404e-01,  ..., -2.6512e-01,\n",
       "             4.2418e-01, -1.7512e-01],\n",
       "           ...,\n",
       "           [ 4.0215e-01, -4.3318e-01,  6.0663e-01,  ..., -1.9054e-01,\n",
       "             1.6575e-01,  3.5752e-01],\n",
       "           [ 3.4148e-01, -4.0393e-01,  5.4524e-01,  ..., -1.9446e-01,\n",
       "             1.2790e-01,  3.3415e-01],\n",
       "           [ 3.6159e-01, -3.9564e-01,  6.1297e-01,  ..., -2.3071e-01,\n",
       "             2.2084e-01,  4.0373e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.8635e-04, -9.3673e-03, -1.0110e-03,  ..., -1.0382e-03,\n",
       "             2.4980e-03,  1.7132e-03],\n",
       "           [-1.7332e-01,  2.1704e-01, -2.2928e-01,  ...,  1.8539e-01,\n",
       "             7.6149e-02, -2.5755e-01],\n",
       "           [-2.7386e-01, -4.5038e-02,  4.9741e-02,  ...,  1.3852e-01,\n",
       "             2.6825e-02, -1.3920e-02],\n",
       "           ...,\n",
       "           [ 1.4124e-01, -1.4114e-01,  2.6566e-01,  ...,  3.6451e-01,\n",
       "            -1.8885e-01,  3.6930e-02],\n",
       "           [ 8.6114e-02, -1.6247e-01,  2.7609e-01,  ...,  3.9223e-01,\n",
       "            -2.2757e-01,  8.4255e-02],\n",
       "           [ 1.5215e-01, -1.6154e-01,  2.8200e-01,  ...,  4.0726e-01,\n",
       "            -1.5934e-01,  6.0490e-02]],\n",
       " \n",
       "          [[-1.1617e-02,  7.3138e-03,  8.7272e-06,  ..., -9.9051e-03,\n",
       "            -1.1054e-02,  1.6142e-03],\n",
       "           [ 3.2129e-01, -3.8083e-01,  2.8454e-01,  ...,  1.6975e-02,\n",
       "             4.7744e-01, -1.9375e-02],\n",
       "           [ 1.6292e-02, -2.8924e-01,  3.5415e-01,  ...,  5.0982e-01,\n",
       "            -3.1916e-02,  2.5770e-02],\n",
       "           ...,\n",
       "           [-4.7058e-02,  1.4248e-02, -1.6546e-01,  ...,  7.5238e-02,\n",
       "             2.8510e-03,  2.1149e-01],\n",
       "           [-2.3909e-02,  2.6689e-02, -1.9033e-01,  ..., -4.8114e-02,\n",
       "            -7.7795e-02,  2.5207e-01],\n",
       "           [-3.7456e-02,  1.3554e-02, -1.3712e-01,  ..., -1.2240e-02,\n",
       "            -3.5906e-02,  2.3198e-01]],\n",
       " \n",
       "          [[ 4.3985e-03, -6.3655e-04, -6.1307e-03,  ...,  2.3045e-03,\n",
       "            -7.2673e-03,  7.2229e-04],\n",
       "           [-3.0182e-01, -3.2081e-01, -3.7736e-01,  ..., -6.1328e-01,\n",
       "            -3.1787e-01,  3.6063e-02],\n",
       "           [-6.6096e-02, -2.8907e-01,  4.5262e-01,  ...,  1.9075e-02,\n",
       "            -1.1168e-01, -6.2257e-02],\n",
       "           ...,\n",
       "           [-3.4736e-01, -1.0223e-01,  7.0802e-02,  ...,  1.2920e-02,\n",
       "             1.0706e-02,  2.7255e-01],\n",
       "           [-2.6196e-01, -8.5477e-02,  5.2238e-02,  ..., -3.6837e-02,\n",
       "            -1.3339e-02,  2.9238e-01],\n",
       "           [-4.0275e-01, -9.7093e-02,  1.1587e-01,  ..., -7.7674e-02,\n",
       "             3.4282e-02,  2.9309e-01]]]]),\n",
       " tensor([[[[-5.9000e-02,  3.0525e-03,  3.4827e-03,  ...,  7.9131e-03,\n",
       "            -3.1762e-03,  3.7706e-02],\n",
       "           [ 1.7773e-01, -4.0437e-01,  1.6505e-01,  ...,  3.6843e-01,\n",
       "             7.1776e-01, -5.0493e-01],\n",
       "           [ 1.1700e-01, -4.6265e-01,  2.1154e-01,  ...,  3.2395e-01,\n",
       "             3.3272e-02, -1.1535e-01],\n",
       "           ...,\n",
       "           [ 2.7691e-01, -9.4837e-02,  1.3295e-01,  ..., -1.1950e-02,\n",
       "            -1.0341e-01, -6.5442e-02],\n",
       "           [ 1.4505e-01, -1.4228e-01,  1.8591e-01,  ...,  4.0901e-02,\n",
       "            -1.2384e-01,  2.2208e-03],\n",
       "           [ 2.1959e-01, -9.5532e-02,  1.5845e-01,  ...,  1.3018e-02,\n",
       "            -1.0237e-01, -5.2533e-02]],\n",
       " \n",
       "          [[-4.7116e-03, -1.9058e-02, -8.5030e-03,  ..., -2.3654e-02,\n",
       "            -1.7502e-03, -8.3319e-03],\n",
       "           [ 1.3110e-01,  3.2795e-01,  4.5152e-01,  ...,  4.8378e-01,\n",
       "            -5.5628e-01,  3.0580e-01],\n",
       "           [-2.0880e-01,  2.2829e-01,  4.5016e-01,  ...,  1.4527e-01,\n",
       "            -3.4122e-01,  3.8967e-01],\n",
       "           ...,\n",
       "           [ 1.7001e-01, -4.2370e-01,  3.1058e-02,  ...,  6.4685e-01,\n",
       "             1.0679e-04, -4.9130e-01],\n",
       "           [ 2.8829e-01, -4.4227e-01,  1.2552e-02,  ...,  7.3468e-01,\n",
       "             5.8461e-02, -5.3596e-01],\n",
       "           [ 1.7350e-01, -3.9455e-01, -2.3479e-02,  ...,  7.1934e-01,\n",
       "            -3.8550e-02, -5.6722e-01]],\n",
       " \n",
       "          [[ 5.8284e-03,  8.7511e-03,  3.0000e-03,  ...,  1.4193e-02,\n",
       "            -1.7155e-03, -4.6489e-03],\n",
       "           [-5.5893e-01,  9.6725e-02, -3.8740e-01,  ..., -1.7542e-01,\n",
       "             6.5070e-01,  6.4808e-02],\n",
       "           [-9.5215e-01,  1.7708e-01, -3.3694e-01,  ...,  4.6187e-01,\n",
       "             5.6436e-01, -3.1800e-01],\n",
       "           ...,\n",
       "           [-5.6140e-01, -6.6141e-02,  2.6447e-01,  ...,  1.3199e-01,\n",
       "            -1.8102e-01, -4.2701e-02],\n",
       "           [-4.7454e-01,  1.7196e-03,  3.3367e-01,  ...,  1.2549e-01,\n",
       "            -1.1603e-01,  2.8292e-02],\n",
       "           [-5.7868e-01, -5.5549e-02,  2.9683e-01,  ...,  1.1341e-01,\n",
       "            -1.7794e-01, -4.9623e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.2011e-04, -1.0395e-03, -2.2068e-03,  ...,  5.4762e-03,\n",
       "             9.8882e-03, -2.5149e-03],\n",
       "           [-8.6389e-02,  3.0547e-01, -3.1560e-01,  ..., -6.7610e-01,\n",
       "             3.8269e-01,  5.2850e-01],\n",
       "           [-5.4261e-01,  2.7933e-01,  2.4132e-01,  ..., -7.0368e-01,\n",
       "             9.5117e-02,  9.9369e-02],\n",
       "           ...,\n",
       "           [ 2.1565e-01,  1.0198e-01,  1.4654e-01,  ...,  6.3608e-02,\n",
       "            -4.9457e-01, -1.2613e-01],\n",
       "           [ 2.3979e-01,  9.7307e-02,  6.2732e-02,  ...,  1.0412e-01,\n",
       "            -4.0104e-01, -9.8566e-02],\n",
       "           [ 2.6167e-01,  8.5569e-02,  1.2675e-01,  ...,  8.3300e-02,\n",
       "            -5.1105e-01, -1.0562e-01]],\n",
       " \n",
       "          [[-2.9864e-05,  1.1846e-03,  4.2336e-03,  ...,  1.8845e-03,\n",
       "             2.1310e-03, -2.6615e-03],\n",
       "           [ 1.0218e+00,  3.7739e-01,  3.0767e-01,  ...,  2.0187e-02,\n",
       "            -7.2866e-02, -6.4185e-03],\n",
       "           [ 5.6361e-01,  1.5848e-01,  6.6426e-01,  ...,  4.3074e-01,\n",
       "             2.9780e-01,  2.4790e-01],\n",
       "           ...,\n",
       "           [ 1.6993e-02,  3.9509e-02, -2.0633e-03,  ..., -2.4129e-02,\n",
       "            -1.8107e-01, -1.8574e-01],\n",
       "           [-6.4727e-02,  7.2259e-02, -6.8043e-02,  ..., -6.4512e-02,\n",
       "            -2.6865e-01, -2.9124e-01],\n",
       "           [-4.1622e-02,  3.7183e-02, -2.2759e-02,  ...,  2.0296e-02,\n",
       "            -1.5872e-01, -1.8670e-01]],\n",
       " \n",
       "          [[-4.5032e-03, -1.6999e-03, -8.3609e-03,  ..., -7.3670e-03,\n",
       "             4.6104e-03, -2.9029e-03],\n",
       "           [ 9.4666e-02,  4.0705e-01,  4.2607e-02,  ..., -3.0232e-02,\n",
       "             6.6680e-01,  3.5282e-01],\n",
       "           [ 2.8048e-01,  2.7200e-02, -3.4594e-01,  ...,  4.5738e-01,\n",
       "            -5.4880e-02, -5.3554e-02],\n",
       "           ...,\n",
       "           [ 1.4497e-01,  1.5184e-01, -2.3682e-01,  ...,  2.0023e-01,\n",
       "            -6.2754e-02, -6.0853e-02],\n",
       "           [ 1.2198e-01,  1.2839e-01, -2.4745e-01,  ...,  1.6273e-01,\n",
       "            -9.6600e-02, -1.5474e-01],\n",
       "           [ 1.6907e-01,  1.8106e-01, -2.9360e-01,  ...,  1.3276e-01,\n",
       "            -9.7862e-02, -8.2822e-02]]]]),\n",
       " tensor([[[[ 0.0093,  0.0008,  0.0039,  ...,  0.0066,  0.0032,  0.0062],\n",
       "           [-0.5010, -0.0386, -0.2072,  ..., -0.4820, -0.1122,  0.0099],\n",
       "           [-0.5733,  0.6154,  0.3166,  ..., -0.3983,  0.2492,  0.0985],\n",
       "           ...,\n",
       "           [-0.2285, -0.2058,  0.2255,  ..., -0.1628,  0.1161,  0.3351],\n",
       "           [-0.1517, -0.2530,  0.2715,  ..., -0.1334,  0.0694,  0.2322],\n",
       "           [-0.1181, -0.2096,  0.2544,  ..., -0.1838,  0.0784,  0.3793]],\n",
       " \n",
       "          [[-0.0220,  0.0286,  0.0113,  ..., -0.1327,  0.0009,  0.0055],\n",
       "           [ 0.4679,  0.2465, -0.2776,  ..., -0.4679,  0.1529, -0.0425],\n",
       "           [ 0.1072,  0.2501, -0.0974,  ..., -0.6160, -0.1809,  0.0423],\n",
       "           ...,\n",
       "           [-0.0521,  0.0029, -0.0569,  ..., -0.1079,  0.3277,  0.0783],\n",
       "           [-0.0669,  0.0178, -0.0311,  ..., -0.1337,  0.3519,  0.0995],\n",
       "           [-0.0523,  0.0099, -0.0452,  ..., -0.0789,  0.3373,  0.0629]],\n",
       " \n",
       "          [[-0.0095, -0.0082, -0.0036,  ...,  0.0134,  0.0151, -0.0142],\n",
       "           [ 0.1171,  0.6924, -0.1247,  ..., -0.3119, -0.4216,  0.2400],\n",
       "           [ 0.6360,  0.3561, -0.1002,  ...,  0.1305, -0.1559,  0.4250],\n",
       "           ...,\n",
       "           [ 0.1008, -0.3224,  0.2223,  ...,  0.3728, -0.1625,  0.0433],\n",
       "           [ 0.0519, -0.2501,  0.1870,  ...,  0.3754, -0.1485,  0.0498],\n",
       "           [ 0.0708, -0.2614,  0.2350,  ...,  0.3531, -0.1894,  0.0904]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0053, -0.0328,  0.0228,  ...,  0.0250, -0.0081, -0.0087],\n",
       "           [-0.2804,  0.5998,  0.6056,  ..., -0.1524, -0.2018, -0.3073],\n",
       "           [ 0.0809,  0.3277,  0.7174,  ..., -0.1555,  0.3596, -0.2142],\n",
       "           ...,\n",
       "           [ 0.0327,  0.2202, -0.1841,  ..., -0.0302, -0.2877,  0.0591],\n",
       "           [ 0.0365,  0.1976, -0.2142,  ..., -0.0136, -0.2747,  0.0966],\n",
       "           [ 0.0615,  0.2370, -0.2014,  ..., -0.0302, -0.3117,  0.0621]],\n",
       " \n",
       "          [[ 0.0021, -0.0038,  0.0110,  ...,  0.0269, -0.0168, -0.0068],\n",
       "           [-0.2032, -0.1805, -0.1351,  ..., -0.1631, -0.5879, -0.4100],\n",
       "           [-0.3680, -0.1618, -0.0938,  ...,  0.3207, -0.4273, -0.2673],\n",
       "           ...,\n",
       "           [-0.0283, -0.1612,  0.4987,  ..., -0.0744,  0.2042,  0.1594],\n",
       "           [ 0.0060, -0.1715,  0.4402,  ..., -0.0409,  0.2327,  0.1816],\n",
       "           [-0.0150, -0.1894,  0.5028,  ..., -0.0648,  0.2090,  0.1598]],\n",
       " \n",
       "          [[ 0.0047,  0.0051,  0.0022,  ...,  0.0037, -0.0108,  0.0080],\n",
       "           [ 0.2682, -0.4581,  0.2985,  ..., -0.3369,  0.4332,  0.3638],\n",
       "           [ 0.0874, -0.3707, -0.0794,  ...,  0.1975,  0.3627,  0.3757],\n",
       "           ...,\n",
       "           [-0.1016,  0.2761, -0.1094,  ..., -0.3167,  0.1732,  0.0127],\n",
       "           [-0.1161,  0.1899, -0.1890,  ..., -0.2242,  0.1548, -0.0193],\n",
       "           [-0.1037,  0.2162, -0.0792,  ..., -0.2648,  0.1421,  0.0298]]]]),\n",
       " tensor([[[[ 1.8866e-03, -7.6369e-03,  2.2188e-03,  ...,  2.4239e-03,\n",
       "            -4.2447e-03, -8.4031e-03],\n",
       "           [ 5.8959e-02, -6.7205e-01, -2.0700e-01,  ...,  5.4810e-01,\n",
       "            -3.2985e-02,  1.1515e-01],\n",
       "           [-1.9077e-01, -4.2165e-01, -3.6717e-02,  ..., -4.3285e-01,\n",
       "            -2.2020e-03,  4.0310e-02],\n",
       "           ...,\n",
       "           [-2.5956e-02, -3.7961e-01, -8.1013e-03,  ...,  1.5508e-02,\n",
       "            -1.8161e-03,  4.6044e-01],\n",
       "           [ 7.1875e-02, -3.9592e-01,  2.4734e-02,  ...,  6.8146e-02,\n",
       "            -4.5279e-02,  4.7660e-01],\n",
       "           [-4.7011e-02, -4.2103e-01,  5.0813e-02,  ...,  9.3684e-02,\n",
       "             1.1466e-02,  4.6133e-01]],\n",
       " \n",
       "          [[ 1.0490e-02,  5.6647e-02,  1.1990e-02,  ...,  3.6960e-02,\n",
       "            -4.1366e-02, -1.7282e-02],\n",
       "           [ 2.8724e-02, -4.8026e-01, -1.1799e-01,  ..., -4.2748e-01,\n",
       "             1.7761e-01, -4.8658e-02],\n",
       "           [ 4.2099e-01, -4.8540e-01, -7.6377e-02,  ..., -4.0879e-01,\n",
       "             1.3738e-01,  2.4331e-02],\n",
       "           ...,\n",
       "           [-2.2885e-01, -6.2634e-01,  1.7341e-01,  ...,  1.1509e-01,\n",
       "             7.2439e-01,  2.3258e-02],\n",
       "           [-3.0866e-01, -6.5599e-01,  1.5060e-01,  ...,  5.8089e-02,\n",
       "             7.8385e-01,  1.2183e-01],\n",
       "           [-1.8763e-01, -6.0398e-01,  1.4015e-01,  ...,  1.0302e-01,\n",
       "             7.2064e-01,  4.6598e-02]],\n",
       " \n",
       "          [[-6.7258e-03,  2.0820e-02, -1.8278e-02,  ...,  2.9553e-03,\n",
       "             1.0108e-02,  4.2914e-03],\n",
       "           [-7.2161e-02,  8.0606e-02,  4.2563e-01,  ..., -4.4482e-01,\n",
       "            -1.4534e-02,  1.2109e-01],\n",
       "           [ 7.4088e-02,  3.6595e-01,  6.3771e-01,  ..., -2.4662e-01,\n",
       "             2.0352e-01, -4.5849e-02],\n",
       "           ...,\n",
       "           [-2.5674e-03, -1.9131e-01, -1.7836e-01,  ..., -1.4429e-01,\n",
       "             5.8419e-02,  1.4412e-01],\n",
       "           [ 1.5126e-02, -1.5160e-01, -1.7444e-01,  ..., -1.7240e-01,\n",
       "             7.5283e-02,  1.4753e-01],\n",
       "           [ 2.1067e-03, -1.9225e-01, -1.7314e-01,  ..., -1.4790e-01,\n",
       "             7.7275e-02,  1.0599e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.2464e-03,  1.0111e-02,  9.5372e-03,  ...,  9.6442e-03,\n",
       "            -5.2884e-03,  5.0167e-03],\n",
       "           [ 4.2839e-01,  7.5228e-02, -7.8110e-02,  ...,  5.6212e-01,\n",
       "            -1.8307e-01,  8.8694e-02],\n",
       "           [ 2.8879e-01, -6.4512e-01,  2.4964e-01,  ...,  3.7910e-01,\n",
       "            -8.8661e-02, -2.0186e-01],\n",
       "           ...,\n",
       "           [-9.6186e-02, -6.3002e-02,  1.1539e-01,  ...,  2.2765e-01,\n",
       "            -1.8515e-01,  1.9669e-01],\n",
       "           [-8.0047e-02,  5.0992e-02,  1.1383e-01,  ...,  1.7192e-01,\n",
       "            -1.4205e-01,  2.0662e-02],\n",
       "           [-1.1258e-01, -4.0287e-02,  1.1722e-01,  ...,  2.3428e-01,\n",
       "            -1.8559e-01,  6.0854e-02]],\n",
       " \n",
       "          [[-5.7637e-03,  6.2530e-04,  7.2028e-03,  ..., -1.0003e-02,\n",
       "            -1.6340e-02,  1.3802e-03],\n",
       "           [ 8.1111e-02, -3.7621e-01,  5.7239e-02,  ...,  1.7728e-01,\n",
       "             1.1372e-01,  1.7399e-01],\n",
       "           [ 2.2337e-01,  3.6417e-02,  9.9408e-02,  ..., -2.8565e-02,\n",
       "            -4.3110e-01,  3.2941e-02],\n",
       "           ...,\n",
       "           [ 7.5551e-02,  1.2675e-01,  8.6510e-02,  ..., -1.5344e-01,\n",
       "            -4.3773e-02,  1.0012e-01],\n",
       "           [ 5.8359e-02,  1.3164e-01,  5.8298e-02,  ..., -1.2147e-01,\n",
       "            -7.2584e-02,  7.3730e-02],\n",
       "           [ 5.1508e-02,  1.1821e-01,  9.0016e-02,  ..., -1.4586e-01,\n",
       "            -3.1334e-02,  9.0908e-02]],\n",
       " \n",
       "          [[ 2.5155e-03, -7.4711e-03, -2.2856e-02,  ..., -2.1293e-03,\n",
       "             1.8952e-04,  3.5861e-03],\n",
       "           [ 6.5772e-02, -9.6007e-02, -6.9414e-02,  ...,  1.3538e-01,\n",
       "             2.9886e-01,  2.9703e-01],\n",
       "           [ 2.5695e-01, -7.0405e-02,  2.7842e-01,  ...,  1.1907e-01,\n",
       "             3.5142e-01, -6.5843e-02],\n",
       "           ...,\n",
       "           [-1.7066e-01, -1.1915e-01, -1.3444e-02,  ..., -1.0002e-01,\n",
       "            -2.9012e-01, -2.3722e-01],\n",
       "           [-1.2643e-01, -1.3680e-01, -2.9089e-02,  ..., -1.4054e-01,\n",
       "            -2.0666e-01, -1.9202e-01],\n",
       "           [-1.5279e-01, -1.2668e-01,  4.5724e-03,  ..., -1.3559e-01,\n",
       "            -2.7450e-01, -2.1316e-01]]]]),\n",
       " tensor([[[[ 1.2535e-02,  2.2904e-02, -2.1595e-03,  ...,  4.2686e-04,\n",
       "             2.6278e-02, -1.8676e-03],\n",
       "           [-2.0442e-01, -1.1014e-01,  2.3196e-01,  ..., -4.1625e-01,\n",
       "            -4.5119e-01,  6.6305e-01],\n",
       "           [-4.6794e-02,  1.5666e-01, -1.2082e-01,  ...,  1.9576e-01,\n",
       "             1.1716e-01, -1.1903e-01],\n",
       "           ...,\n",
       "           [-5.2639e-02,  1.2483e-01,  6.7189e-01,  ...,  4.4434e-02,\n",
       "            -8.3791e-01, -2.8410e-01],\n",
       "           [-2.7064e-02,  9.8175e-02,  6.8164e-01,  ..., -3.9045e-02,\n",
       "            -7.6409e-01, -2.5690e-01],\n",
       "           [-3.5972e-02,  1.0549e-01,  7.1724e-01,  ...,  7.5703e-02,\n",
       "            -7.9141e-01, -2.8978e-01]],\n",
       " \n",
       "          [[-9.9649e-02, -8.7521e-02,  6.4061e-02,  ...,  1.6741e-02,\n",
       "             2.6095e-02, -7.5219e-02],\n",
       "           [-6.9092e-02, -1.4324e-01, -2.7526e-01,  ..., -2.2969e-01,\n",
       "             2.1441e-01, -2.5634e-01],\n",
       "           [-1.6164e-01,  1.8684e-01, -2.6598e-01,  ..., -1.0242e-01,\n",
       "             1.9645e-03,  2.5181e-01],\n",
       "           ...,\n",
       "           [ 4.2005e-01,  7.4566e-01,  1.8214e-01,  ..., -4.0405e-01,\n",
       "             3.5090e-01, -4.6515e-01],\n",
       "           [ 3.4075e-01,  6.0613e-01,  1.8581e-01,  ..., -3.3214e-01,\n",
       "             3.6041e-01, -5.1746e-01],\n",
       "           [ 3.7110e-01,  7.0174e-01,  2.0689e-01,  ..., -3.9129e-01,\n",
       "             4.4185e-01, -4.9188e-01]],\n",
       " \n",
       "          [[ 6.9483e-02,  5.1411e-02, -5.0395e-02,  ...,  3.3345e-02,\n",
       "             2.6729e-02,  2.8594e-02],\n",
       "           [-3.6887e-01, -7.7977e-01,  6.8618e-01,  ...,  2.6565e-01,\n",
       "             1.2421e-01,  2.6009e-01],\n",
       "           [-6.3275e-02, -4.7334e-01,  1.9421e-01,  ...,  6.6145e-02,\n",
       "             1.5464e-02, -4.0406e-01],\n",
       "           ...,\n",
       "           [-4.5207e-02, -1.1631e-01, -5.0077e-02,  ..., -1.3709e-02,\n",
       "            -4.2267e-01,  3.2210e-01],\n",
       "           [ 1.4756e-02, -9.7929e-02, -1.0590e-02,  ..., -1.2456e-02,\n",
       "            -3.7622e-01,  3.1722e-01],\n",
       "           [-4.9047e-02, -9.0461e-02, -3.9830e-02,  ..., -9.6752e-03,\n",
       "            -4.4441e-01,  3.4008e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 7.1119e-03, -3.5042e-02,  2.5181e-03,  ..., -3.8581e-03,\n",
       "             2.1181e-02, -2.3418e-02],\n",
       "           [ 5.8565e-01, -1.8747e-01, -1.9291e-01,  ...,  1.2790e-02,\n",
       "             5.4072e-02, -3.5146e-01],\n",
       "           [ 1.2612e-02, -2.2940e-01, -1.1371e-01,  ..., -2.5463e-01,\n",
       "             5.4778e-03,  7.3006e-02],\n",
       "           ...,\n",
       "           [-4.3129e-02,  1.4859e-01,  2.4120e-02,  ...,  7.7274e-02,\n",
       "            -6.5670e-02,  2.7950e-02],\n",
       "           [-7.8468e-02,  1.0162e-01,  2.9976e-03,  ...,  9.8309e-02,\n",
       "            -8.4491e-02, -1.0430e-02],\n",
       "           [-6.5630e-02,  1.4455e-01,  4.0635e-02,  ...,  5.8556e-02,\n",
       "            -6.5808e-02,  6.3452e-03]],\n",
       " \n",
       "          [[-2.9676e-02,  2.2703e-02, -2.4005e-03,  ..., -8.3020e-05,\n",
       "            -1.0200e-02,  1.3816e-02],\n",
       "           [-1.4159e-01, -1.1042e-01,  1.5288e-01,  ...,  1.1843e-01,\n",
       "             3.1821e-01,  4.1491e-02],\n",
       "           [-1.8608e-01, -2.0628e-02, -2.4005e-01,  ...,  1.2387e-01,\n",
       "             3.9270e-01, -7.4120e-02],\n",
       "           ...,\n",
       "           [-3.0680e-02,  7.7702e-02,  1.4112e-02,  ..., -4.2593e-02,\n",
       "             4.9768e-01,  1.2616e-01],\n",
       "           [-5.2996e-02,  9.5374e-02,  4.1647e-02,  ..., -1.0877e-01,\n",
       "             4.4920e-01,  1.0772e-01],\n",
       "           [-2.6725e-02,  5.9686e-02,  1.4713e-02,  ..., -7.4390e-02,\n",
       "             4.5794e-01,  1.0672e-01]],\n",
       " \n",
       "          [[ 4.8969e-04,  6.6180e-03,  1.1962e-02,  ...,  1.4958e-02,\n",
       "            -1.3231e-02, -1.0807e-04],\n",
       "           [-4.4775e-01, -3.8584e-01, -1.9143e-02,  ..., -1.0441e-01,\n",
       "            -1.1037e-01, -2.6219e-01],\n",
       "           [ 4.3785e-04, -1.2311e-01, -2.4339e-01,  ...,  6.3254e-03,\n",
       "            -1.4984e-01, -3.0514e-01],\n",
       "           ...,\n",
       "           [-3.3725e-01, -7.1810e-01, -2.6565e-01,  ..., -2.5734e-01,\n",
       "            -9.0464e-03,  4.1180e-01],\n",
       "           [-3.0111e-01, -6.5468e-01, -3.2591e-01,  ..., -2.8894e-01,\n",
       "            -6.1767e-02,  3.6328e-01],\n",
       "           [-3.3615e-01, -6.9048e-01, -2.4146e-01,  ..., -2.8510e-01,\n",
       "             2.6148e-03,  4.0608e-01]]]]),\n",
       " tensor([[[[ 1.6161e-02,  2.7538e-02,  9.7017e-03,  ..., -8.4585e-03,\n",
       "            -7.1529e-03, -1.5902e-02],\n",
       "           [-2.0328e-01,  1.5041e-01,  4.7846e-01,  ...,  2.2609e-01,\n",
       "             1.0129e-01, -2.5380e-01],\n",
       "           [-4.9747e-01, -4.6525e-01,  1.7160e-01,  ...,  4.3839e-01,\n",
       "            -4.0725e-01,  1.7395e-01],\n",
       "           ...,\n",
       "           [ 2.8611e-01,  2.6531e-01, -2.7546e-01,  ..., -1.0135e-01,\n",
       "             1.0075e-01, -1.0030e-01],\n",
       "           [ 2.5799e-01,  2.9033e-01, -2.2459e-01,  ..., -1.2318e-01,\n",
       "             1.0104e-01, -4.9903e-02],\n",
       "           [ 3.0530e-01,  2.7995e-01, -2.4851e-01,  ..., -1.1916e-01,\n",
       "             9.3476e-02, -9.6587e-02]],\n",
       " \n",
       "          [[-2.1874e-02, -4.6209e-03, -1.0849e-02,  ..., -1.4652e-02,\n",
       "            -2.9159e-03,  2.7266e-02],\n",
       "           [ 3.4866e-01, -5.5887e-02, -1.1827e-01,  ...,  1.6326e-02,\n",
       "             2.2927e-01,  1.6507e-01],\n",
       "           [ 6.5020e-02, -7.8470e-01, -6.7600e-02,  ...,  5.0532e-01,\n",
       "             5.8251e-01,  4.2702e-01],\n",
       "           ...,\n",
       "           [-2.6251e-01,  7.5500e-02,  2.3477e-01,  ..., -3.0785e-02,\n",
       "            -1.1759e-01, -2.9461e-01],\n",
       "           [-1.9974e-01,  7.4568e-02,  1.8481e-01,  ..., -4.6454e-03,\n",
       "            -9.5330e-02, -3.3268e-01],\n",
       "           [-2.6720e-01,  8.0859e-02,  2.1306e-01,  ..., -3.0003e-02,\n",
       "            -1.2367e-01, -3.1931e-01]],\n",
       " \n",
       "          [[-9.6413e-03, -1.6896e-02,  9.8698e-03,  ...,  6.9843e-03,\n",
       "            -1.9692e-03,  1.0195e-03],\n",
       "           [-7.8277e-01, -3.3300e-01,  3.3584e-01,  ...,  3.6386e-01,\n",
       "            -6.4252e-01, -9.6721e-01],\n",
       "           [-2.4917e-01, -1.6284e-01, -8.7053e-02,  ...,  8.9797e-01,\n",
       "             5.7510e-02, -5.3385e-01],\n",
       "           ...,\n",
       "           [ 2.3078e-01,  3.1514e-01, -1.3672e-01,  ..., -2.4092e-01,\n",
       "            -4.6182e-01,  3.9040e-01],\n",
       "           [ 2.9769e-01,  2.1271e-01, -7.9001e-02,  ..., -2.5747e-01,\n",
       "            -3.7897e-01,  2.7164e-01],\n",
       "           [ 2.0665e-01,  2.5082e-01, -8.0827e-02,  ..., -2.4172e-01,\n",
       "            -4.1335e-01,  3.2897e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.9398e-02, -2.9675e-02,  6.4367e-03,  ...,  3.3720e-03,\n",
       "             8.8676e-03, -2.5241e-02],\n",
       "           [-2.3441e-01,  1.9490e-01, -2.7777e-01,  ...,  2.3345e-01,\n",
       "            -1.3193e-01,  1.8962e-01],\n",
       "           [-1.0722e-01,  7.3039e-02,  5.6168e-02,  ..., -1.7737e-01,\n",
       "            -2.8417e-01, -2.9217e-02],\n",
       "           ...,\n",
       "           [-2.6408e-01, -9.2664e-02, -1.8612e-01,  ...,  1.9497e-01,\n",
       "            -1.3516e-01,  4.2270e-01],\n",
       "           [-2.0121e-01,  1.2993e-02, -1.9406e-01,  ...,  2.0311e-01,\n",
       "            -1.1974e-01,  4.2407e-01],\n",
       "           [-2.5145e-01, -3.7211e-02, -2.0000e-01,  ...,  2.2779e-01,\n",
       "            -1.1986e-01,  4.2834e-01]],\n",
       " \n",
       "          [[-5.3107e-03, -3.8648e-03,  1.9185e-03,  ..., -8.7540e-03,\n",
       "            -7.3561e-03, -8.0138e-03],\n",
       "           [ 4.4208e-01, -6.6122e-02,  3.0386e-02,  ...,  2.9071e-01,\n",
       "            -3.3250e-01, -1.4332e-01],\n",
       "           [-2.7976e-01, -3.6659e-01,  2.3719e-01,  ...,  2.4900e-01,\n",
       "            -4.9304e-02, -4.1820e-01],\n",
       "           ...,\n",
       "           [-4.6972e-02, -8.9955e-03,  7.7708e-02,  ...,  3.9701e-01,\n",
       "             2.5450e-02,  1.8276e-01],\n",
       "           [-4.5446e-02, -1.5158e-02,  6.5714e-02,  ...,  3.9251e-01,\n",
       "            -3.9821e-02,  1.7745e-01],\n",
       "           [-3.8567e-02, -5.2058e-03,  5.0299e-02,  ...,  3.7428e-01,\n",
       "             1.3681e-02,  1.7408e-01]],\n",
       " \n",
       "          [[-4.2922e-02, -7.1736e-03,  2.1775e-02,  ...,  1.3131e-02,\n",
       "            -8.6913e-04, -1.1902e-02],\n",
       "           [-1.7840e-01,  3.6403e-01, -1.0869e-01,  ...,  1.4362e-01,\n",
       "            -2.4674e-01,  1.4542e-03],\n",
       "           [-7.5598e-01,  1.4540e-01, -1.0805e+00,  ..., -9.4367e-01,\n",
       "            -6.8572e-02,  1.3129e-01],\n",
       "           ...,\n",
       "           [-1.1919e-01, -3.2310e-01,  1.8372e-01,  ..., -3.1583e-01,\n",
       "            -3.3897e-02, -1.4906e-01],\n",
       "           [-1.5493e-01, -2.7836e-01,  2.2201e-01,  ..., -3.3319e-01,\n",
       "            -3.8850e-02, -1.0420e-01],\n",
       "           [-1.2236e-01, -3.3723e-01,  1.9704e-01,  ..., -3.2727e-01,\n",
       "            -4.4803e-02, -1.2403e-01]]]]),\n",
       " tensor([[[[-1.5470e-02, -1.8574e-03,  8.6153e-03,  ...,  7.6992e-03,\n",
       "             2.5628e-02,  2.4087e-02],\n",
       "           [-4.5026e-01, -3.8700e-01,  1.4924e-01,  ..., -2.6434e-01,\n",
       "            -1.2868e-01, -9.3104e-01],\n",
       "           [-5.1278e-01, -6.4021e-01, -8.9921e-02,  ...,  3.4962e-02,\n",
       "             8.1663e-01, -5.6065e-01],\n",
       "           ...,\n",
       "           [-6.8391e-02, -1.7857e-02, -4.2340e-01,  ...,  6.9913e-02,\n",
       "            -1.0466e-01,  2.1530e-01],\n",
       "           [-8.3703e-02, -6.5506e-04, -4.1611e-01,  ...,  8.5908e-02,\n",
       "            -7.2505e-02,  2.1226e-01],\n",
       "           [-7.9607e-02, -1.1482e-02, -4.1291e-01,  ...,  1.0189e-01,\n",
       "            -1.1387e-01,  1.8925e-01]],\n",
       " \n",
       "          [[ 4.9698e-03,  3.1646e-02, -9.3633e-03,  ...,  1.6307e-01,\n",
       "             9.6403e-03, -1.4476e-03],\n",
       "           [ 4.9661e-02,  9.0360e-03,  2.5832e-01,  ...,  2.6461e-01,\n",
       "            -2.6549e-01,  1.5591e-01],\n",
       "           [-1.6575e-01,  2.8302e-02,  2.7327e-01,  ..., -6.9440e-04,\n",
       "             1.8320e-01, -1.6156e-01],\n",
       "           ...,\n",
       "           [-3.4992e-01,  3.3547e-01, -6.5383e-02,  ...,  3.4832e-01,\n",
       "             3.2625e-01, -3.8571e-02],\n",
       "           [-3.4613e-01,  3.1353e-01, -9.6389e-02,  ...,  3.8051e-01,\n",
       "             3.2716e-01, -4.7746e-02],\n",
       "           [-3.8492e-01,  3.5841e-01, -5.7941e-02,  ...,  3.5176e-01,\n",
       "             3.3358e-01, -4.9684e-02]],\n",
       " \n",
       "          [[ 3.4705e-03, -9.4690e-03, -1.9156e-03,  ..., -5.9253e-03,\n",
       "             4.9764e-04, -2.2109e-03],\n",
       "           [ 1.7946e-01,  3.8253e-01,  5.8647e-01,  ..., -8.4817e-03,\n",
       "            -3.3496e-01,  2.3789e-01],\n",
       "           [-2.6939e-01, -1.0415e-01,  8.0688e-01,  ..., -9.7484e-02,\n",
       "            -2.6993e-01,  3.5963e-01],\n",
       "           ...,\n",
       "           [-3.1408e-01, -3.5644e-02,  6.1765e-01,  ..., -5.3029e-02,\n",
       "             5.9376e-01, -1.7093e-01],\n",
       "           [-1.3042e-01,  4.3685e-02,  5.9366e-01,  ..., -4.7333e-02,\n",
       "             3.9986e-01, -4.4305e-02],\n",
       "           [-1.6685e-01,  1.2922e-02,  5.7045e-01,  ..., -1.3673e-02,\n",
       "             5.5036e-01, -1.7379e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.5682e-03,  6.6571e-03,  5.7347e-03,  ..., -1.6122e-03,\n",
       "             1.7892e-02, -3.0711e-03],\n",
       "           [-5.8274e-01, -6.8398e-01, -7.1286e-02,  ..., -3.0752e-01,\n",
       "            -1.2771e-01,  5.7193e-01],\n",
       "           [-1.8989e-01, -5.8777e-01,  1.0654e+00,  ..., -2.4388e-01,\n",
       "            -5.8566e-01,  5.0473e-01],\n",
       "           ...,\n",
       "           [-1.4508e-01,  2.7836e-01, -2.0014e-01,  ..., -1.6509e-01,\n",
       "            -5.8466e-02, -2.5676e-01],\n",
       "           [-1.6197e-01,  2.5210e-01, -1.6843e-01,  ..., -2.0283e-01,\n",
       "            -5.8669e-02, -2.2770e-01],\n",
       "           [-1.2336e-01,  2.6676e-01, -1.9832e-01,  ..., -1.8200e-01,\n",
       "            -7.4683e-02, -2.3866e-01]],\n",
       " \n",
       "          [[ 6.9087e-04,  6.1964e-03,  1.4820e-03,  ..., -1.6301e-03,\n",
       "             3.2493e-02, -2.5166e-02],\n",
       "           [-5.7279e-01, -1.2695e-01,  1.8622e-01,  ..., -9.4733e-03,\n",
       "             3.1008e-01, -3.3534e-02],\n",
       "           [-5.5774e-01,  1.9523e-02, -1.4448e-02,  ..., -4.5227e-02,\n",
       "            -2.4657e-01,  9.3272e-01],\n",
       "           ...,\n",
       "           [ 2.7461e-01,  3.1581e-01, -1.0155e-01,  ..., -3.8973e-01,\n",
       "             3.9380e-01, -5.9481e-02],\n",
       "           [ 2.5863e-01,  3.1035e-01, -1.1276e-01,  ..., -3.4180e-01,\n",
       "             4.4168e-01, -1.0948e-01],\n",
       "           [ 2.6922e-01,  3.0150e-01, -9.8874e-02,  ..., -3.8163e-01,\n",
       "             3.8621e-01, -7.2177e-02]],\n",
       " \n",
       "          [[ 2.4620e-02, -4.4206e-02,  3.1208e-02,  ..., -4.0285e-02,\n",
       "            -2.2387e-02,  2.1858e-02],\n",
       "           [-3.9993e-01,  2.0133e-01,  7.4055e-01,  ...,  1.1542e-02,\n",
       "             1.7975e-01, -2.5748e-02],\n",
       "           [-3.0613e-01,  5.6909e-02,  6.6081e-01,  ..., -1.4772e-01,\n",
       "             2.7689e-01,  4.6027e-01],\n",
       "           ...,\n",
       "           [ 2.7036e-01, -1.4170e-01,  1.7608e-01,  ...,  5.9442e-03,\n",
       "             2.9355e-01,  4.2686e-01],\n",
       "           [ 2.7996e-01, -7.8556e-02,  1.8928e-01,  ...,  4.6187e-02,\n",
       "             2.6098e-01,  4.5613e-01],\n",
       "           [ 2.9901e-01, -1.4251e-01,  1.9079e-01,  ...,  1.1950e-02,\n",
       "             2.7595e-01,  4.5008e-01]]]]),\n",
       " tensor([[[[-1.7941e-02,  5.3526e-02,  5.3676e-02,  ...,  6.0029e-03,\n",
       "            -2.1083e-03, -1.6049e-02],\n",
       "           [ 2.9625e-01, -3.6007e-01,  1.2831e-01,  ..., -3.2624e-01,\n",
       "            -7.5992e-02,  4.6257e-01],\n",
       "           [-5.9212e-01, -7.9477e-01, -2.9869e-01,  ..., -5.9917e-01,\n",
       "             2.8612e-01,  2.7441e-01],\n",
       "           ...,\n",
       "           [-2.3705e-01,  8.1976e-02, -1.0181e-01,  ..., -3.4928e-03,\n",
       "            -1.2583e-01, -9.7663e-02],\n",
       "           [-2.6442e-01,  1.3355e-01, -6.5789e-02,  ..., -4.8229e-02,\n",
       "            -1.4812e-01, -7.1644e-02],\n",
       "           [-2.8853e-01,  6.5696e-02, -1.1611e-01,  ..., -2.3023e-02,\n",
       "            -1.2122e-01, -9.3625e-02]],\n",
       " \n",
       "          [[-3.9704e-03, -5.3119e-03, -1.4655e-02,  ...,  3.2955e-02,\n",
       "            -2.5065e-04, -3.0900e-04],\n",
       "           [ 9.2458e-02,  4.7695e-02,  1.3412e-03,  ..., -2.5146e-01,\n",
       "            -1.2662e-01, -1.5868e-01],\n",
       "           [-9.7907e-02,  4.0266e-01, -1.3571e-01,  ..., -4.7467e-02,\n",
       "             2.1676e-01, -3.0529e-01],\n",
       "           ...,\n",
       "           [-5.3518e-02, -7.6570e-02, -4.6691e-02,  ..., -2.2394e-01,\n",
       "            -1.9033e-01, -1.7696e-03],\n",
       "           [-1.1885e-02, -8.3788e-02, -8.9541e-02,  ..., -2.3919e-01,\n",
       "            -1.5565e-01,  2.5128e-02],\n",
       "           [-3.7944e-02, -8.4628e-02, -5.9978e-02,  ..., -1.9725e-01,\n",
       "            -1.6990e-01,  3.5761e-03]],\n",
       " \n",
       "          [[-2.4735e-02,  1.1999e-03,  1.0802e-02,  ..., -1.6009e-02,\n",
       "             1.9884e-02,  1.5330e-02],\n",
       "           [-2.3814e-01, -1.0468e-01, -6.4436e-02,  ...,  1.5349e-02,\n",
       "            -1.3452e-01,  3.7333e-01],\n",
       "           [-2.1199e-01, -2.0160e-01, -2.7963e-02,  ...,  8.5441e-02,\n",
       "            -1.4596e-01,  1.2479e-01],\n",
       "           ...,\n",
       "           [-5.9211e-03,  5.3513e-02,  3.5566e-02,  ...,  1.6654e-02,\n",
       "            -2.7398e-01, -2.0165e-01],\n",
       "           [-7.9470e-03,  4.6902e-02, -3.7089e-03,  ...,  2.3440e-03,\n",
       "            -2.5535e-01, -1.8567e-01],\n",
       "           [-8.8217e-04,  5.7114e-02,  1.9002e-02,  ...,  2.6817e-02,\n",
       "            -2.7234e-01, -1.9206e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.4064e-02,  2.2572e-02,  1.2086e-02,  ...,  7.8534e-03,\n",
       "             1.9599e-02, -4.8525e-02],\n",
       "           [-6.1757e-01,  6.3915e-01,  8.3329e-02,  ..., -2.8226e-02,\n",
       "            -8.6982e-02,  3.1185e-01],\n",
       "           [-3.4237e-01,  6.5895e-01, -8.8351e-02,  ..., -9.1257e-02,\n",
       "             1.0269e-01,  7.4208e-01],\n",
       "           ...,\n",
       "           [-2.3774e-01, -1.8501e-02,  6.3731e-02,  ...,  2.0886e-01,\n",
       "             1.5778e-01, -1.2643e-01],\n",
       "           [-2.5266e-01, -5.6938e-02,  2.9387e-02,  ...,  2.3877e-01,\n",
       "             9.6542e-02, -1.3555e-01],\n",
       "           [-1.9551e-01, -4.6101e-02,  5.1081e-02,  ...,  2.0221e-01,\n",
       "             1.3881e-01, -9.1585e-02]],\n",
       " \n",
       "          [[ 2.0544e-02, -2.2329e-02, -7.0940e-03,  ...,  5.0237e-03,\n",
       "             2.4337e-02,  3.7656e-03],\n",
       "           [ 6.7576e-02, -4.5074e-01, -1.1838e-01,  ..., -1.0758e-01,\n",
       "             2.8696e-01,  3.8602e-01],\n",
       "           [ 3.2730e-01, -4.6965e-01,  1.8085e-01,  ..., -5.6774e-01,\n",
       "             1.1137e-01,  3.9920e-01],\n",
       "           ...,\n",
       "           [ 3.5863e-01, -1.1569e-01, -4.5111e-01,  ..., -1.6241e-01,\n",
       "            -1.0074e-01, -3.8315e-01],\n",
       "           [ 3.5421e-01, -1.1740e-01, -4.7946e-01,  ..., -1.4310e-01,\n",
       "            -9.8597e-02, -3.4254e-01],\n",
       "           [ 3.6629e-01, -1.1581e-01, -4.6439e-01,  ..., -1.5777e-01,\n",
       "            -1.0607e-01, -3.7266e-01]],\n",
       " \n",
       "          [[-4.2712e-02, -1.7929e-02, -1.1628e-02,  ..., -1.4023e-02,\n",
       "             1.6396e-02,  7.9997e-03],\n",
       "           [ 3.4463e-01, -3.3991e-01, -2.5213e-01,  ...,  6.6874e-02,\n",
       "            -6.8529e-02, -1.5027e-01],\n",
       "           [ 5.3074e-02, -4.8979e-01, -2.3990e-01,  ...,  4.3358e-01,\n",
       "             1.5747e-01,  2.8892e-01],\n",
       "           ...,\n",
       "           [ 3.6043e-02,  2.9853e-01, -4.4551e-01,  ...,  1.4345e-01,\n",
       "            -6.0878e-02,  2.5730e-01],\n",
       "           [ 1.2652e-02,  3.0250e-01, -3.7236e-01,  ...,  1.3774e-01,\n",
       "             2.9270e-03,  2.6491e-01],\n",
       "           [ 4.0304e-02,  3.0198e-01, -4.2528e-01,  ...,  1.6067e-01,\n",
       "            -4.4021e-02,  2.7707e-01]]]]),\n",
       " tensor([[[[ 4.1113e-02,  1.2042e-02,  4.1623e-03,  ...,  4.1150e-03,\n",
       "            -4.0770e-03, -5.9165e-02],\n",
       "           [ 1.9592e-01,  2.3748e-02, -1.5300e-01,  ..., -1.3944e-01,\n",
       "             3.2670e-01,  5.0975e-01],\n",
       "           [ 3.3309e-01,  2.8878e-01, -3.2995e-01,  ..., -2.4649e-01,\n",
       "            -1.9059e-01,  6.1930e-01],\n",
       "           ...,\n",
       "           [ 5.0916e-01,  9.2535e-02, -3.0570e-01,  ...,  8.6951e-01,\n",
       "             4.0531e-01, -9.7920e-01],\n",
       "           [ 4.1126e-01,  9.4800e-02, -3.2950e-01,  ...,  8.1185e-01,\n",
       "             3.2780e-01, -9.1777e-01],\n",
       "           [ 4.1351e-01,  1.1534e-01, -3.3681e-01,  ...,  9.0274e-01,\n",
       "             4.1915e-01, -9.9231e-01]],\n",
       " \n",
       "          [[ 2.5551e-03, -6.5590e-04,  1.7520e-03,  ..., -2.1867e-03,\n",
       "             7.1799e-03,  3.7665e-03],\n",
       "           [-4.7031e-01, -3.5404e-02,  3.2341e-03,  ..., -1.8932e-01,\n",
       "            -5.9676e-01,  3.6789e-01],\n",
       "           [ 5.3165e-01, -8.9591e-02,  5.0701e-02,  ...,  2.1110e-01,\n",
       "            -2.3387e-01,  6.8640e-01],\n",
       "           ...,\n",
       "           [ 1.2614e-01,  8.7491e-02,  6.5608e-01,  ..., -2.8293e-01,\n",
       "             6.1028e-02, -7.9769e-02],\n",
       "           [ 1.7310e-01,  4.7721e-02,  6.2518e-01,  ..., -2.7279e-01,\n",
       "             4.1123e-02, -5.1855e-02],\n",
       "           [ 1.3893e-01,  7.7057e-02,  6.3625e-01,  ..., -2.9329e-01,\n",
       "             6.6479e-02, -5.8759e-02]],\n",
       " \n",
       "          [[ 6.3724e-03, -7.0963e-04, -4.0907e-04,  ...,  1.5351e-02,\n",
       "             1.2828e-03, -4.8608e-03],\n",
       "           [-1.5357e-01,  2.5882e-01,  8.9017e-02,  ...,  7.5335e-02,\n",
       "             7.5429e-03, -3.4387e-01],\n",
       "           [-5.7689e-01,  8.6215e-02,  6.9924e-03,  ...,  1.0111e-02,\n",
       "            -8.4904e-02, -2.4724e-01],\n",
       "           ...,\n",
       "           [ 2.6590e-01, -9.6982e-02, -5.0376e-02,  ...,  7.8433e-03,\n",
       "             4.7653e-02, -2.3102e-01],\n",
       "           [ 2.4361e-01, -5.5258e-02, -5.4739e-02,  ..., -7.9922e-03,\n",
       "             4.5483e-02, -2.2118e-01],\n",
       "           [ 2.5406e-01, -8.4880e-02, -8.2650e-03,  ...,  2.5619e-03,\n",
       "             5.9542e-02, -2.3622e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.9746e-02, -2.1958e-02, -7.7823e-03,  ...,  8.5299e-03,\n",
       "            -2.7968e-02,  2.7693e-02],\n",
       "           [-1.0361e-01, -7.8283e-02,  1.4160e-01,  ..., -1.1268e-01,\n",
       "            -1.7189e-01, -3.7144e-02],\n",
       "           [ 3.1553e-01, -1.8861e-01,  1.4880e-02,  ...,  1.2450e-02,\n",
       "            -1.4595e-02,  4.8448e-01],\n",
       "           ...,\n",
       "           [-1.2342e-01, -4.9270e-02,  3.0467e-01,  ..., -3.3047e-02,\n",
       "            -5.6550e-02,  3.1970e-02],\n",
       "           [-1.1541e-01, -5.1273e-02,  2.8922e-01,  ..., -4.3609e-02,\n",
       "            -2.4223e-02,  6.1045e-02],\n",
       "           [-1.1364e-01, -4.0151e-02,  3.0306e-01,  ..., -4.6529e-02,\n",
       "            -5.8749e-02,  3.9629e-02]],\n",
       " \n",
       "          [[-2.3024e-03, -2.3890e-02,  1.6305e-02,  ..., -2.1686e-02,\n",
       "            -2.7167e-03, -3.0660e-02],\n",
       "           [-3.4767e-01, -8.1501e-02, -3.1688e-01,  ...,  1.1758e-01,\n",
       "             1.8448e-01,  5.1239e-01],\n",
       "           [-6.5844e-01,  4.0547e-01, -5.2955e-01,  ..., -3.5115e-01,\n",
       "             2.6722e-01, -5.6817e-01],\n",
       "           ...,\n",
       "           [ 3.5023e-01, -7.0704e-01, -4.0510e-01,  ..., -2.9828e-01,\n",
       "            -1.7200e-01, -6.9527e-01],\n",
       "           [ 3.6092e-01, -7.1676e-01, -3.8869e-01,  ..., -3.0201e-01,\n",
       "            -1.4163e-01, -6.9383e-01],\n",
       "           [ 3.9020e-01, -6.9358e-01, -4.0902e-01,  ..., -3.2329e-01,\n",
       "            -1.6175e-01, -7.2313e-01]],\n",
       " \n",
       "          [[ 3.0047e-02,  1.8437e-03, -1.1858e-02,  ...,  3.6455e-03,\n",
       "             3.5163e-03,  1.1927e-02],\n",
       "           [ 3.2755e-01, -2.3516e-01,  8.7128e-01,  ...,  2.4885e-01,\n",
       "            -6.7578e-01,  5.2981e-01],\n",
       "           [ 3.5999e-01, -6.7292e-01,  7.7878e-01,  ..., -6.3922e-01,\n",
       "            -2.1266e-02, -5.6684e-01],\n",
       "           ...,\n",
       "           [-2.1324e-01, -3.7404e-01, -9.4188e-02,  ...,  1.8621e-01,\n",
       "             4.6994e-01, -2.8753e-02],\n",
       "           [-1.6242e-01, -3.4325e-01, -1.2465e-01,  ...,  1.5190e-01,\n",
       "             3.7764e-01, -2.7515e-02],\n",
       "           [-1.7484e-01, -3.8594e-01, -1.3507e-01,  ...,  1.9443e-01,\n",
       "             4.3181e-01, -3.2784e-02]]]]),\n",
       " tensor([[[[ 0.0035,  0.0084, -0.0075,  ...,  0.0181, -0.0059, -0.0127],\n",
       "           [-0.2251, -0.0920, -0.0745,  ...,  0.4034,  0.0691, -0.1096],\n",
       "           [-0.3158, -0.4517,  0.2865,  ..., -0.1406,  0.5092, -0.2778],\n",
       "           ...,\n",
       "           [ 0.4210, -0.4820, -0.0088,  ...,  0.3271,  0.1006,  0.2966],\n",
       "           [ 0.3597, -0.4189, -0.0263,  ...,  0.3213,  0.0669,  0.2416],\n",
       "           [ 0.4250, -0.4107, -0.0109,  ...,  0.3398,  0.0642,  0.3168]],\n",
       " \n",
       "          [[-0.0122, -0.0302, -0.0356,  ...,  0.0560, -0.0067, -0.0120],\n",
       "           [-0.4448,  0.3737,  0.2984,  ..., -0.4687, -0.5532,  0.3066],\n",
       "           [-0.2901, -0.0909,  0.0353,  ...,  0.3977, -0.9618,  0.1852],\n",
       "           ...,\n",
       "           [ 0.0183, -0.2358,  0.0457,  ...,  0.5713,  0.4950,  0.2332],\n",
       "           [ 0.0469, -0.2743,  0.0247,  ...,  0.5338,  0.4244,  0.2176],\n",
       "           [ 0.0622, -0.2481,  0.0418,  ...,  0.5694,  0.4534,  0.2495]],\n",
       " \n",
       "          [[-0.0532, -0.0074, -0.0087,  ...,  0.0337,  0.0289,  0.0132],\n",
       "           [ 0.2230,  0.5372, -0.2647,  ..., -0.8441,  0.2510, -0.2865],\n",
       "           [ 0.4764,  0.6037,  0.1697,  ..., -0.5908,  0.4810,  0.7832],\n",
       "           ...,\n",
       "           [ 0.3981, -0.7351,  0.0693,  ...,  0.8531, -0.0391, -0.2810],\n",
       "           [ 0.4123, -0.7237,  0.0776,  ...,  0.8708, -0.0021, -0.2513],\n",
       "           [ 0.4164, -0.7199,  0.0706,  ...,  0.8839, -0.0310, -0.2848]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0055,  0.0079,  0.0635,  ...,  0.0145, -0.0224, -0.0232],\n",
       "           [ 0.0281, -0.2875, -0.1855,  ...,  0.0601,  0.4683,  0.6456],\n",
       "           [ 0.4268, -0.1954, -0.0789,  ...,  0.3333,  0.9027,  0.2556],\n",
       "           ...,\n",
       "           [ 0.2255, -0.6398, -0.7937,  ...,  0.3280,  0.0282,  0.1319],\n",
       "           [ 0.2173, -0.5893, -0.7054,  ...,  0.2868, -0.0396,  0.1136],\n",
       "           [ 0.2249, -0.6160, -0.7654,  ...,  0.3497,  0.0574,  0.1316]],\n",
       " \n",
       "          [[-0.0115, -0.0103, -0.0053,  ..., -0.0128,  0.0046, -0.0038],\n",
       "           [ 0.1052,  0.2070, -0.1262,  ..., -0.0550, -0.8726, -0.0939],\n",
       "           [-0.0517,  0.2307, -0.1204,  ...,  0.6559, -0.4155, -0.1046],\n",
       "           ...,\n",
       "           [-0.0059,  0.0368, -0.6362,  ...,  0.2798, -0.2893, -0.0738],\n",
       "           [-0.0023,  0.0206, -0.5933,  ...,  0.2594, -0.2914, -0.0486],\n",
       "           [ 0.0070,  0.0455, -0.6341,  ...,  0.2819, -0.2739, -0.1075]],\n",
       " \n",
       "          [[ 0.0082, -0.0062,  0.0052,  ...,  0.0109,  0.0297,  0.0024],\n",
       "           [ 0.0062, -0.1266,  0.3971,  ..., -0.3529,  0.0357,  0.1639],\n",
       "           [ 0.1377, -0.5570, -0.0803,  ..., -0.2144, -0.0383, -0.2826],\n",
       "           ...,\n",
       "           [ 0.0949,  0.1794, -0.3145,  ..., -0.1068, -0.2174, -0.0395],\n",
       "           [ 0.0753,  0.1761, -0.3291,  ..., -0.1012, -0.2285, -0.0606],\n",
       "           [ 0.0979,  0.1656, -0.3055,  ..., -0.1211, -0.2432, -0.0868]]]]),\n",
       " tensor([[[[-1.0592e-02, -1.1225e-02, -1.4035e-02,  ...,  1.3950e-02,\n",
       "             1.0779e-02, -5.9708e-03],\n",
       "           [-2.1233e-01, -2.2543e-01, -6.6061e-01,  ..., -3.0284e-02,\n",
       "            -1.3177e-01, -2.6566e-01],\n",
       "           [-3.5177e-01,  5.7665e-02, -4.3983e-02,  ...,  2.0692e-01,\n",
       "            -3.5757e-01, -2.3483e-01],\n",
       "           ...,\n",
       "           [ 1.4076e-01,  1.0909e-01,  3.4638e-01,  ..., -4.6532e-01,\n",
       "             9.3318e-02,  1.2768e-01],\n",
       "           [ 1.1055e-01,  6.6127e-02,  3.0831e-01,  ..., -4.5081e-01,\n",
       "             1.4818e-02,  1.0263e-01],\n",
       "           [ 1.7096e-01,  1.0656e-01,  3.1349e-01,  ..., -4.9439e-01,\n",
       "             6.2462e-02,  9.4236e-02]],\n",
       " \n",
       "          [[-3.6977e-02, -1.7596e-02,  6.3675e-03,  ..., -9.1164e-03,\n",
       "             9.7122e-03,  1.1739e-02],\n",
       "           [-3.8815e-01,  4.5259e-01,  8.0333e-01,  ...,  6.4261e-01,\n",
       "             1.4931e+00,  1.3942e-01],\n",
       "           [-6.2217e-02, -3.4162e-01, -2.4539e-01,  ...,  4.2726e-02,\n",
       "             4.2540e-01, -5.1503e-01],\n",
       "           ...,\n",
       "           [ 2.8918e-01, -3.5135e-02, -3.8754e-01,  ..., -2.4085e-01,\n",
       "             9.4612e-02, -1.9385e-01],\n",
       "           [ 3.2839e-01,  4.5236e-02, -5.3533e-01,  ..., -4.0586e-01,\n",
       "             1.4591e-01, -1.6098e-01],\n",
       "           [ 2.6226e-01, -9.5222e-02, -3.8333e-01,  ..., -2.6605e-01,\n",
       "             1.0857e-01, -1.0189e-01]],\n",
       " \n",
       "          [[ 1.5174e-03, -2.8347e-03, -1.9113e-02,  ..., -1.4898e-02,\n",
       "            -1.8655e-02, -4.1318e-03],\n",
       "           [ 1.4143e-01,  4.6954e-01, -1.0579e-01,  ..., -3.2212e-01,\n",
       "             9.5190e-02, -2.0422e-02],\n",
       "           [ 1.5230e-01,  2.8746e-01,  4.6638e-01,  ...,  7.4363e-01,\n",
       "             4.7562e-02,  2.1510e-01],\n",
       "           ...,\n",
       "           [ 1.7866e-01,  7.5989e-02,  1.0242e-01,  ...,  2.0902e-01,\n",
       "            -2.4297e-01,  1.3264e-03],\n",
       "           [ 2.1976e-01,  1.0145e-01,  6.4717e-02,  ...,  2.0134e-01,\n",
       "            -2.0446e-01, -1.3413e-02],\n",
       "           [ 1.9099e-01,  8.0435e-02,  1.0422e-01,  ...,  2.1264e-01,\n",
       "            -2.4688e-01, -1.1903e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.3425e-02,  1.8843e-02, -1.2420e-02,  ..., -2.7767e-02,\n",
       "             1.6856e-03,  1.2396e-02],\n",
       "           [ 9.9542e-02, -1.3561e-01, -6.1215e-01,  ..., -6.2190e-01,\n",
       "             1.9952e-02, -1.2767e-01],\n",
       "           [ 2.0061e-01,  3.4125e-01, -3.2466e-01,  ..., -5.1032e-01,\n",
       "            -1.4619e-01, -3.0913e-01],\n",
       "           ...,\n",
       "           [-4.6036e-01,  2.5949e-01,  4.8314e-03,  ..., -6.7489e-02,\n",
       "            -2.6435e-01, -1.3655e-01],\n",
       "           [-4.9190e-01,  2.3266e-01,  4.3150e-02,  ..., -4.1132e-02,\n",
       "            -1.7109e-01, -1.7661e-01],\n",
       "           [-4.6677e-01,  3.0483e-01, -1.0377e-02,  ..., -6.4818e-02,\n",
       "            -2.6650e-01, -1.2108e-01]],\n",
       " \n",
       "          [[-4.3962e-03, -1.6555e-02, -1.4231e-02,  ...,  9.7152e-03,\n",
       "             6.6440e-03,  3.6366e-03],\n",
       "           [-3.2907e-01, -6.8099e-01, -4.6555e-01,  ..., -5.0895e-02,\n",
       "            -1.9798e-01, -1.3651e+00],\n",
       "           [ 6.4260e-01, -6.3667e-01,  7.6991e-01,  ..., -2.3786e-01,\n",
       "            -6.1505e-01, -1.0222e+00],\n",
       "           ...,\n",
       "           [-1.1013e-01, -1.7511e-02, -1.8683e-01,  ..., -5.3321e-02,\n",
       "             6.2742e-02,  3.9106e-01],\n",
       "           [-5.2033e-02, -2.6368e-02, -1.0928e-01,  ..., -1.7289e-02,\n",
       "             5.2365e-02,  4.1496e-01],\n",
       "           [-9.8811e-02, -3.6606e-02, -1.3308e-01,  ...,  8.2475e-03,\n",
       "             3.8307e-02,  4.0914e-01]],\n",
       " \n",
       "          [[-1.5694e-02,  1.2333e-02, -1.1447e-01,  ...,  2.9420e-03,\n",
       "            -3.5542e-02, -2.3602e-03],\n",
       "           [ 2.1735e-01, -3.0963e-01,  6.1437e-01,  ...,  5.1576e-01,\n",
       "             2.1273e-01,  3.9906e-01],\n",
       "           [ 9.8797e-01,  1.6293e-01,  6.2364e-01,  ...,  8.1541e-02,\n",
       "            -3.3365e-01, -2.1689e-01],\n",
       "           ...,\n",
       "           [ 6.4825e-02, -2.2757e-01,  2.4521e-01,  ..., -1.3240e-01,\n",
       "            -2.3709e-01,  2.2346e-01],\n",
       "           [ 3.3397e-02, -2.1484e-01,  2.5705e-01,  ..., -9.1373e-02,\n",
       "            -2.6188e-01,  2.2832e-01],\n",
       "           [ 5.6794e-02, -2.5528e-01,  2.2102e-01,  ..., -1.1712e-01,\n",
       "            -2.5184e-01,  1.9113e-01]]]]),\n",
       " tensor([[[[-4.7070e-03,  3.0867e-01,  4.8799e-03,  ...,  3.3680e-02,\n",
       "             1.4713e-02, -7.6719e-03],\n",
       "           [ 1.1253e-01, -5.2170e-01,  6.5344e-01,  ...,  2.0453e-01,\n",
       "             2.0282e-01, -1.6740e-01],\n",
       "           [-1.9540e-01, -1.7931e+00, -3.3379e-01,  ..., -5.0139e-03,\n",
       "            -1.4089e-01, -1.8128e-01],\n",
       "           ...,\n",
       "           [ 3.6521e-02, -3.5186e-01, -2.3817e-01,  ..., -5.9960e-01,\n",
       "             8.4845e-02,  1.7832e-01],\n",
       "           [ 6.4462e-02, -1.6539e-01, -2.2956e-01,  ..., -6.2134e-01,\n",
       "            -2.8511e-02,  2.7259e-01],\n",
       "           [ 4.8030e-02, -3.1758e-01, -2.5059e-01,  ..., -5.8527e-01,\n",
       "             1.1061e-01,  2.7052e-01]],\n",
       " \n",
       "          [[ 3.0296e-03,  1.7998e-02, -2.0839e-02,  ...,  1.0002e-03,\n",
       "             3.5163e-04, -2.9904e-01],\n",
       "           [-6.8719e-01,  4.0107e-02,  1.9345e-01,  ..., -3.9347e-01,\n",
       "            -4.6354e-01,  1.5472e+00],\n",
       "           [-6.6177e-01, -1.8932e-01,  3.3509e-01,  ..., -5.6775e-01,\n",
       "             4.5720e-02,  9.1526e-01],\n",
       "           ...,\n",
       "           [ 3.3199e-01, -7.3231e-02, -4.2018e-01,  ..., -1.2831e-01,\n",
       "             2.5568e-02,  3.4637e-01],\n",
       "           [ 3.6415e-01, -6.9420e-02, -4.0199e-01,  ..., -1.1663e-01,\n",
       "             1.4401e-02,  2.8610e-01],\n",
       "           [ 3.7067e-01, -6.5457e-02, -4.0500e-01,  ..., -1.2365e-01,\n",
       "             1.0100e-02,  2.9223e-01]],\n",
       " \n",
       "          [[-5.3899e-03,  6.7658e-03, -1.0362e-02,  ...,  1.6280e-02,\n",
       "            -2.6215e-03, -2.5677e-02],\n",
       "           [-1.2139e-01, -4.5869e-01, -4.4823e-01,  ...,  9.0303e-02,\n",
       "             3.7309e-01, -3.7593e-01],\n",
       "           [-4.8976e-01, -6.1668e-01, -5.5525e-02,  ...,  6.8077e-02,\n",
       "             9.4274e-01, -3.1762e-01],\n",
       "           ...,\n",
       "           [ 1.4531e-01,  1.2852e-01,  9.2102e-03,  ...,  1.4411e-01,\n",
       "            -2.5430e-02, -8.9539e-02],\n",
       "           [ 1.6979e-01,  1.7411e-01, -1.5772e-02,  ...,  1.1079e-01,\n",
       "            -5.1526e-02, -8.2681e-02],\n",
       "           [ 1.3813e-01,  2.3557e-01, -1.2306e-02,  ...,  1.0527e-01,\n",
       "            -3.6396e-02, -7.4795e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.9506e-02,  6.9415e-03,  6.6632e-03,  ..., -2.8432e-02,\n",
       "            -1.2377e-02, -1.4771e-02],\n",
       "           [ 4.2146e-01, -2.0333e-01, -2.2046e-01,  ...,  1.7511e-04,\n",
       "             5.9362e-01, -1.5803e-01],\n",
       "           [ 1.8147e-01, -6.4780e-01, -4.6783e-02,  ...,  1.0866e-01,\n",
       "             5.5790e-01,  2.5910e-01],\n",
       "           ...,\n",
       "           [-9.5462e-03,  1.5723e-01,  3.4767e-01,  ...,  3.8738e-01,\n",
       "             5.1145e-01,  1.9469e-01],\n",
       "           [-6.1616e-02,  1.2221e-01,  3.7168e-01,  ...,  2.8783e-01,\n",
       "             5.1922e-01,  2.1548e-01],\n",
       "           [-4.2594e-02,  1.1241e-01,  3.9738e-01,  ...,  3.8634e-01,\n",
       "             5.0616e-01,  1.8133e-01]],\n",
       " \n",
       "          [[-2.1237e-02, -9.6493e-03, -8.7213e-03,  ..., -1.3156e-03,\n",
       "            -3.8059e-01, -2.3946e-03],\n",
       "           [-1.4175e-01,  3.2665e-02, -2.8177e-01,  ..., -6.5004e-02,\n",
       "             1.6294e-01, -1.3024e-01],\n",
       "           [ 2.1777e-01, -8.7235e-02, -4.5209e-02,  ..., -1.9657e-01,\n",
       "             2.7470e-01, -6.1565e-02],\n",
       "           ...,\n",
       "           [-2.6974e-02, -1.6041e-01, -1.6376e-02,  ...,  7.0802e-02,\n",
       "             1.2395e-01, -1.0668e-01],\n",
       "           [-2.9779e-02, -1.4008e-01, -6.4488e-02,  ...,  5.7753e-02,\n",
       "             6.7521e-02, -1.0488e-01],\n",
       "           [-1.7526e-02, -1.7956e-01, -3.0154e-02,  ...,  4.5605e-02,\n",
       "             8.8989e-02, -1.1016e-01]],\n",
       " \n",
       "          [[ 1.6645e-03, -1.0027e-02,  1.8757e-02,  ..., -9.6282e-03,\n",
       "            -6.0096e-03,  7.6976e-03],\n",
       "           [ 2.1634e-01, -8.2470e-02, -3.1652e-01,  ...,  1.5607e-01,\n",
       "            -4.7365e-01,  4.9930e-01],\n",
       "           [-7.6857e-01, -5.1922e-01,  4.4484e-02,  ..., -2.5883e-01,\n",
       "            -1.1254e-01,  3.4002e-01],\n",
       "           ...,\n",
       "           [ 9.8091e-02, -1.0834e-02, -2.1036e-01,  ..., -5.3825e-02,\n",
       "             4.3821e-01,  3.8288e-01],\n",
       "           [ 6.1035e-02, -2.4592e-02, -2.1843e-01,  ..., -9.7181e-02,\n",
       "             4.2293e-01,  3.9697e-01],\n",
       "           [ 5.2480e-02, -1.9024e-02, -2.6975e-01,  ..., -7.2662e-02,\n",
       "             4.3807e-01,  3.3957e-01]]]]),\n",
       " tensor([[[[ 1.9792e-02, -1.4440e-02,  6.9060e-03,  ..., -8.5864e-03,\n",
       "            -4.9918e-03, -5.9601e-03],\n",
       "           [-9.4775e-02,  1.8978e-01,  4.7820e-01,  ..., -2.0118e-01,\n",
       "             1.9842e-01,  9.5302e-02],\n",
       "           [ 1.0692e+00,  1.4368e-01,  9.7445e-02,  ..., -2.4043e-01,\n",
       "             3.3751e-01,  4.8132e-01],\n",
       "           ...,\n",
       "           [ 7.6995e-02, -2.6054e-01,  1.8113e-01,  ...,  7.1076e-02,\n",
       "             4.4462e-02, -3.1511e-01],\n",
       "           [ 3.2387e-02, -2.1155e-01,  2.2396e-01,  ...,  7.1241e-02,\n",
       "             1.7971e-02, -2.6495e-01],\n",
       "           [ 4.1875e-02, -2.6276e-01,  2.3011e-01,  ...,  4.6287e-02,\n",
       "             4.8019e-02, -3.0537e-01]],\n",
       " \n",
       "          [[ 9.8752e-03, -1.7124e-03,  9.3664e-03,  ...,  6.8841e-04,\n",
       "            -7.0508e-03, -1.4226e-02],\n",
       "           [-9.9199e-02, -6.6251e-01, -2.7691e-01,  ..., -1.6913e-01,\n",
       "            -5.0023e-01,  5.1267e-02],\n",
       "           [-1.9670e-01, -5.0566e-02, -5.2740e-01,  ..., -8.5692e-01,\n",
       "            -4.4837e-01, -8.1062e-03],\n",
       "           ...,\n",
       "           [-3.0035e-01, -3.7774e-01,  1.6162e-01,  ...,  2.0917e-01,\n",
       "             2.9272e-01, -5.0740e-01],\n",
       "           [-3.1143e-01, -3.0318e-01,  6.3629e-02,  ...,  1.8258e-01,\n",
       "             2.4445e-01, -4.2830e-01],\n",
       "           [-3.0245e-01, -3.3690e-01,  1.4009e-01,  ...,  2.1302e-01,\n",
       "             3.1476e-01, -4.9323e-01]],\n",
       " \n",
       "          [[-2.3667e-03,  6.9975e-03,  1.3135e-02,  ...,  1.7987e-02,\n",
       "            -2.5874e-02, -4.5377e-02],\n",
       "           [ 2.5679e-02, -5.2020e-01,  5.3408e-02,  ..., -4.2266e-01,\n",
       "            -2.4639e-01,  1.6085e-01],\n",
       "           [ 1.8578e-01,  6.0631e-01,  1.5373e-02,  ...,  1.1537e-01,\n",
       "             5.1113e-01, -4.7715e-01],\n",
       "           ...,\n",
       "           [-4.0110e-01, -2.2944e-01, -3.9440e-01,  ...,  3.6666e-02,\n",
       "             2.5758e-01,  4.4354e-01],\n",
       "           [-3.3750e-01, -1.5895e-01, -3.7823e-01,  ..., -4.2344e-02,\n",
       "             3.1130e-01,  3.1672e-01],\n",
       "           [-3.3219e-01, -2.2145e-01, -4.2908e-01,  ..., -2.4656e-02,\n",
       "             3.0167e-01,  4.7652e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0060e-02,  4.8267e-03, -3.6001e-04,  ..., -1.1708e-02,\n",
       "            -1.8576e-02, -4.3555e-03],\n",
       "           [ 1.7119e-01,  7.6366e-01, -1.6575e-01,  ..., -6.5900e-02,\n",
       "             5.1521e-01, -3.7071e-01],\n",
       "           [ 8.6644e-01,  5.2124e-01, -8.1814e-01,  ..., -7.3059e-01,\n",
       "             2.3665e-01,  5.3350e-01],\n",
       "           ...,\n",
       "           [ 9.4637e-02, -1.2898e-01,  1.2485e-01,  ..., -1.4216e-02,\n",
       "             2.1677e-01,  3.5110e-01],\n",
       "           [ 1.3597e-01, -1.5568e-01,  1.7773e-01,  ..., -2.4804e-02,\n",
       "             2.1478e-01,  3.6945e-01],\n",
       "           [ 1.2574e-01, -1.6150e-01,  1.4458e-01,  ...,  1.3480e-02,\n",
       "             2.4848e-01,  3.4908e-01]],\n",
       " \n",
       "          [[-8.4090e-03,  6.3886e-03, -4.1796e-02,  ...,  1.6225e-02,\n",
       "            -9.4142e-04, -4.6437e-03],\n",
       "           [-4.5011e-01, -6.2587e-01,  2.1762e-01,  ..., -3.1801e-01,\n",
       "            -1.1206e-01,  5.8609e-01],\n",
       "           [-2.4714e-01,  1.1406e-01,  2.8834e-01,  ...,  2.3095e-01,\n",
       "             9.7868e-01,  2.4395e-02],\n",
       "           ...,\n",
       "           [ 6.2637e-03,  3.0737e-02,  4.8245e-02,  ..., -3.9100e-02,\n",
       "            -3.3727e-01,  2.9521e-01],\n",
       "           [ 2.1752e-02, -8.2347e-02,  1.8083e-01,  ..., -1.3893e-01,\n",
       "            -3.3138e-01,  2.5048e-01],\n",
       "           [ 2.0728e-02,  6.0528e-02,  8.7946e-02,  ..., -7.2172e-02,\n",
       "            -3.7443e-01,  3.1601e-01]],\n",
       " \n",
       "          [[ 2.8317e-03,  2.0027e-02, -5.9075e-03,  ...,  3.4196e-03,\n",
       "             1.6646e-02,  2.5357e-03],\n",
       "           [-3.7606e-01, -1.5451e-01, -7.3518e-01,  ..., -1.6202e-01,\n",
       "             6.3471e-02,  3.5443e-01],\n",
       "           [ 4.7178e-01,  4.4088e-01, -2.7107e-01,  ...,  1.2635e-01,\n",
       "             2.6354e-01,  1.3670e-01],\n",
       "           ...,\n",
       "           [-5.6754e-02, -3.6524e-01, -2.0708e-01,  ..., -3.1540e-01,\n",
       "            -1.1005e-01, -2.0986e-01],\n",
       "           [-1.0047e-01, -3.7752e-01, -2.0645e-01,  ..., -2.8003e-01,\n",
       "            -1.0525e-01, -1.2448e-01],\n",
       "           [-4.0838e-02, -3.4374e-01, -1.6584e-01,  ..., -3.0149e-01,\n",
       "            -1.1298e-01, -1.5893e-01]]]]),\n",
       " tensor([[[[ 3.5589e-03,  8.0485e-03,  6.9237e-03,  ...,  6.2562e-03,\n",
       "            -3.3959e-03,  2.8367e-03],\n",
       "           [ 4.1889e-02,  7.0577e-01,  5.0461e-01,  ...,  7.3274e-01,\n",
       "            -1.7958e-01,  2.4575e-01],\n",
       "           [ 1.6999e-02, -2.3096e-01, -4.1838e-02,  ...,  3.9395e-01,\n",
       "            -1.0428e-01,  1.1655e-01],\n",
       "           ...,\n",
       "           [-1.0348e-01,  3.7996e-01, -4.1865e-01,  ..., -4.4375e-01,\n",
       "             1.0469e-01,  1.5546e-01],\n",
       "           [-6.3205e-02,  3.0851e-01, -4.3887e-01,  ..., -3.7988e-01,\n",
       "             4.3704e-02,  1.8972e-01],\n",
       "           [-4.5725e-02,  3.2386e-01, -4.0312e-01,  ..., -4.3252e-01,\n",
       "             7.1141e-02,  1.8906e-01]],\n",
       " \n",
       "          [[-3.1021e-03, -2.1682e-02,  5.1232e-03,  ..., -1.1233e-02,\n",
       "            -1.9955e-02, -9.7872e-03],\n",
       "           [-1.2047e+00, -7.4384e-02,  5.4281e-01,  ...,  1.6718e-01,\n",
       "            -3.8175e-01, -2.8697e-01],\n",
       "           [-6.3033e-01,  7.1938e-01, -7.8718e-02,  ..., -3.2778e-01,\n",
       "            -4.4847e-01, -7.3570e-02],\n",
       "           ...,\n",
       "           [ 6.5375e-02,  5.6268e-01,  2.3479e-01,  ..., -2.6225e-01,\n",
       "             7.1529e-02, -1.5715e-01],\n",
       "           [ 6.8932e-02,  4.8634e-01,  2.7307e-01,  ..., -2.1053e-01,\n",
       "             7.9906e-02, -1.5993e-01],\n",
       "           [ 5.5360e-02,  6.0273e-01,  2.5011e-01,  ..., -2.5234e-01,\n",
       "             8.5244e-02, -1.1087e-01]],\n",
       " \n",
       "          [[ 4.5387e-03,  2.9455e-03, -4.1981e-03,  ...,  4.5143e-02,\n",
       "            -1.7408e-03,  1.7196e-03],\n",
       "           [-2.7468e-01, -4.1350e-01,  3.8761e-01,  ..., -1.1844e-01,\n",
       "            -2.6658e-01, -2.2384e-01],\n",
       "           [-3.2666e-01, -4.3395e-01,  7.7410e-02,  ..., -5.5578e-02,\n",
       "             1.6074e-01,  1.7407e-01],\n",
       "           ...,\n",
       "           [ 4.0615e-01,  1.1829e-01, -7.0517e-02,  ..., -1.2328e-01,\n",
       "            -6.2462e-01,  5.4826e-02],\n",
       "           [ 3.7424e-01,  6.5765e-02, -1.2647e-01,  ..., -4.9630e-02,\n",
       "            -6.1525e-01,  7.2044e-02],\n",
       "           [ 3.7967e-01,  1.2883e-01, -8.8706e-02,  ..., -9.9899e-02,\n",
       "            -6.0985e-01,  6.6222e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.4218e-03, -6.7475e-03,  6.4791e-02,  ...,  1.8487e-03,\n",
       "            -5.3828e-02, -1.9704e-02],\n",
       "           [ 8.4565e-01,  9.5181e-02,  2.1774e-01,  ..., -3.7936e-01,\n",
       "             1.4008e+00,  3.0803e-01],\n",
       "           [-5.2526e-01, -1.5401e-01, -3.1491e-01,  ..., -4.7839e-01,\n",
       "             3.2343e-01, -5.8326e-01],\n",
       "           ...,\n",
       "           [-1.2321e-01,  6.7583e-01,  5.7315e-02,  ..., -3.4018e-02,\n",
       "            -9.5786e-01, -5.8067e-02],\n",
       "           [-2.3013e-02,  7.5796e-01, -4.9465e-02,  ..., -4.1985e-02,\n",
       "            -9.4247e-01,  1.0657e-01],\n",
       "           [-9.1847e-02,  7.1630e-01,  6.0869e-02,  ..., -3.6198e-02,\n",
       "            -9.4399e-01, -2.5360e-02]],\n",
       " \n",
       "          [[-4.9424e-03,  1.8734e-03, -1.0714e-02,  ...,  2.4381e-02,\n",
       "            -8.2083e-03,  8.3759e-04],\n",
       "           [-9.3795e-02, -5.0670e-01,  1.6763e-01,  ...,  2.8837e-01,\n",
       "             5.4304e-01,  9.9046e-01],\n",
       "           [ 6.7881e-01,  2.6330e-01, -5.2422e-01,  ..., -3.0320e-01,\n",
       "            -1.0403e+00,  6.2036e-01],\n",
       "           ...,\n",
       "           [-3.4425e-01,  1.2230e-02, -7.8311e-02,  ...,  2.7302e-01,\n",
       "            -1.7547e-01,  4.3951e-01],\n",
       "           [-3.2165e-01,  9.0991e-04, -4.3241e-02,  ...,  3.0127e-01,\n",
       "            -2.2017e-01,  3.9990e-01],\n",
       "           [-2.6567e-01,  1.3195e-02, -4.2419e-02,  ...,  2.8267e-01,\n",
       "            -2.0254e-01,  4.2603e-01]],\n",
       " \n",
       "          [[ 4.8228e-03, -3.5043e-03,  1.6645e-02,  ..., -1.0085e-02,\n",
       "             1.8942e-02, -3.6313e-03],\n",
       "           [ 4.5668e-02,  7.0523e-01,  4.2599e-01,  ...,  4.6676e-01,\n",
       "            -1.3120e-01, -2.5867e-02],\n",
       "           [ 5.0074e-01,  6.2702e-01,  2.3452e-01,  ..., -9.7372e-02,\n",
       "            -6.8923e-01,  2.7157e-01],\n",
       "           ...,\n",
       "           [-3.0080e-01, -1.7028e-01, -1.8145e-01,  ..., -4.1149e-01,\n",
       "             6.8466e-02,  1.2572e-01],\n",
       "           [-3.1020e-01, -1.7569e-01, -1.1951e-01,  ..., -3.8312e-01,\n",
       "             8.9517e-02,  1.4487e-01],\n",
       "           [-3.3210e-01, -1.3805e-01, -1.5202e-01,  ..., -4.1663e-01,\n",
       "             8.9291e-02,  9.3917e-02]]]]),\n",
       " tensor([[[[ 2.0279e-03, -5.7763e-03, -5.9355e-03,  ...,  1.1843e-02,\n",
       "             3.6320e-03, -3.8685e-03],\n",
       "           [-1.4114e-01,  2.7383e-01,  4.9679e-01,  ...,  2.5571e-01,\n",
       "             2.3510e-01, -5.4470e-01],\n",
       "           [ 1.1721e-03, -1.9462e-01,  3.5607e-01,  ..., -5.0383e-01,\n",
       "            -1.5683e-01, -5.8710e-01],\n",
       "           ...,\n",
       "           [ 4.3533e-03,  2.0165e-01, -1.5626e-01,  ..., -3.6511e-02,\n",
       "            -1.2728e-01,  5.6297e-02],\n",
       "           [-1.6951e-02,  1.8103e-01, -1.1016e-01,  ..., -2.9244e-02,\n",
       "            -1.3537e-01,  8.6465e-02],\n",
       "           [-9.1084e-03,  2.0234e-01, -1.6227e-01,  ..., -1.0947e-02,\n",
       "            -1.7306e-01,  6.5071e-02]],\n",
       " \n",
       "          [[-7.1419e-03,  2.3108e-03, -9.5248e-03,  ...,  2.2170e-02,\n",
       "             8.9317e-04, -9.3054e-03],\n",
       "           [ 1.1396e-01, -5.6808e-01,  3.7183e-01,  ...,  5.4466e-01,\n",
       "             2.9692e-01,  2.0265e-01],\n",
       "           [ 4.4768e-01,  4.0656e-01,  2.2595e-01,  ...,  2.4002e-01,\n",
       "             4.4606e-02,  8.8433e-02],\n",
       "           ...,\n",
       "           [ 7.7928e-02, -5.7568e-01,  7.6445e-02,  ..., -1.1649e-02,\n",
       "            -2.4750e-01, -2.6538e-01],\n",
       "           [ 5.7727e-02, -5.2845e-01,  6.6440e-02,  ..., -7.9596e-02,\n",
       "            -2.4771e-01, -2.6300e-01],\n",
       "           [ 7.3565e-02, -5.9465e-01,  4.8529e-02,  ..., -8.2792e-02,\n",
       "            -2.9527e-01, -3.0218e-01]],\n",
       " \n",
       "          [[-8.1322e-03,  4.4398e-03, -9.1143e-03,  ..., -7.6004e-04,\n",
       "            -2.2097e-03,  1.1387e-02],\n",
       "           [ 1.8678e-01, -3.8884e-02,  6.1356e-02,  ..., -7.8900e-02,\n",
       "            -3.4925e-01,  3.0190e-01],\n",
       "           [-2.8179e-01,  6.3038e-01,  5.1384e-01,  ...,  8.1867e-02,\n",
       "            -5.5795e-01, -5.7423e-01],\n",
       "           ...,\n",
       "           [ 1.4786e-01,  1.9125e-01,  8.6687e-02,  ..., -2.5775e-01,\n",
       "             1.3251e-02, -4.5631e-01],\n",
       "           [ 1.5780e-01,  1.9351e-01,  1.4021e-01,  ..., -1.7960e-01,\n",
       "             4.1928e-03, -4.9082e-01],\n",
       "           [ 1.4139e-01,  1.9539e-01,  1.1078e-01,  ..., -2.2666e-01,\n",
       "            -3.5837e-03, -5.0132e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.6872e-03, -3.6873e-03, -1.2770e-02,  ..., -4.0980e-03,\n",
       "             1.0720e-03, -1.8202e-03],\n",
       "           [ 1.7210e-02,  7.6399e-02,  5.0202e-01,  ...,  1.6639e-01,\n",
       "            -6.0721e-02, -4.0821e-02],\n",
       "           [-1.2575e+00,  5.3891e-01, -2.6577e-01,  ...,  5.2137e-01,\n",
       "             1.5066e-01, -6.4110e-02],\n",
       "           ...,\n",
       "           [-1.0643e-02,  2.8403e-01,  3.6889e-01,  ..., -1.4837e-01,\n",
       "             2.3775e-01,  8.8068e-02],\n",
       "           [ 8.9610e-03,  2.8976e-01,  3.5059e-01,  ..., -5.1897e-02,\n",
       "             1.7483e-01,  9.1299e-02],\n",
       "           [ 1.8581e-02,  2.8862e-01,  3.8319e-01,  ..., -1.1751e-01,\n",
       "             1.9305e-01,  6.5100e-02]],\n",
       " \n",
       "          [[ 1.9437e-03,  2.3148e-03,  2.5073e-03,  ...,  4.2298e-03,\n",
       "            -8.5710e-03, -1.7307e-02],\n",
       "           [-1.4334e-02,  3.3439e-01, -1.2761e-01,  ...,  4.0350e-01,\n",
       "            -1.6132e-01, -4.0932e-01],\n",
       "           [ 4.4114e-01,  4.2447e-01,  1.6952e+00,  ..., -4.4774e-01,\n",
       "             1.1213e+00, -1.5842e+00],\n",
       "           ...,\n",
       "           [ 3.1339e-02, -6.2521e-02,  2.0111e-01,  ..., -2.0760e-01,\n",
       "             5.4762e-02,  2.6569e-01],\n",
       "           [ 3.1706e-02, -3.8193e-02,  1.7841e-01,  ..., -1.4087e-01,\n",
       "             6.4712e-02,  2.5248e-01],\n",
       "           [ 3.8242e-02, -3.8175e-02,  1.9283e-01,  ..., -2.0329e-01,\n",
       "             5.2164e-02,  2.4066e-01]],\n",
       " \n",
       "          [[ 8.0051e-03,  5.0437e-04,  6.5218e-03,  ..., -1.2772e-02,\n",
       "            -1.0065e-03, -1.4549e-03],\n",
       "           [-3.9790e-01, -3.1199e-01,  4.1520e-01,  ...,  9.2534e-01,\n",
       "            -4.8562e-02, -6.3233e-01],\n",
       "           [-1.3635e-01, -8.4311e-02,  1.8223e-01,  ...,  2.7141e-01,\n",
       "            -3.7257e-01, -5.9935e-01],\n",
       "           ...,\n",
       "           [ 1.3603e-01,  1.5040e-01, -3.8936e-01,  ..., -7.9235e-01,\n",
       "            -1.2491e-01, -1.9338e-01],\n",
       "           [ 1.0343e-01,  1.2630e-01, -3.8347e-01,  ..., -7.6360e-01,\n",
       "            -9.3255e-02, -1.9224e-01],\n",
       "           [ 1.1872e-01,  1.4244e-01, -3.8103e-01,  ..., -7.8816e-01,\n",
       "            -1.0779e-01, -2.0525e-01]]]]),\n",
       " tensor([[[[ 1.5553e-02, -1.5773e-02, -7.4722e-03,  ..., -1.2590e-02,\n",
       "            -1.7766e-02,  1.1946e-02],\n",
       "           [ 1.5515e-01, -1.3591e-01,  4.9296e-01,  ..., -1.9031e-01,\n",
       "             1.6810e-03,  2.7886e-01],\n",
       "           [-2.7259e-01,  1.8465e-01, -3.0790e-02,  ..., -8.7575e-02,\n",
       "            -1.6223e-01,  3.2109e-01],\n",
       "           ...,\n",
       "           [-3.9063e-01, -1.5382e-01, -2.9211e-01,  ..., -7.7298e-02,\n",
       "            -1.9420e-01, -6.0107e-02],\n",
       "           [-3.5316e-01, -1.9370e-01, -2.7719e-01,  ..., -4.6511e-02,\n",
       "            -1.9514e-01, -7.1556e-02],\n",
       "           [-4.2243e-01, -2.1098e-01, -2.6309e-01,  ..., -7.5863e-02,\n",
       "            -1.9885e-01, -7.2949e-02]],\n",
       " \n",
       "          [[ 1.1048e-02,  1.3030e-02, -2.8228e-03,  ...,  1.0714e-02,\n",
       "            -1.1907e-02,  1.0425e-02],\n",
       "           [-1.2213e-02,  2.4046e-01,  5.0196e-02,  ..., -4.2728e-01,\n",
       "             3.0448e-01,  8.3111e-01],\n",
       "           [-4.5571e-02, -7.2433e-01,  1.1190e+00,  ..., -7.2862e-01,\n",
       "            -1.0200e+00,  5.1369e-02],\n",
       "           ...,\n",
       "           [-3.1671e-01, -3.6786e-01, -3.4702e-01,  ..., -3.5719e-01,\n",
       "            -1.6097e-01,  3.4114e-01],\n",
       "           [-2.5947e-01, -4.9575e-01, -2.9622e-01,  ..., -3.7424e-01,\n",
       "            -1.6229e-01,  3.2291e-01],\n",
       "           [-2.6614e-01, -4.4304e-01, -3.3586e-01,  ..., -3.5187e-01,\n",
       "            -8.3101e-02,  3.0117e-01]],\n",
       " \n",
       "          [[ 1.5741e-02,  3.3709e-03, -4.1705e-03,  ..., -8.7973e-02,\n",
       "            -3.3142e-03, -2.8744e-03],\n",
       "           [-3.8915e-01, -3.0281e-01,  6.7247e-01,  ..., -1.0308e+00,\n",
       "            -3.2441e-01, -2.9770e-01],\n",
       "           [-3.7254e-01, -1.9418e-01, -3.2469e-01,  ..., -6.9653e-01,\n",
       "            -4.0093e-01, -1.1438e+00],\n",
       "           ...,\n",
       "           [-1.9638e-01,  7.0817e-01,  2.8424e-01,  ..., -8.9820e-01,\n",
       "            -2.8559e-01, -2.5706e-01],\n",
       "           [-1.0977e-01,  7.1822e-01,  3.6682e-01,  ..., -9.5671e-01,\n",
       "            -2.2092e-01, -2.4756e-01],\n",
       "           [-2.1305e-01,  7.1487e-01,  2.8518e-01,  ..., -8.6269e-01,\n",
       "            -2.8265e-01, -1.9611e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.7816e-02, -1.0023e-02, -3.2873e-03,  ..., -3.0137e-03,\n",
       "             3.1397e-03,  5.1641e-03],\n",
       "           [ 4.7472e-01, -1.5617e-01, -2.8182e-01,  ..., -2.4249e-01,\n",
       "             6.4045e-01,  2.7825e-01],\n",
       "           [ 2.3072e-02, -1.2973e+00,  5.2168e-01,  ...,  5.4032e-01,\n",
       "             3.7962e-01, -4.1673e-01],\n",
       "           ...,\n",
       "           [ 7.9688e-02, -3.0822e-02,  9.1284e-02,  ..., -1.4978e-01,\n",
       "             2.6528e-01,  1.2986e-02],\n",
       "           [ 1.1801e-01, -6.1233e-02,  8.2474e-02,  ..., -1.5946e-01,\n",
       "             2.6443e-01,  4.4570e-02],\n",
       "           [ 1.4888e-01, -5.2948e-03,  8.9085e-02,  ..., -1.7518e-01,\n",
       "             2.8600e-01,  6.3969e-02]],\n",
       " \n",
       "          [[-1.1985e-02, -9.1748e-03, -3.2859e-03,  ..., -1.6245e-03,\n",
       "            -1.8521e-03, -9.3664e-03],\n",
       "           [ 4.5769e-01,  3.8308e-01, -5.8004e-01,  ..., -5.0227e-01,\n",
       "             4.3945e-01,  2.7080e-01],\n",
       "           [-3.7486e-01,  2.7408e-01,  4.8906e-01,  ..., -4.6493e-01,\n",
       "            -1.6236e-01,  9.9187e-02],\n",
       "           ...,\n",
       "           [-3.8517e-01,  8.1437e-02,  2.3457e-01,  ...,  3.1046e-01,\n",
       "            -7.6077e-01,  3.2206e-01],\n",
       "           [-4.7764e-01,  2.3787e-01,  1.8439e-01,  ...,  3.4153e-01,\n",
       "            -6.9836e-01,  4.4560e-01],\n",
       "           [-3.6525e-01,  1.1617e-01,  1.2677e-01,  ...,  3.6908e-01,\n",
       "            -8.6701e-01,  2.6685e-01]],\n",
       " \n",
       "          [[ 4.5234e-04, -7.3946e-03, -9.1317e-02,  ...,  2.9789e-03,\n",
       "            -4.2171e-03, -3.8997e-04],\n",
       "           [-6.4550e-02,  2.0245e-01,  7.2241e-01,  ...,  2.3818e-01,\n",
       "            -2.4161e-01,  1.0731e-01],\n",
       "           [-8.1100e-01, -1.8002e-01,  8.8426e-01,  ..., -4.9786e-02,\n",
       "            -7.6918e-01,  4.3046e-01],\n",
       "           ...,\n",
       "           [-6.3174e-03,  9.4671e-02, -3.6218e-01,  ...,  7.5905e-01,\n",
       "             3.2431e-02, -7.6616e-02],\n",
       "           [-1.7564e-03,  8.8093e-02, -3.2598e-01,  ...,  7.9699e-01,\n",
       "             7.9467e-02, -7.7619e-02],\n",
       "           [ 3.1269e-03,  9.2806e-02, -3.2458e-01,  ...,  7.7961e-01,\n",
       "             5.7133e-02, -6.9074e-02]]]]),\n",
       " tensor([[[[ 1.2010e-02, -1.3329e-03, -1.9146e-02,  ...,  8.3896e-03,\n",
       "             7.8966e-03, -7.1783e-03],\n",
       "           [-8.4518e-02, -7.1861e-02,  7.2761e-02,  ..., -1.6959e-01,\n",
       "            -1.8177e-01,  1.2508e-01],\n",
       "           [ 1.7181e-01, -2.4199e-01,  3.6996e-01,  ..., -8.1610e-01,\n",
       "             1.6959e-01,  8.3440e-01],\n",
       "           ...,\n",
       "           [ 4.4875e-02, -3.9920e-01,  3.1142e-01,  ...,  2.3554e-02,\n",
       "             2.5805e-01,  2.1404e-01],\n",
       "           [ 5.0038e-02, -3.6884e-01,  3.3258e-01,  ..., -2.7641e-02,\n",
       "             2.3726e-01,  1.9811e-01],\n",
       "           [ 7.6830e-03, -3.8368e-01,  3.0459e-01,  ...,  8.0006e-03,\n",
       "             2.4454e-01,  2.1584e-01]],\n",
       " \n",
       "          [[-4.2978e-04,  3.1558e-02,  1.8638e-03,  ..., -8.2754e-03,\n",
       "            -1.2864e-02, -1.6673e-02],\n",
       "           [-2.5539e-01, -2.1944e-02, -1.7019e-01,  ...,  1.2440e+00,\n",
       "            -4.5016e-01,  2.6456e-01],\n",
       "           [-1.4013e+00, -1.0247e+00,  7.2071e-01,  ...,  6.7606e-01,\n",
       "             2.4555e-01,  4.4085e-01],\n",
       "           ...,\n",
       "           [-1.0331e-01, -1.4732e-01,  8.4603e-02,  ..., -2.6280e-01,\n",
       "             8.5626e-02,  4.5759e-01],\n",
       "           [-7.2204e-02, -1.2621e-01,  3.2098e-02,  ..., -2.6055e-01,\n",
       "             1.8315e-01,  5.3218e-01],\n",
       "           [-5.1024e-02, -1.6185e-01,  6.6016e-02,  ..., -3.0364e-01,\n",
       "             1.3406e-01,  5.0313e-01]],\n",
       " \n",
       "          [[ 1.2038e-02,  1.5591e-02, -1.7662e-02,  ...,  9.7311e-03,\n",
       "             5.2840e-03,  2.8037e-02],\n",
       "           [ 7.5270e-01, -9.3890e-01,  1.7159e-01,  ..., -4.0137e-01,\n",
       "            -6.4711e-01,  1.3011e-01],\n",
       "           [ 1.8334e-01,  5.8638e-01,  4.8570e-01,  ..., -7.4079e-02,\n",
       "            -6.7126e-02, -5.1861e-01],\n",
       "           ...,\n",
       "           [ 6.1393e-02, -5.2896e-01, -1.7748e+00,  ...,  7.3407e-01,\n",
       "             7.1351e-01, -1.7645e-01],\n",
       "           [ 3.9803e-02, -4.6978e-01, -1.7867e+00,  ...,  6.8337e-01,\n",
       "             7.5032e-01, -1.6420e-01],\n",
       "           [ 4.3628e-02, -5.2505e-01, -1.7674e+00,  ...,  7.2010e-01,\n",
       "             7.1689e-01, -1.6947e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.8746e-02, -1.5950e-02, -3.4733e-03,  ..., -1.3444e-02,\n",
       "            -1.7260e-03, -1.2768e-03],\n",
       "           [ 9.6673e-01, -1.4041e-01,  9.0303e-01,  ..., -1.9206e-01,\n",
       "            -3.5121e-01, -3.5054e-02],\n",
       "           [ 7.7811e-01,  4.8272e-01,  1.3185e+00,  ..., -8.1062e-01,\n",
       "            -8.6377e-01, -4.8370e-01],\n",
       "           ...,\n",
       "           [ 2.1933e-01,  7.1993e-01,  1.1928e-01,  ...,  2.0967e-01,\n",
       "             1.2502e-01,  1.2030e-01],\n",
       "           [ 2.2872e-01,  7.4844e-01,  2.1821e-01,  ...,  1.0827e-01,\n",
       "             9.9772e-02,  1.1837e-01],\n",
       "           [ 2.1499e-01,  7.8463e-01,  1.9268e-01,  ...,  2.5690e-01,\n",
       "             1.1201e-01,  1.6246e-01]],\n",
       " \n",
       "          [[-5.7910e-03, -5.7165e-03,  1.3684e-02,  ..., -3.4225e-02,\n",
       "             1.1386e-02,  3.1293e-03],\n",
       "           [ 9.2432e-01, -1.9831e-01, -1.1067e-01,  ..., -2.7740e-01,\n",
       "            -1.4012e-01,  2.8315e-01],\n",
       "           [ 5.6256e-01, -1.6658e-01,  5.0323e-01,  ...,  2.6285e-01,\n",
       "             6.0382e-01, -2.8885e-01],\n",
       "           ...,\n",
       "           [ 1.9531e-01,  4.3034e-01, -5.5471e-02,  ..., -1.5588e-01,\n",
       "            -3.8412e-01, -3.7166e-01],\n",
       "           [ 2.0166e-01,  4.9330e-01,  1.0136e-02,  ..., -8.0247e-02,\n",
       "            -2.5553e-01, -3.3861e-01],\n",
       "           [ 1.9736e-01,  4.7119e-01, -1.0303e-01,  ..., -1.8621e-01,\n",
       "            -3.3669e-01, -3.7153e-01]],\n",
       " \n",
       "          [[-2.1798e-03,  1.6569e-02,  9.5145e-03,  ...,  1.1007e-02,\n",
       "             4.3387e-03, -9.8372e-03],\n",
       "           [ 1.1212e+00, -1.7106e-01, -3.6027e-01,  ..., -6.1272e-01,\n",
       "            -3.4477e-01, -2.8715e-01],\n",
       "           [ 7.1365e-01, -4.7601e-01, -1.4845e-01,  ...,  5.3322e-01,\n",
       "            -2.4398e-01,  1.4932e-01],\n",
       "           ...,\n",
       "           [-4.1912e-02,  9.8802e-02, -1.5453e-03,  ..., -2.8399e-01,\n",
       "            -6.8698e-01,  1.1150e-01],\n",
       "           [-2.4208e-01, -7.2966e-02, -1.7028e-01,  ..., -3.9625e-01,\n",
       "            -6.7319e-01,  1.4687e-01],\n",
       "           [-1.3624e-01,  5.8524e-02, -6.2573e-02,  ..., -3.4768e-01,\n",
       "            -6.0966e-01,  7.2410e-02]]]]),\n",
       " tensor([[[[-1.3289e-03, -4.1340e-03,  5.3878e-03,  ...,  3.7090e-03,\n",
       "            -7.6517e-03,  1.1949e-03],\n",
       "           [ 2.4682e-01,  6.5692e-01,  3.7071e-02,  ..., -1.0774e-01,\n",
       "            -4.1762e-01,  3.6914e-01],\n",
       "           [ 1.5070e-01,  4.1308e-01, -9.7093e-01,  ..., -6.2118e-01,\n",
       "            -1.4092e-01,  1.1264e-02],\n",
       "           ...,\n",
       "           [-5.5724e-01, -3.7290e-01, -1.2799e-01,  ...,  1.4977e-01,\n",
       "             6.0400e-01, -7.0364e-04],\n",
       "           [-5.4628e-01, -3.8230e-01, -1.9440e-01,  ...,  1.5839e-01,\n",
       "             5.6752e-01, -3.8376e-02],\n",
       "           [-5.2317e-01, -3.8485e-01, -1.1169e-01,  ...,  1.7125e-01,\n",
       "             5.7276e-01,  1.4913e-03]],\n",
       " \n",
       "          [[ 3.2093e-02, -1.4582e-04,  2.3722e-02,  ..., -9.9949e-03,\n",
       "             3.9061e-03, -7.6910e-03],\n",
       "           [ 4.6824e-01,  9.6753e-01, -1.4578e-01,  ..., -3.2956e-01,\n",
       "            -5.1840e-01, -6.4735e-01],\n",
       "           [-2.2919e-01,  3.9329e-01, -3.2980e-02,  ..., -4.4708e-01,\n",
       "            -6.2378e-01, -1.1751e+00],\n",
       "           ...,\n",
       "           [-2.6734e-01, -3.7881e-01,  6.6253e-01,  ...,  5.2408e-01,\n",
       "             2.3835e-01,  1.1503e+00],\n",
       "           [-5.0712e-02, -4.6359e-01,  5.9363e-01,  ...,  6.1209e-01,\n",
       "             2.7458e-01,  1.1913e+00],\n",
       "           [-2.5809e-01, -3.7754e-01,  7.1168e-01,  ...,  4.8090e-01,\n",
       "             2.3994e-01,  1.1832e+00]],\n",
       " \n",
       "          [[ 3.1032e-03, -1.9565e-03, -3.6762e-03,  ...,  9.2958e-03,\n",
       "            -1.0225e-02,  4.2748e-03],\n",
       "           [-2.4198e-01, -5.8766e-01,  9.3719e-04,  ..., -1.2840e-01,\n",
       "            -3.6180e-01,  9.3465e-02],\n",
       "           [-8.3411e-01, -3.4507e-01, -6.1031e-01,  ..., -4.5606e-02,\n",
       "             5.7847e-01,  1.7808e-01],\n",
       "           ...,\n",
       "           [ 1.1262e+00, -7.8234e-01,  4.6156e-01,  ..., -4.9647e-01,\n",
       "             3.1833e-02,  7.0251e-01],\n",
       "           [ 1.0923e+00, -9.5624e-01,  5.6264e-01,  ..., -6.2303e-01,\n",
       "             1.2622e-02,  7.4400e-01],\n",
       "           [ 1.0191e+00, -8.2112e-01,  4.7932e-01,  ..., -4.7586e-01,\n",
       "             4.4734e-02,  7.3392e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.4747e-02, -2.2235e-03,  4.6928e-04,  ..., -5.3029e-03,\n",
       "             3.0213e-02,  3.0894e-03],\n",
       "           [ 8.1817e-02, -4.5988e-01, -2.0840e+00,  ...,  1.1152e+00,\n",
       "             3.9089e-01, -1.4417e+00],\n",
       "           [ 2.2267e-02, -3.4620e-01,  5.9298e-01,  ...,  1.8207e+00,\n",
       "             5.2893e-01,  7.1481e-01],\n",
       "           ...,\n",
       "           [-2.5921e+00,  5.6042e-01,  3.5262e-01,  ..., -5.6470e-01,\n",
       "             7.6005e-01,  3.2061e-01],\n",
       "           [-2.6747e+00,  6.2909e-01,  3.5154e-01,  ..., -4.5286e-01,\n",
       "             8.0832e-01,  1.2828e-01],\n",
       "           [-2.5740e+00,  5.6219e-01,  2.9771e-01,  ..., -5.1211e-01,\n",
       "             8.2942e-01,  3.4072e-01]],\n",
       " \n",
       "          [[ 8.5317e-03, -1.3835e-02,  1.9677e-06,  ..., -5.3441e-03,\n",
       "            -9.4021e-04,  1.9729e-04],\n",
       "           [-3.3029e-01, -6.5643e-01, -2.9565e-01,  ..., -2.6871e-02,\n",
       "            -2.1979e-01, -2.0724e-01],\n",
       "           [-3.0453e-01, -6.9807e-01, -6.6232e-02,  ...,  1.1643e-01,\n",
       "            -2.4894e-01, -3.8307e-01],\n",
       "           ...,\n",
       "           [ 1.3255e-01,  4.8773e-01,  3.9934e-01,  ...,  1.1525e-01,\n",
       "             4.6962e-01, -5.8233e-01],\n",
       "           [ 1.5598e-01,  5.7296e-01,  3.2706e-01,  ...,  1.2800e-01,\n",
       "             4.7243e-01, -6.0206e-01],\n",
       "           [ 1.0830e-01,  4.5931e-01,  3.7913e-01,  ...,  7.4173e-02,\n",
       "             4.4538e-01, -5.7660e-01]],\n",
       " \n",
       "          [[-9.5433e-03,  1.2307e-02, -6.6302e-03,  ...,  9.4651e-04,\n",
       "            -9.4756e-03, -7.3861e-03],\n",
       "           [ 2.7264e-01, -6.8592e-01, -3.8735e-01,  ...,  1.8799e-01,\n",
       "             2.5607e-01,  3.0524e-01],\n",
       "           [ 5.5619e-01,  4.2699e-01,  3.8254e-01,  ..., -7.9566e-02,\n",
       "             5.0110e-04, -7.8458e-01],\n",
       "           ...,\n",
       "           [ 6.5367e-01,  6.4134e-01, -4.5866e-01,  ..., -7.6964e-01,\n",
       "             1.7196e-01,  1.8001e-01],\n",
       "           [ 7.0494e-01,  5.7291e-01, -4.9565e-01,  ..., -7.3991e-01,\n",
       "             1.6834e-01,  1.6842e-01],\n",
       "           [ 6.5237e-01,  6.2253e-01, -4.1939e-01,  ..., -8.1228e-01,\n",
       "             2.2857e-01,  1.3439e-01]]]]),\n",
       " tensor([[[[-8.0993e-03,  1.6182e-02, -3.3738e-03,  ..., -5.1410e-03,\n",
       "             4.9255e-03,  1.2519e-02],\n",
       "           [-2.8218e-01,  1.1804e+00, -2.8950e-01,  ..., -9.2833e-02,\n",
       "             4.6385e-01,  1.8375e-01],\n",
       "           [-4.9091e-01, -2.2088e-01, -3.6942e-01,  ...,  1.7831e-02,\n",
       "             2.8191e-01,  3.2528e-01],\n",
       "           ...,\n",
       "           [ 2.0064e-01,  2.8122e-01,  3.8672e-01,  ..., -5.1752e-01,\n",
       "            -3.1398e-01, -7.3163e-01],\n",
       "           [ 1.4329e-01,  2.6259e-01,  4.9127e-01,  ..., -5.2235e-01,\n",
       "            -4.1865e-01, -8.0575e-01],\n",
       "           [ 1.8951e-01,  3.1135e-01,  4.0164e-01,  ..., -5.2321e-01,\n",
       "            -3.0274e-01, -7.1873e-01]],\n",
       " \n",
       "          [[-1.7711e-02, -2.0878e-03, -1.1093e-02,  ...,  1.1912e-02,\n",
       "             1.0761e-03,  1.2681e-03],\n",
       "           [ 1.9416e-01,  9.8870e-01, -2.5789e-01,  ...,  3.2445e-01,\n",
       "             4.9554e-02,  6.9335e-02],\n",
       "           [ 8.1284e-01,  5.2393e-01, -4.7333e-01,  ...,  3.2185e-02,\n",
       "            -1.0296e+00, -3.0549e-01],\n",
       "           ...,\n",
       "           [ 3.5691e-01, -5.2502e-01,  1.3456e-01,  ..., -2.6537e-01,\n",
       "             3.8863e-01, -1.7284e-01],\n",
       "           [ 3.5126e-01, -4.7319e-01,  2.3582e-01,  ..., -3.0628e-01,\n",
       "             3.5433e-01, -1.6863e-01],\n",
       "           [ 3.6609e-01, -5.2470e-01,  1.1992e-01,  ..., -2.7342e-01,\n",
       "             3.5275e-01, -1.9469e-01]],\n",
       " \n",
       "          [[-2.7652e-03,  1.2242e-02,  5.3652e-03,  ..., -6.2347e-03,\n",
       "             7.7748e-03,  1.2296e-02],\n",
       "           [-2.7747e-01,  4.5357e-01, -5.6373e-01,  ..., -4.4318e-01,\n",
       "            -4.2584e-01, -1.3091e+00],\n",
       "           [-4.8956e-01, -1.0012e-01, -1.0741e-01,  ..., -6.9846e-01,\n",
       "            -1.0049e-01, -4.6408e-01],\n",
       "           ...,\n",
       "           [-7.3793e-01, -1.5899e-02, -1.3168e-01,  ...,  1.3432e-02,\n",
       "             3.1015e-01, -1.8516e-01],\n",
       "           [-8.5325e-01, -1.2962e-01, -9.3222e-02,  ..., -3.3187e-02,\n",
       "             2.5927e-01, -1.6077e-01],\n",
       "           [-7.4292e-01, -8.8806e-02, -1.2433e-01,  ...,  1.4364e-02,\n",
       "             3.7079e-01, -1.9466e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.5274e-03, -1.7334e-03,  1.1445e-03,  ..., -2.9407e-02,\n",
       "            -5.6940e-03, -2.3393e-03],\n",
       "           [ 8.9135e-01,  1.4117e+00,  4.1708e-02,  ...,  6.8688e-01,\n",
       "             5.6697e-02, -6.5246e-01],\n",
       "           [ 7.5688e-01, -4.9202e-01, -7.2997e-01,  ...,  3.5935e-01,\n",
       "             5.9775e-01, -1.1156e+00],\n",
       "           ...,\n",
       "           [ 3.4474e-01, -4.8582e-01,  4.3373e-01,  ...,  4.8043e-02,\n",
       "            -3.3467e-01, -1.3404e-01],\n",
       "           [ 1.4793e-01, -9.0496e-01,  4.9414e-01,  ..., -1.5388e-01,\n",
       "            -2.6854e-01, -6.5147e-02],\n",
       "           [ 3.8245e-01, -4.7552e-01,  4.6800e-01,  ...,  9.9544e-02,\n",
       "            -3.0077e-01, -8.5541e-02]],\n",
       " \n",
       "          [[-9.1667e-03,  3.9219e-03,  4.2380e-03,  ...,  1.3534e-03,\n",
       "             5.4164e-03,  1.7199e-02],\n",
       "           [ 1.5753e+00,  9.3184e-01, -3.4276e-01,  ..., -9.4536e-02,\n",
       "            -4.2245e-01, -5.2985e-01],\n",
       "           [ 4.2570e-01,  5.9548e-01,  4.5577e-01,  ..., -3.2691e-01,\n",
       "            -8.3510e-02,  7.4675e-02],\n",
       "           ...,\n",
       "           [-8.4203e-03, -2.2709e-01,  2.7957e-01,  ...,  1.5709e-01,\n",
       "             1.0028e-01, -6.2147e-01],\n",
       "           [ 1.1680e-01, -1.4602e-01,  3.9609e-01,  ...,  2.7089e-01,\n",
       "            -8.7041e-03, -4.8217e-01],\n",
       "           [ 1.0116e-03, -2.1046e-01,  2.7436e-01,  ...,  1.5968e-01,\n",
       "             1.0287e-01, -6.4763e-01]],\n",
       " \n",
       "          [[ 1.0680e-02, -2.9921e-02, -1.8020e-02,  ...,  6.4888e-03,\n",
       "            -2.0180e-02,  1.1503e-03],\n",
       "           [ 7.9796e-01,  6.2461e-01, -6.7804e-01,  ...,  3.0605e-01,\n",
       "            -1.5276e-01,  4.5491e-01],\n",
       "           [-3.5419e-02, -1.5334e-01, -4.2327e-01,  ...,  5.4541e-01,\n",
       "            -4.2953e-01, -5.0004e-01],\n",
       "           ...,\n",
       "           [ 1.2629e-01, -2.9625e-02, -2.9224e-01,  ...,  1.1147e-01,\n",
       "             1.7203e-01, -5.2038e-01],\n",
       "           [ 1.5418e-01, -1.5379e-01, -3.3849e-01,  ...,  1.8667e-01,\n",
       "             2.3518e-01, -6.2181e-01],\n",
       "           [ 1.2585e-01, -6.4495e-02, -2.7723e-01,  ...,  7.9385e-02,\n",
       "             1.6119e-01, -5.2568e-01]]]]),\n",
       " tensor([[[[-1.1880e-02,  2.2670e-02,  4.4029e-03,  ..., -3.3866e-02,\n",
       "             8.3311e-03, -1.4538e-02],\n",
       "           [ 4.1058e-01,  8.7891e-01,  8.3739e-01,  ...,  4.1151e-01,\n",
       "            -4.0111e-01, -1.6157e-01],\n",
       "           [ 1.0492e+00,  5.5473e-01, -1.7877e-01,  ..., -2.1681e-02,\n",
       "             4.8799e-01,  2.5046e-01],\n",
       "           ...,\n",
       "           [-2.5502e-01,  7.3720e-01, -2.9464e-01,  ..., -6.2870e-03,\n",
       "             3.6740e-02,  5.9553e-01],\n",
       "           [-3.1480e-01,  8.2345e-01, -1.9319e-01,  ...,  5.1908e-03,\n",
       "             1.2430e-01,  4.1947e-01],\n",
       "           [-2.5972e-01,  7.8105e-01, -2.9771e-01,  ...,  1.2563e-02,\n",
       "             1.4919e-01,  6.0599e-01]],\n",
       " \n",
       "          [[-2.2202e-02,  5.1478e-03,  4.9518e-03,  ...,  1.9860e-02,\n",
       "            -2.7535e-03,  4.5588e-03],\n",
       "           [ 3.9990e-01, -9.0854e-02,  8.3114e-02,  ...,  1.3344e-01,\n",
       "             7.2958e-01,  3.7991e-01],\n",
       "           [ 3.5943e-01,  7.3996e-01,  8.2089e-01,  ...,  4.7139e-01,\n",
       "            -4.6063e-01,  4.7690e-01],\n",
       "           ...,\n",
       "           [-4.1985e-02,  2.3954e-01,  8.1416e-01,  ...,  1.0463e+00,\n",
       "             2.8999e-01,  1.8236e-01],\n",
       "           [-9.5806e-02,  2.4101e-01,  8.6054e-01,  ...,  1.1266e+00,\n",
       "             2.5527e-01,  1.2662e-01],\n",
       "           [-5.6892e-02,  1.8353e-01,  8.4023e-01,  ...,  1.0543e+00,\n",
       "             2.3053e-01,  1.7581e-01]],\n",
       " \n",
       "          [[ 8.8266e-04, -1.0788e-03,  1.3745e-02,  ...,  2.5063e-02,\n",
       "            -1.1969e-02,  6.6412e-03],\n",
       "           [ 2.8159e-01, -4.6210e-01, -1.9029e-01,  ...,  2.2110e-01,\n",
       "             7.2808e-01, -5.9390e-01],\n",
       "           [ 5.2957e-01,  2.4911e-01,  8.1271e-01,  ...,  7.2832e-02,\n",
       "             4.9751e-01, -2.5954e-01],\n",
       "           ...,\n",
       "           [ 6.6803e-01, -3.8576e-01, -4.4324e-01,  ...,  1.0095e+00,\n",
       "            -4.3312e-01,  1.1681e-01],\n",
       "           [ 7.0479e-01, -4.7698e-01, -4.0220e-01,  ...,  9.9864e-01,\n",
       "            -5.2658e-01,  2.3026e-01],\n",
       "           [ 6.7626e-01, -3.8609e-01, -4.3284e-01,  ...,  1.0130e+00,\n",
       "            -4.3814e-01,  1.4922e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.0320e-02,  1.7041e-02,  1.7667e-02,  ...,  3.2918e-03,\n",
       "            -1.1626e-02,  2.4887e-02],\n",
       "           [ 6.6609e-01, -8.0633e-02,  3.3159e-03,  ...,  5.2333e-01,\n",
       "             3.1802e-01,  5.3790e-01],\n",
       "           [ 1.3719e-01, -5.0441e-02,  5.9012e-02,  ...,  1.2203e+00,\n",
       "            -2.4105e-01, -7.6471e-01],\n",
       "           ...,\n",
       "           [ 5.1699e-01, -4.3218e-01,  4.3452e-01,  ...,  9.2237e-02,\n",
       "             2.8188e-02,  5.2049e-01],\n",
       "           [ 5.6540e-01, -3.0428e-01,  5.2072e-01,  ...,  1.4252e-01,\n",
       "            -2.9372e-02,  6.9590e-01],\n",
       "           [ 5.5151e-01, -4.0696e-01,  4.3039e-01,  ...,  4.3736e-02,\n",
       "             2.5543e-02,  5.0518e-01]],\n",
       " \n",
       "          [[-2.8090e-02,  2.3271e-02, -2.0974e-02,  ..., -2.1858e-02,\n",
       "            -4.4570e-02,  8.3596e-03],\n",
       "           [ 1.0850e+00, -6.7943e-01, -5.0924e-01,  ..., -1.1265e-02,\n",
       "             4.0546e-01, -9.6798e-01],\n",
       "           [-1.7642e-01, -4.8534e-01, -7.1164e-01,  ..., -2.2034e-01,\n",
       "             3.6869e-01, -7.5706e-01],\n",
       "           ...,\n",
       "           [ 8.6624e-02, -7.6346e-01, -9.7869e-01,  ...,  9.3478e-02,\n",
       "             6.1984e-01,  5.3275e-01],\n",
       "           [ 5.9496e-02, -6.8235e-01, -1.0940e+00,  ...,  7.7084e-02,\n",
       "             5.5069e-01,  6.6014e-01],\n",
       "           [ 8.4349e-02, -8.2086e-01, -9.6579e-01,  ...,  1.0561e-01,\n",
       "             6.3486e-01,  4.9105e-01]],\n",
       " \n",
       "          [[-7.2157e-03,  1.9875e-03, -1.1705e-02,  ...,  2.3730e-03,\n",
       "            -7.1342e-03, -7.2208e-03],\n",
       "           [ 2.4428e-01,  4.3187e-01, -1.8924e-01,  ...,  3.3376e-01,\n",
       "            -1.1357e+00,  4.3430e-02],\n",
       "           [ 7.6945e-01,  8.0456e-02,  1.4532e-01,  ..., -1.5315e-01,\n",
       "            -9.7192e-01,  7.1824e-01],\n",
       "           ...,\n",
       "           [-7.7439e-01,  7.1455e-01, -1.3016e-01,  ...,  7.4727e-01,\n",
       "             5.5104e-01, -3.3351e-01],\n",
       "           [-7.7538e-01,  5.7314e-01, -7.4161e-02,  ...,  7.4816e-01,\n",
       "             5.0085e-01, -1.5125e-01],\n",
       "           [-8.1027e-01,  7.5862e-01, -2.1157e-01,  ...,  7.0825e-01,\n",
       "             6.5406e-01, -3.7940e-01]]]]),\n",
       " tensor([[[[-2.4458e-02, -1.2470e-03,  2.9792e-02,  ..., -3.2385e-02,\n",
       "            -8.6230e-03,  4.0525e-02],\n",
       "           [ 1.2863e-01, -3.4035e-02, -9.1121e-01,  ...,  1.0706e+00,\n",
       "            -3.8756e-01,  7.7599e-01],\n",
       "           [ 5.6389e-01, -2.6135e-02, -5.2619e-01,  ...,  1.0942e-01,\n",
       "             3.7496e-01,  1.3271e-01],\n",
       "           ...,\n",
       "           [-1.7484e-01,  5.6377e-01, -4.2311e-02,  ..., -2.6192e-01,\n",
       "            -1.5874e-01, -8.1685e-01],\n",
       "           [-1.7020e-01,  6.3322e-01,  2.4706e-02,  ..., -3.2358e-01,\n",
       "            -2.5609e-01, -9.7810e-01],\n",
       "           [-1.8713e-01,  5.4497e-01, -5.4739e-02,  ..., -2.1674e-01,\n",
       "            -1.1553e-01, -8.8005e-01]],\n",
       " \n",
       "          [[-9.4001e-03,  2.3188e-02, -1.5422e-02,  ..., -3.0188e-03,\n",
       "            -4.1482e-03,  2.3019e-02],\n",
       "           [ 7.6397e-01, -6.6622e-01,  1.8792e-01,  ...,  3.8122e-01,\n",
       "             8.8963e-01, -7.3473e-01],\n",
       "           [ 2.2364e+00,  3.2520e-01,  3.3075e-01,  ..., -4.4522e-01,\n",
       "            -3.7034e-01,  8.3636e-01],\n",
       "           ...,\n",
       "           [ 8.0103e-01, -6.8243e-01,  2.6564e-01,  ...,  2.6399e-01,\n",
       "            -2.9230e-01,  4.9990e-01],\n",
       "           [ 8.1960e-01, -6.8560e-01,  2.5539e-01,  ...,  3.2816e-01,\n",
       "            -1.6871e-01,  4.2765e-01],\n",
       "           [ 8.0967e-01, -6.5307e-01,  2.5197e-01,  ...,  2.7077e-01,\n",
       "            -2.4608e-01,  5.2454e-01]],\n",
       " \n",
       "          [[ 2.8747e-03,  1.5951e-02, -2.0435e-02,  ...,  9.0871e-03,\n",
       "             1.3654e-03, -3.5173e-03],\n",
       "           [-6.5399e-01, -3.9515e-01, -2.6083e-01,  ..., -3.2249e-04,\n",
       "             1.4578e-01, -2.3616e-01],\n",
       "           [ 2.6713e-01,  4.9016e-01, -9.5254e-01,  ..., -9.2725e-02,\n",
       "             7.3510e-01,  5.5954e-01],\n",
       "           ...,\n",
       "           [ 3.1997e-02, -3.8638e-01,  3.8315e-01,  ..., -3.0777e-01,\n",
       "            -1.1801e+00,  1.0385e+00],\n",
       "           [-2.1639e-01, -3.5419e-01,  2.1663e-01,  ..., -2.2426e-01,\n",
       "            -1.0419e+00,  1.1045e+00],\n",
       "           [ 3.1996e-02, -4.2746e-01,  4.1520e-01,  ..., -2.8409e-01,\n",
       "            -1.1448e+00,  1.0331e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.6240e-02,  6.3263e-03, -1.0205e-02,  ...,  3.4314e-02,\n",
       "             7.7163e-03,  1.1884e-02],\n",
       "           [-1.6960e-01, -4.4480e-01,  3.0874e-01,  ..., -4.4465e-01,\n",
       "             1.9657e-01, -2.2649e-01],\n",
       "           [-2.2081e-01,  5.6459e-02,  1.0676e+00,  ..., -5.8113e-01,\n",
       "            -5.5280e-01,  1.7947e-01],\n",
       "           ...,\n",
       "           [-1.4235e+00,  1.2457e+00,  9.2080e-01,  ...,  7.2375e-01,\n",
       "             1.1138e+00, -8.5696e-02],\n",
       "           [-1.6247e+00,  1.0383e+00,  9.3757e-01,  ...,  9.2640e-01,\n",
       "             1.2068e+00, -6.2110e-02],\n",
       "           [-1.4467e+00,  1.2412e+00,  8.6963e-01,  ...,  7.8574e-01,\n",
       "             1.1063e+00, -8.9196e-02]],\n",
       " \n",
       "          [[-1.0058e-02,  2.0781e-02, -4.2402e-02,  ...,  5.3503e-03,\n",
       "            -1.0982e-02, -1.0635e-02],\n",
       "           [-6.3449e-01,  1.1098e-01,  5.8769e-02,  ...,  4.8173e-02,\n",
       "             1.6132e+00, -1.0370e+00],\n",
       "           [ 4.9431e-01, -4.5080e-01,  8.1440e-03,  ..., -6.8672e-02,\n",
       "             4.1390e-01, -7.4810e-01],\n",
       "           ...,\n",
       "           [ 1.4206e+00, -2.7519e-01,  5.1082e-01,  ...,  8.7962e-02,\n",
       "            -1.2236e-01,  8.3309e-01],\n",
       "           [ 1.4508e+00, -3.9263e-01,  4.1216e-01,  ..., -5.2702e-02,\n",
       "            -9.6392e-02,  8.3122e-01],\n",
       "           [ 1.3818e+00, -3.0860e-01,  5.2209e-01,  ...,  5.4443e-02,\n",
       "            -1.2785e-01,  8.1565e-01]],\n",
       " \n",
       "          [[ 6.7808e-03, -2.8603e-03, -1.2967e-02,  ...,  2.6759e-03,\n",
       "             3.9706e-03, -4.1930e-03],\n",
       "           [ 1.4155e+00, -3.8043e-01, -9.6734e-01,  ..., -8.8791e-01,\n",
       "             8.6100e-01, -3.9677e-01],\n",
       "           [ 3.5721e-01,  2.8495e-02, -8.3066e-01,  ..., -3.9120e-01,\n",
       "            -6.0311e-02,  1.1474e+00],\n",
       "           ...,\n",
       "           [ 1.1248e+00, -2.3146e-01,  1.7734e-01,  ...,  6.5838e-01,\n",
       "             3.0246e-01, -5.4757e-01],\n",
       "           [ 1.1545e+00,  1.2415e-02,  3.1294e-02,  ...,  7.3676e-01,\n",
       "             3.1907e-01, -5.6404e-01],\n",
       "           [ 1.0721e+00, -2.2422e-01,  1.7779e-01,  ...,  6.2983e-01,\n",
       "             2.5011e-01, -6.3625e-01]]]]),\n",
       " tensor([[[[ 4.7705e-03,  2.2497e-02,  1.8477e-02,  ..., -3.6647e-02,\n",
       "            -3.5384e-03, -6.3464e-03],\n",
       "           [ 3.8088e-01, -7.7972e-02, -3.1156e-01,  ...,  4.8271e-02,\n",
       "             3.2611e-01,  5.7372e-01],\n",
       "           [-2.8235e-02, -2.1022e-01, -4.0821e-01,  ...,  6.4475e-01,\n",
       "            -1.1148e+00,  4.4204e-01],\n",
       "           ...,\n",
       "           [ 3.8987e-02, -5.4598e-01,  1.1885e-02,  ...,  7.5322e-01,\n",
       "             1.9143e-02,  6.1960e-01],\n",
       "           [ 8.3628e-02, -6.0157e-01, -8.5883e-03,  ...,  7.9174e-01,\n",
       "            -2.2295e-03,  5.0195e-01],\n",
       "           [ 3.4853e-02, -5.9356e-01,  1.9190e-03,  ...,  6.8819e-01,\n",
       "             2.1829e-02,  6.5091e-01]],\n",
       " \n",
       "          [[ 3.6064e-03, -2.9161e-03,  1.3397e-04,  ..., -1.2633e-02,\n",
       "             7.7408e-03,  8.4873e-03],\n",
       "           [ 3.0650e-01, -6.6456e-01,  4.0104e-02,  ...,  7.8253e-01,\n",
       "             8.6897e-03,  9.6458e-01],\n",
       "           [-5.8115e-01, -6.0398e-01, -4.4025e-01,  ...,  5.2647e-01,\n",
       "            -4.5364e-01, -1.2138e+00],\n",
       "           ...,\n",
       "           [-4.0777e-01,  4.3638e-02,  8.5952e-01,  ...,  1.2955e+00,\n",
       "             4.4735e-01,  2.1925e-01],\n",
       "           [-5.0565e-01,  1.7377e-01,  7.5752e-01,  ...,  1.3681e+00,\n",
       "             4.6620e-01,  1.8165e-01],\n",
       "           [-4.8805e-01,  5.5448e-02,  8.5812e-01,  ...,  1.2479e+00,\n",
       "             4.6940e-01,  2.2138e-01]],\n",
       " \n",
       "          [[ 5.6104e-03,  8.1091e-03,  1.6648e-02,  ..., -5.2643e-03,\n",
       "             1.8191e-02, -2.4545e-02],\n",
       "           [-1.7967e-01, -2.4876e-01, -6.2265e-01,  ..., -1.1143e-01,\n",
       "             2.2357e-02, -1.2759e-01],\n",
       "           [ 3.1962e-01,  1.3952e-01, -1.1128e+00,  ...,  2.7222e-01,\n",
       "            -1.0555e-01, -7.1681e-01],\n",
       "           ...,\n",
       "           [ 1.7034e-01,  8.0235e-01, -3.3414e-01,  ...,  7.8149e-02,\n",
       "             1.0841e-01, -5.3120e-01],\n",
       "           [ 2.1638e-01,  7.8370e-01, -4.2779e-01,  ...,  5.4617e-02,\n",
       "             1.4911e-01, -4.6335e-01],\n",
       "           [ 1.4841e-01,  8.6958e-01, -4.0408e-01,  ...,  9.0169e-02,\n",
       "             1.4860e-01, -5.0471e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.2068e-02,  9.7868e-03,  1.2773e-02,  ..., -9.4421e-03,\n",
       "            -1.4846e-03, -1.4202e-02],\n",
       "           [ 4.6282e-02, -1.3824e-02,  4.2573e-01,  ..., -1.4450e-01,\n",
       "             2.1484e-01, -4.5644e-01],\n",
       "           [-4.7037e-01, -2.9889e-01,  1.3143e+00,  ...,  6.7580e-01,\n",
       "            -4.4170e-01, -2.6242e-01],\n",
       "           ...,\n",
       "           [-9.7752e-01, -5.7134e-02, -5.3459e-01,  ...,  3.0395e-01,\n",
       "             2.9196e-01, -1.9300e-01],\n",
       "           [-9.1140e-01, -1.7309e-01, -5.0212e-01,  ...,  3.8904e-01,\n",
       "             4.3737e-01, -3.8578e-01],\n",
       "           [-9.6188e-01, -5.4481e-02, -4.7555e-01,  ...,  3.4213e-01,\n",
       "             3.5172e-01, -2.3044e-01]],\n",
       " \n",
       "          [[-4.4092e-02, -3.6346e-04, -9.8635e-03,  ..., -8.9762e-03,\n",
       "             1.4646e-02, -6.5098e-02],\n",
       "           [-7.7568e-01,  2.1018e-01, -3.5762e-01,  ...,  8.7334e-01,\n",
       "             6.6462e-01, -8.8233e-01],\n",
       "           [-2.4535e-01,  1.5867e+00,  2.1635e-01,  ..., -6.2583e-01,\n",
       "            -2.9339e-01,  4.5351e-01],\n",
       "           ...,\n",
       "           [ 4.7449e-01,  1.7725e-01, -3.2063e-01,  ..., -2.2854e-01,\n",
       "            -3.8764e-01,  8.5269e-01],\n",
       "           [ 3.2879e-01,  4.3157e-01, -1.7122e-01,  ..., -3.0002e-01,\n",
       "            -3.4605e-01,  6.3062e-01],\n",
       "           [ 4.1899e-01,  1.5130e-01, -3.3172e-01,  ..., -2.3668e-01,\n",
       "            -3.3028e-01,  9.0816e-01]],\n",
       " \n",
       "          [[-5.3989e-03, -1.5418e-02,  3.0082e-02,  ...,  8.3186e-04,\n",
       "            -2.8970e-02,  2.7346e-02],\n",
       "           [ 6.2602e-01, -1.0264e+00, -1.4785e+00,  ...,  6.8901e-01,\n",
       "             2.5906e-01, -1.1119e-01],\n",
       "           [ 7.8587e-01, -2.4805e+00, -1.1256e-01,  ..., -3.0154e-01,\n",
       "             5.6293e-01, -8.2543e-01],\n",
       "           ...,\n",
       "           [-8.2335e-03,  2.0010e-02, -5.8289e-01,  ..., -3.5307e-01,\n",
       "             1.0141e+00,  1.3210e-01],\n",
       "           [-4.8979e-02,  1.3381e-01, -5.0246e-01,  ..., -7.1073e-01,\n",
       "             1.0540e+00,  3.8431e-02],\n",
       "           [-8.0439e-02, -2.8837e-03, -6.0709e-01,  ..., -3.8233e-01,\n",
       "             9.9281e-01,  1.6324e-01]]]]),\n",
       " tensor([[[[ 1.8799e-02, -1.3466e-02, -9.3573e-03,  ...,  1.4494e-02,\n",
       "             7.9183e-03, -2.0407e-02],\n",
       "           [ 9.0468e-01, -2.5387e-01,  1.9708e-01,  ...,  7.8676e-02,\n",
       "            -1.1355e+00,  2.5003e-01],\n",
       "           [ 9.6101e-02, -1.4699e-01,  1.4163e+00,  ..., -4.6549e-02,\n",
       "             6.7818e-01,  1.3889e+00],\n",
       "           ...,\n",
       "           [ 1.7033e-01, -6.4901e-01, -4.1839e-01,  ..., -3.5637e-01,\n",
       "            -3.0101e-01,  3.9259e-01],\n",
       "           [ 1.8023e-01, -8.0550e-01, -3.2861e-01,  ..., -6.1811e-01,\n",
       "            -3.1310e-01,  4.0789e-01],\n",
       "           [ 1.5115e-01, -5.6511e-01, -4.0082e-01,  ..., -3.7199e-01,\n",
       "            -2.3771e-01,  3.8612e-01]],\n",
       " \n",
       "          [[-9.1175e-03, -9.6445e-03,  3.2719e-02,  ...,  6.6979e-03,\n",
       "             1.3754e-02,  9.3923e-03],\n",
       "           [ 3.3249e-01, -5.5700e-01,  8.6191e-02,  ..., -6.0049e-01,\n",
       "             9.7198e-01,  2.3019e-01],\n",
       "           [ 1.4322e+00,  3.8024e-01, -3.0535e-01,  ..., -3.9597e-01,\n",
       "             3.4720e-01, -7.3661e-01],\n",
       "           ...,\n",
       "           [-3.6279e-01, -3.8367e-01, -1.3756e-01,  ..., -3.1446e-01,\n",
       "            -1.3789e-01,  1.1466e+00],\n",
       "           [-3.8704e-01, -4.3942e-01, -5.9288e-02,  ..., -3.7666e-01,\n",
       "            -1.3213e-01,  1.0208e+00],\n",
       "           [-3.7792e-01, -3.8112e-01, -1.9114e-01,  ..., -2.6716e-01,\n",
       "            -1.2830e-01,  1.1269e+00]],\n",
       " \n",
       "          [[ 1.5225e-02, -5.8783e-03,  8.0653e-03,  ..., -2.6909e-02,\n",
       "             2.1133e-02,  3.0649e-02],\n",
       "           [-5.4297e-01,  1.6726e-01,  7.7131e-01,  ...,  5.3741e-01,\n",
       "            -8.8744e-01,  3.8091e-01],\n",
       "           [ 6.5518e-01,  7.8123e-01, -1.1828e+00,  ..., -3.1317e-01,\n",
       "            -3.1210e-01, -3.5848e-01],\n",
       "           ...,\n",
       "           [ 1.1240e+00, -3.2617e-01, -8.2895e-01,  ...,  6.8473e-01,\n",
       "             1.0051e+00, -7.6775e-02],\n",
       "           [ 8.5336e-01, -1.8913e-01, -7.3826e-01,  ...,  6.9408e-01,\n",
       "             8.3450e-01, -2.3079e-01],\n",
       "           [ 1.0501e+00, -3.2467e-01, -8.5716e-01,  ...,  7.4496e-01,\n",
       "             1.0570e+00, -8.7173e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.5713e-03,  6.6524e-04, -1.7932e-02,  ..., -4.9693e-03,\n",
       "             5.5209e-03,  1.1076e-02],\n",
       "           [ 6.0343e-01,  1.1058e-02, -3.4133e-01,  ..., -1.1419e+00,\n",
       "            -8.0472e-01, -6.9434e-01],\n",
       "           [ 5.8233e-02, -6.1810e-01,  7.9958e-01,  ..., -6.8525e-01,\n",
       "            -3.7935e-02, -6.0198e-01],\n",
       "           ...,\n",
       "           [ 6.1689e-01,  4.0526e-01, -6.1588e-01,  ...,  3.9669e-01,\n",
       "             5.4387e-01,  1.2123e-01],\n",
       "           [ 5.0058e-01,  4.9762e-01, -4.3361e-01,  ...,  1.4305e-01,\n",
       "             5.4099e-01,  9.3409e-02],\n",
       "           [ 6.4235e-01,  3.8180e-01, -5.9911e-01,  ...,  4.4994e-01,\n",
       "             5.4893e-01,  8.9089e-03]],\n",
       " \n",
       "          [[ 2.9162e-02,  7.0714e-03, -2.4739e-02,  ..., -7.3147e-02,\n",
       "             8.4177e-02, -1.9510e-02],\n",
       "           [-1.2685e+00, -2.7432e-03, -1.3192e+00,  ..., -1.2334e-01,\n",
       "             3.7365e-02,  5.7234e-01],\n",
       "           [ 3.2196e-01,  5.6143e-01, -7.9059e-01,  ...,  3.7631e-01,\n",
       "             8.9972e-02,  9.3142e-01],\n",
       "           ...,\n",
       "           [ 3.1114e-01, -5.1756e-01, -1.0496e-01,  ...,  6.0159e-01,\n",
       "             5.3703e-01,  2.6653e-01],\n",
       "           [-1.6516e-02, -2.6559e-01, -1.0107e-01,  ...,  3.2186e-01,\n",
       "             2.2561e-01,  2.2674e-01],\n",
       "           [ 2.5530e-01, -5.4493e-01, -9.5961e-02,  ...,  4.6032e-01,\n",
       "             5.1355e-01,  2.7908e-01]],\n",
       " \n",
       "          [[ 2.2219e-02, -4.6554e-03, -8.4276e-03,  ..., -3.1407e-04,\n",
       "            -1.0695e-02, -9.6796e-03],\n",
       "           [-8.6638e-01,  3.7560e-02,  4.1370e-01,  ...,  1.1438e+00,\n",
       "             2.5260e-01, -7.1292e-01],\n",
       "           [-5.1894e-01,  5.8510e-01,  1.8599e+00,  ..., -6.0173e-01,\n",
       "            -9.3429e-01, -6.1790e-01],\n",
       "           ...,\n",
       "           [ 7.9828e-01, -8.9360e-01,  6.5482e-01,  ..., -4.4082e-01,\n",
       "            -5.6496e-01, -1.6198e-01],\n",
       "           [ 8.2801e-01, -1.0602e+00,  8.2140e-01,  ..., -6.5363e-01,\n",
       "            -2.2810e-01, -8.6444e-02],\n",
       "           [ 8.2585e-01, -9.0205e-01,  6.8394e-01,  ..., -4.9895e-01,\n",
       "            -5.9896e-01, -2.5765e-01]]]]),\n",
       " tensor([[[[-8.4946e-03,  1.3717e-02, -1.2952e-02,  ..., -4.1021e-03,\n",
       "            -1.3901e-03, -1.3886e-02],\n",
       "           [ 1.8758e-01, -1.0322e+00,  2.9769e-01,  ...,  4.9845e-01,\n",
       "             7.7083e-01, -5.8552e-01],\n",
       "           [ 7.5776e-02,  5.3161e-02,  8.8948e-01,  ..., -8.6831e-01,\n",
       "             2.2841e+00, -8.0988e-01],\n",
       "           ...,\n",
       "           [ 1.9938e-01, -1.2687e-01,  5.6015e-02,  ..., -2.9070e-01,\n",
       "            -2.5932e-01, -5.4514e-02],\n",
       "           [ 2.2806e-01,  5.0250e-02, -2.0911e-02,  ..., -3.3494e-01,\n",
       "            -2.0399e-01, -6.1322e-02],\n",
       "           [ 1.6465e-01, -1.8087e-01,  1.4518e-01,  ..., -3.5042e-01,\n",
       "            -2.4635e-01, -6.3211e-04]],\n",
       " \n",
       "          [[-2.2689e-03, -9.2885e-04,  5.8066e-03,  ..., -6.5551e-03,\n",
       "             8.4539e-03,  2.4721e-02],\n",
       "           [ 3.9087e-02, -5.0337e-01, -2.3184e-01,  ..., -3.9772e-02,\n",
       "            -2.0570e-01, -8.2200e-02],\n",
       "           [-7.0778e-01,  1.0783e-01,  1.0266e+00,  ..., -4.3412e-01,\n",
       "            -7.9730e-02,  9.4259e-01],\n",
       "           ...,\n",
       "           [ 1.5494e-01, -1.5276e-01, -7.6518e-01,  ..., -4.2349e-01,\n",
       "            -5.6326e-01,  2.2377e-01],\n",
       "           [ 1.9244e-01, -1.4711e-01, -7.4107e-01,  ..., -3.6121e-01,\n",
       "            -4.9399e-01,  1.9898e-02],\n",
       "           [ 1.6082e-01, -1.2767e-01, -7.7866e-01,  ..., -4.8559e-01,\n",
       "            -5.2995e-01,  2.2997e-01]],\n",
       " \n",
       "          [[-2.4338e-02, -2.0166e-02, -3.0262e-02,  ...,  2.7298e-03,\n",
       "             4.0581e-02, -5.2908e-02],\n",
       "           [ 4.4048e-01,  1.0748e-01,  1.5809e-03,  ..., -1.6568e-01,\n",
       "            -4.6642e-02,  4.3082e-01],\n",
       "           [-1.0530e+00,  3.3489e-02, -6.7921e-02,  ...,  2.3789e-01,\n",
       "            -5.0572e-01,  2.7930e-01],\n",
       "           ...,\n",
       "           [-1.3065e+00, -1.1702e+00, -1.3828e-01,  ...,  2.7479e-01,\n",
       "            -1.5974e+00,  6.1218e-01],\n",
       "           [-1.1587e+00, -1.2681e+00, -7.3427e-02,  ...,  2.8431e-01,\n",
       "            -1.4808e+00,  1.2320e+00],\n",
       "           [-1.3126e+00, -1.1817e+00, -1.7589e-01,  ...,  2.9717e-01,\n",
       "            -1.5996e+00,  5.3321e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.2745e-02,  3.7276e-02, -5.0865e-03,  ..., -2.5629e-02,\n",
       "            -6.3260e-02, -4.2276e-02],\n",
       "           [-1.8398e-01,  6.9268e-02,  3.7246e-01,  ...,  1.3191e-01,\n",
       "            -1.6279e-01, -7.6544e-01],\n",
       "           [ 7.7000e-01,  9.1445e-01,  4.9847e-01,  ...,  4.0161e-01,\n",
       "             2.7598e-01, -2.0666e-01],\n",
       "           ...,\n",
       "           [ 1.3800e+00, -6.7696e-02, -7.1984e-02,  ..., -5.9027e-01,\n",
       "             1.0093e+00, -7.1769e-01],\n",
       "           [ 1.0729e+00, -3.3419e-02, -2.4502e-01,  ..., -6.4706e-01,\n",
       "             8.3920e-01, -7.7920e-01],\n",
       "           [ 1.4067e+00, -1.2365e-01, -2.9433e-02,  ..., -4.9602e-01,\n",
       "             1.0440e+00, -7.2227e-01]],\n",
       " \n",
       "          [[ 2.3730e-03,  3.4988e-03,  2.4536e-02,  ...,  9.2509e-03,\n",
       "            -1.8787e-02, -3.4816e-02],\n",
       "           [ 9.7704e-02, -4.1916e-01,  2.6945e-01,  ..., -9.1332e-01,\n",
       "            -5.2894e-01, -5.2014e-03],\n",
       "           [-9.4095e-02,  1.7650e-01, -5.5260e-01,  ..., -5.9460e-01,\n",
       "            -3.0279e-01,  5.2060e-01],\n",
       "           ...,\n",
       "           [ 4.9663e-01,  2.8188e-02, -6.4750e-01,  ...,  2.3503e-01,\n",
       "            -2.0464e+00,  2.1364e+00],\n",
       "           [ 5.2718e-01,  2.2210e-02, -6.1700e-01,  ...,  1.0109e-01,\n",
       "            -1.9822e+00,  1.9677e+00],\n",
       "           [ 5.2338e-01,  7.5890e-02, -7.3928e-01,  ...,  2.5256e-01,\n",
       "            -2.0713e+00,  2.2073e+00]],\n",
       " \n",
       "          [[-1.4927e-02, -2.9214e-02, -1.2022e-02,  ...,  6.0910e-03,\n",
       "            -2.8431e-03, -1.3462e-02],\n",
       "           [ 2.2909e-01,  8.9676e-01, -1.2136e+00,  ..., -4.4446e-01,\n",
       "            -1.1034e+00,  1.1130e+00],\n",
       "           [ 7.4614e-01, -1.0214e-02,  8.7058e-02,  ...,  8.3750e-01,\n",
       "            -2.3138e-01,  8.0355e-02],\n",
       "           ...,\n",
       "           [-1.9795e-01, -5.3344e-01,  1.8993e-01,  ...,  1.4272e-01,\n",
       "            -3.1375e-01, -1.2825e-01],\n",
       "           [-4.6387e-01, -6.3796e-01,  2.0767e-01,  ...,  2.3539e-01,\n",
       "            -2.8487e-01, -2.6344e-01],\n",
       "           [-1.4512e-01, -5.2881e-01,  2.4017e-01,  ...,  2.0493e-01,\n",
       "            -3.1503e-01, -9.3854e-02]]]]),\n",
       " tensor([[[[-4.5543e-04,  2.8523e-02, -3.8700e-02,  ...,  1.1348e-02,\n",
       "             7.4196e-04,  1.5306e-02],\n",
       "           [ 1.8072e-01, -1.6807e-01,  1.8206e-01,  ..., -9.5909e-01,\n",
       "            -5.7526e-01, -9.5882e-01],\n",
       "           [ 1.0652e+00, -8.4791e-01,  1.5404e-01,  ..., -1.0322e+00,\n",
       "            -9.9160e-01,  9.3939e-02],\n",
       "           ...,\n",
       "           [-1.5714e-01,  4.4120e-02,  2.1538e-01,  ...,  6.3466e-01,\n",
       "             1.6781e+00,  6.8161e-01],\n",
       "           [-1.5546e-01, -6.1987e-02,  1.1772e-01,  ...,  6.3416e-01,\n",
       "             1.7122e+00,  7.6354e-01],\n",
       "           [-1.6398e-01,  6.0353e-02,  1.8756e-01,  ...,  6.1635e-01,\n",
       "             1.6690e+00,  6.3327e-01]],\n",
       " \n",
       "          [[-8.0821e-03, -3.3550e-03,  1.7654e-02,  ...,  1.9198e-02,\n",
       "            -3.5057e-03, -2.3616e-02],\n",
       "           [-5.8735e-02,  1.7368e-01,  3.1428e-02,  ..., -3.5760e-01,\n",
       "            -2.7339e-01, -1.3412e+00],\n",
       "           [ 5.8288e-01,  4.2955e-02, -2.6372e-01,  ...,  7.8959e-01,\n",
       "            -7.1258e-01,  5.0342e-02],\n",
       "           ...,\n",
       "           [ 2.1748e-01, -3.4964e-01,  6.1723e-01,  ..., -8.6932e-01,\n",
       "             6.3488e-01,  9.9264e-01],\n",
       "           [ 2.5549e-01, -3.9959e-01,  6.9843e-01,  ..., -8.8347e-01,\n",
       "             6.7026e-01,  9.9264e-01],\n",
       "           [ 2.2632e-01, -3.2990e-01,  6.1804e-01,  ..., -8.4196e-01,\n",
       "             6.5912e-01,  9.8830e-01]],\n",
       " \n",
       "          [[-2.4120e-04,  1.9151e-02, -3.8379e-02,  ..., -2.6362e-02,\n",
       "             7.9872e-03,  7.2591e-03],\n",
       "           [ 2.3784e-01, -3.3088e-01, -2.0138e-02,  ...,  1.0378e-01,\n",
       "             3.5218e-01,  3.5655e-02],\n",
       "           [ 7.9427e-01,  1.8630e-01,  1.8142e+00,  ...,  1.1813e+00,\n",
       "             5.1688e-01, -6.1616e-01],\n",
       "           ...,\n",
       "           [ 7.6976e-01, -3.2121e-02,  6.7746e-01,  ...,  8.2778e-01,\n",
       "             7.4764e-01,  9.1493e-02],\n",
       "           [ 7.5494e-01,  1.8630e-02,  6.7784e-01,  ...,  8.4309e-01,\n",
       "             7.4640e-01,  7.7022e-02],\n",
       "           [ 8.1298e-01, -3.1916e-02,  6.5715e-01,  ...,  8.4213e-01,\n",
       "             7.6091e-01,  8.9764e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.3870e-03,  2.1505e-03,  5.8225e-03,  ..., -1.2765e-02,\n",
       "             7.0594e-04, -7.9455e-03],\n",
       "           [ 5.9744e-02,  1.4525e-02,  3.5463e-01,  ...,  9.0836e-01,\n",
       "            -2.3460e-01, -6.1623e-01],\n",
       "           [-9.2781e-02, -5.7929e-01, -5.4432e-01,  ..., -3.6842e-01,\n",
       "             6.3957e-01, -4.5359e-01],\n",
       "           ...,\n",
       "           [ 1.1899e+00, -1.2468e-01,  1.0202e+00,  ...,  2.5588e-01,\n",
       "            -6.0921e-02,  4.8081e-02],\n",
       "           [ 1.1276e+00, -1.6191e-01,  1.0306e+00,  ...,  2.5742e-01,\n",
       "            -7.8808e-02,  2.4792e-02],\n",
       "           [ 1.1866e+00, -8.0546e-02,  1.0295e+00,  ...,  2.7157e-01,\n",
       "            -6.1085e-02,  7.0053e-02]],\n",
       " \n",
       "          [[-4.7755e-03,  1.1365e-02, -5.6584e-04,  ..., -7.0594e-03,\n",
       "             1.7041e-02,  1.0813e-02],\n",
       "           [-9.3659e-02,  4.0934e-01,  1.6568e-02,  ...,  9.8532e-01,\n",
       "            -3.2539e-01,  7.0721e-01],\n",
       "           [-8.5623e-01,  6.3974e-02, -1.2502e+00,  ...,  1.4979e-01,\n",
       "             8.5850e-01, -1.6678e+00],\n",
       "           ...,\n",
       "           [ 1.0877e+00, -1.0376e+00,  8.2484e-01,  ..., -3.6734e-01,\n",
       "            -1.4142e+00, -3.7517e-01],\n",
       "           [ 1.1719e+00, -1.0848e+00,  9.3373e-01,  ..., -3.3453e-01,\n",
       "            -1.4895e+00, -4.4183e-01],\n",
       "           [ 1.0602e+00, -9.8950e-01,  8.1709e-01,  ..., -3.6856e-01,\n",
       "            -1.4075e+00, -3.7708e-01]],\n",
       " \n",
       "          [[ 1.6208e-02, -1.6834e-02,  1.4285e-02,  ..., -6.2461e-03,\n",
       "            -3.3560e-02, -1.3234e-02],\n",
       "           [-6.2540e-01, -1.2060e+00, -4.4660e-01,  ..., -4.2923e-01,\n",
       "             3.6764e-03, -7.0703e-01],\n",
       "           [-9.3176e-01,  4.2315e-01, -1.1552e-02,  ..., -8.4466e-02,\n",
       "            -3.2848e-01, -2.5823e-01],\n",
       "           ...,\n",
       "           [-1.8451e+00, -9.3160e-01, -9.4703e-01,  ..., -1.7398e+00,\n",
       "             2.3420e+00,  3.3196e-01],\n",
       "           [-1.7557e+00, -8.5034e-01, -9.7351e-01,  ..., -1.6916e+00,\n",
       "             2.4031e+00,  2.0482e-01],\n",
       "           [-1.8645e+00, -9.4762e-01, -9.4966e-01,  ..., -1.7687e+00,\n",
       "             2.3401e+00,  3.3415e-01]]]]),\n",
       " tensor([[[[ 2.3951e-03, -3.6174e-03,  6.0350e-03,  ...,  4.4502e-03,\n",
       "            -2.0234e-03, -4.2019e-02],\n",
       "           [-2.5288e-01, -7.5647e-01,  1.3627e-01,  ..., -1.0586e+00,\n",
       "             8.9803e-01, -1.3088e+00],\n",
       "           [ 4.9669e-01,  5.6701e-01, -5.9469e-01,  ..., -5.1185e-01,\n",
       "             2.7485e-01, -8.3839e-01],\n",
       "           ...,\n",
       "           [ 7.0732e-03, -1.2430e-01, -4.2903e-02,  ...,  1.4928e-02,\n",
       "             5.2366e-04,  3.0902e-02],\n",
       "           [ 7.0546e-03, -1.2376e-01, -4.2782e-02,  ...,  1.4963e-02,\n",
       "             3.9569e-04,  3.1020e-02],\n",
       "           [ 7.1848e-03, -1.2482e-01, -4.2978e-02,  ...,  1.4865e-02,\n",
       "             7.0821e-04,  3.0806e-02]],\n",
       " \n",
       "          [[-1.0149e-02,  1.1756e-02,  3.2580e-02,  ...,  8.4955e-03,\n",
       "             2.3948e-02, -2.6513e-03],\n",
       "           [ 2.0463e-01, -3.5911e-01,  1.5918e-01,  ...,  2.1606e-01,\n",
       "             1.8757e-01,  3.1632e-01],\n",
       "           [ 1.7744e-01, -1.5608e-01, -1.0578e+00,  ..., -5.5679e-01,\n",
       "             7.8047e-01, -1.1032e-01],\n",
       "           ...,\n",
       "           [ 6.8743e-02, -3.3903e-02, -6.9093e-02,  ..., -2.4975e-02,\n",
       "            -8.5882e-02, -8.0278e-02],\n",
       "           [ 6.8425e-02, -3.3787e-02, -6.8777e-02,  ..., -2.4951e-02,\n",
       "            -8.5522e-02, -8.0044e-02],\n",
       "           [ 6.9105e-02, -3.4089e-02, -6.9444e-02,  ..., -2.5016e-02,\n",
       "            -8.6202e-02, -8.0576e-02]],\n",
       " \n",
       "          [[-2.1499e-02,  3.4925e-02, -1.6212e-02,  ..., -3.8014e-02,\n",
       "             2.8841e-02,  1.0885e-03],\n",
       "           [-5.0621e-02,  2.8467e-02, -2.7688e-01,  ..., -4.2332e-01,\n",
       "            -4.0346e-01, -4.5228e-01],\n",
       "           [ 3.6138e-01,  7.6524e-01, -1.1176e-01,  ..., -3.8484e-01,\n",
       "            -2.5129e-01,  3.8262e-02],\n",
       "           ...,\n",
       "           [-5.4584e-03, -2.5399e-02, -1.7692e-02,  ...,  7.2682e-02,\n",
       "             4.3801e-02, -9.8869e-02],\n",
       "           [-5.5025e-03, -2.5276e-02, -1.7677e-02,  ...,  7.2761e-02,\n",
       "             4.3659e-02, -9.8772e-02],\n",
       "           [-5.4574e-03, -2.5476e-02, -1.7774e-02,  ...,  7.2652e-02,\n",
       "             4.4029e-02, -9.9180e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.2892e-03,  4.5080e-03, -3.6002e-02,  ...,  3.3463e-04,\n",
       "             4.2739e-02,  3.6523e-03],\n",
       "           [-6.0266e-01,  9.9402e-02, -1.3996e+00,  ...,  5.2110e-01,\n",
       "            -6.6206e-02,  2.6714e-01],\n",
       "           [ 5.6406e-01, -5.8999e-01, -5.4515e-01,  ..., -9.1080e-02,\n",
       "             3.1779e-01, -2.2578e-01],\n",
       "           ...,\n",
       "           [ 7.6491e-03,  5.6595e-02,  2.2887e-02,  ...,  3.6333e-02,\n",
       "            -6.2361e-02, -9.5611e-03],\n",
       "           [ 7.4654e-03,  5.6470e-02,  2.3271e-02,  ...,  3.6373e-02,\n",
       "            -6.2284e-02, -9.4168e-03],\n",
       "           [ 7.7300e-03,  5.6736e-02,  2.2602e-02,  ...,  3.6333e-02,\n",
       "            -6.2510e-02, -9.5539e-03]],\n",
       " \n",
       "          [[ 1.2747e-02, -8.6955e-03, -3.3509e-02,  ..., -2.2547e-03,\n",
       "            -2.0852e-02,  1.4664e-02],\n",
       "           [-1.1239e-01,  1.7672e-01, -3.4982e-01,  ...,  2.1681e-01,\n",
       "            -8.6242e-01,  1.1614e-01],\n",
       "           [-3.4089e-01,  1.5179e-01,  9.7097e-03,  ..., -1.5574e-01,\n",
       "            -5.0531e-01,  3.2613e-01],\n",
       "           ...,\n",
       "           [-8.8019e-03,  1.2544e-02,  1.0695e-01,  ...,  5.6087e-02,\n",
       "             1.6821e-02,  7.9418e-02],\n",
       "           [-8.6267e-03,  1.2801e-02,  1.0659e-01,  ...,  5.6097e-02,\n",
       "             1.6844e-02,  7.9348e-02],\n",
       "           [-8.9317e-03,  1.2373e-02,  1.0742e-01,  ...,  5.5984e-02,\n",
       "             1.6810e-02,  7.9534e-02]],\n",
       " \n",
       "          [[ 1.8758e-02, -5.5306e-03,  5.6460e-02,  ..., -8.2069e-02,\n",
       "             4.2586e-03, -5.9775e-02],\n",
       "           [-5.6970e-01, -5.0794e-01, -6.2427e-01,  ..., -1.3971e-01,\n",
       "             6.2691e-01, -4.4171e-01],\n",
       "           [-2.4212e-01,  2.7492e-01, -2.8158e-01,  ..., -7.2203e-01,\n",
       "            -1.1885e-01,  8.1806e-02],\n",
       "           ...,\n",
       "           [ 2.1382e-02,  2.0943e-02, -5.9937e-02,  ...,  1.0854e-02,\n",
       "             2.8073e-02,  3.1376e-02],\n",
       "           [ 2.1494e-02,  2.1017e-02, -5.9877e-02,  ...,  1.0741e-02,\n",
       "             2.8038e-02,  3.1381e-02],\n",
       "           [ 2.1199e-02,  2.1022e-02, -6.0113e-02,  ...,  1.0926e-02,\n",
       "             2.8165e-02,  3.1284e-02]]]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['past_key_value'].value_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练PQ并保存到硬盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss import IndexPQ, METRIC_INNER_PRODUCT\n",
    "import numpy as np\n",
    "\n",
    "M = 16\n",
    "nbits = 8\n",
    "dim = int(model.config.hidden_size / model.config.num_attention_heads)\n",
    "\n",
    "train_data = np\n",
    "\n",
    "indices: List[List[IndexPQ]] = [\n",
    "    [IndexPQ(dim, M, nbits, METRIC_INNER_PRODUCT) for _ in range(model.config.num_attention_heads)] for _ in range(model.config.num_hidden_layers)\n",
    "]\n",
    "\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    for j in range(model.config.num_attention_heads):\n",
    "        tmp = model.model.layers[i].self_attn.middleware['key_states'][:, j, :10000, :].view(-1, 128).cpu().numpy()\n",
    "        # print(tmp.shape)\n",
    "        indices[i][j].train(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save indices to disk\n",
    "from faiss import write_index\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    for j in range(model.config.num_attention_heads):\n",
    "        index_filename = f\"./pq_index/pq_{i}_{j}.index\"\n",
    "        write_index(indices[i][j], index_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss import read_index\n",
    "\n",
    "def restore_index(layer_idx: int, head_idx: int):\n",
    "    \"\"\"\n",
    "    Restore a FAISS IndexPQ from disk.\n",
    "    \n",
    "    Args:\n",
    "    layer_idx (int): The index of the layer in the model.\n",
    "    head_idx (int): The index of the attention head within the layer.\n",
    "\n",
    "    Returns:\n",
    "    faiss.IndexPQ: The restored FAISS index.\n",
    "    \"\"\"\n",
    "    index_filename = f\"./pq_index/pq_{layer_idx}_{head_idx}.index\"\n",
    "    return read_index(index_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv, LlamaDecoderLayer, LlamaMLP, LlamaRMSNorm, LlamaModel, LlamaSdpaAttention, LlamaPreTrainedModel\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from faiss import IndexPQ, IndexFlatIP\n",
    "\n",
    "class KeyStateTensorMocker:\n",
    "    def __init__(self, key_states: Union[torch.Tensor, 'KeyStateTensorMocker'], layer_idx: int) -> None:\n",
    "        self._cache = None\n",
    "        self._shape = None\n",
    "        self._debug_cache = None\n",
    "        \n",
    "        self.layer_idx = layer_idx\n",
    "        if key_states is not None:\n",
    "            if isinstance(key_states, KeyStateTensorMocker):\n",
    "                # This is used when from_legacy_cache is called\n",
    "                self._cache = key_states._cache\n",
    "                self._shape = key_states._shape\n",
    "                self._debug_cache = key_states._debug_cache\n",
    "                return\n",
    "\n",
    "            bsz, num_heads, seq_len, head_dim = key_states.shape\n",
    "            # self._cache = [IndexFlatIP(head_dim) for _ in range(num_heads)]\n",
    "            self._cache = [restore_index(layer_idx, i) for i in range(num_heads)]\n",
    "            self._shape = [bsz, num_heads, 0, head_dim]\n",
    "\n",
    "            self.cat(key_states)          \n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Optional[Tuple[int, int, int, int]]:\n",
    "        # Return the shape if available\n",
    "        return tuple(self._shape)\n",
    "    \n",
    "    def cat(self, key_states: torch.Tensor) -> None:\n",
    "        '''\n",
    "        Update or init the cache with key_states.\n",
    "        '''\n",
    "        bsz, num_heads, seq_len, head_dim = key_states.shape\n",
    "        assert head_dim == self._shape[-1], \"The head dimension of the key_states does not match the cache's head dimension\"\n",
    "        assert num_heads == self._shape[1], \"The number of heads of the key_states does not match the cache's number of heads\"\n",
    "\n",
    "        for b in range(bsz):\n",
    "            for i in range(num_heads):\n",
    "                self._cache[i].add(key_states[b, i, :, :].cpu().numpy())\n",
    "\n",
    "        self._shape[2] += seq_len\n",
    "\n",
    "        if self._debug_cache is None:\n",
    "            self._debug_cache = key_states\n",
    "        else:\n",
    "            self._debug_cache = torch.cat([self._debug_cache, key_states], dim=-2)\n",
    " \n",
    "        pass\n",
    "    def __getitem__(self, idx: int) -> IndexFlatIP:\n",
    "        # print(\"idx\", idx)\n",
    "        return self._cache[idx]\n",
    "\n",
    "    def reconstruct(self) -> torch.Tensor:\n",
    "        '''\n",
    "        Reconstruct the mocker to a torch.Tensor\n",
    "        Inefficient, only for debugging\n",
    "        '''\n",
    "        bsz, num_heads, seq_len, head_dim = self._shape\n",
    "        key_states = torch.zeros(bsz, num_heads, seq_len, head_dim, device='cuda:0')\n",
    "\n",
    "        for b in range(bsz):\n",
    "            for i in range(num_heads):\n",
    "                key_states[b, i, :, :] = torch.tensor(self._cache[i].reconstruct_n(0, seq_len), device=key_states.device)\n",
    "        \n",
    "        return key_states\n",
    "\n",
    "class DatabaseCache(DynamicCache):\n",
    "    def __init__(self) -> None:\n",
    "        self.key_cache : List[KeyStateTensorMocker] = [] # indexed by layer_idx\n",
    "        self.value_cache : List[torch.Tensor] = []\n",
    "        self._debug_key_cache : List[torch.Tensor] = []\n",
    "        self._seen_tokens = 0\n",
    "    \n",
    "    def reorder_cache(self, beam_idx: torch.LongTensor):\n",
    "        raise NotImplementedError(\"Reordering the cache is not currently supported\")\n",
    "\n",
    "    def query(self, query_states, layer_idx, *, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n",
    "        '''\n",
    "        Basically implements SDPA with cache\n",
    "        '''\n",
    "        bsz, num_heads, query_len, head_dim = query_states.shape\n",
    "        seq_len = self._seen_tokens\n",
    "\n",
    "        assert bsz == 1, \"Batch size > 1 is not currently supported\"\n",
    "        \n",
    "        # scale\n",
    "        scaling_factor = 1 / math.sqrt(head_dim) if scale is None else scale\n",
    "        # scaling_factor = 1\n",
    "\n",
    "        # bias\n",
    "        attn_bias = torch.zeros(query_len, seq_len, device=query_states.device, dtype=query_states.dtype)\n",
    "        if is_causal:\n",
    "            assert attn_mask is None, \"is_causal and attn_mask cannot be used together\"\n",
    "            temp_mask = torch.ones(query_len, seq_len, device=query_states.device, dtype=torch.bool).tril(diagonal=0)\n",
    "            attn_bias.masked_fill_(temp_mask.logical_not(), float('-inf'))\n",
    "            attn_bias.to(query_states.dtype)\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_bias.masked_fill(attn_mask.logical_not(), float('-inf'))\n",
    "            else:\n",
    "                attn_bias += attn_mask\n",
    "\n",
    "        # score\n",
    "        if seq_len > 0:\n",
    "            attn_score = torch.zeros(bsz, num_heads, query_len, seq_len, device=query_states.device)\n",
    "            top_k = int(seq_len * 1)\n",
    "            # top_k = seq_len\n",
    "            for b in range(bsz): # TODO: parallelize this\n",
    "                for h in range(num_heads):\n",
    "                    D, I = self.key_cache[layer_idx][h].search(query_states[b, h, :, :].cpu().numpy(), top_k) # TODO: specify k\n",
    "                    # convert D to tensor\n",
    "                    D = torch.tensor(D, device=query_states.device)\n",
    "                    for (idx, cols) in enumerate(I):\n",
    "                        for (jdx, col) in enumerate(cols):\n",
    "                            attn_score[b, h, idx, col] = D[idx, jdx]\n",
    "\n",
    "        else:\n",
    "            attn_score = query_states @ self._debug_key_cache[layer_idx].transpose(-1, -2)\n",
    "\n",
    "\n",
    "        attn_score[torch.abs(attn_score) < 1e-5] = -1e10\n",
    "\n",
    "        attn_score = attn_score * scaling_factor + attn_bias\n",
    "\n",
    "        # softmax\n",
    "        attn_score = torch.softmax(attn_score, dim=-1)\n",
    "\n",
    "        # dropout\n",
    "        attn_score = torch.dropout(attn_score, dropout_p, train=True)\n",
    "\n",
    "        # weighted sum\n",
    "        return attn_score @ self.value_cache[layer_idx]\n",
    "\n",
    "    def update(self, key_states, value_states, layer_idx, cache_kwargs=None) -> None:\n",
    "        '''\n",
    "        Broken change: returns None instead of updated states\n",
    "        '''\n",
    "        # key_states is shaped (bsz, num_heads, seq_len, head_dim)\n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        bsz, num_heads, seq_len, head_dim = key_states.shape\n",
    "\n",
    "        # initialize the cache if it doesn't exist\n",
    "        if len(self.key_cache) <= layer_idx:\n",
    "            self.key_cache.append(KeyStateTensorMocker(key_states, layer_idx))\n",
    "            self.value_cache.append(value_states)\n",
    "\n",
    "            if isinstance(key_states, KeyStateTensorMocker):\n",
    "                # key_states = key_states.reconstruct()\n",
    "                key_states = key_states._debug_cache\n",
    "            self._debug_key_cache.append(key_states)\n",
    "        else:\n",
    "            # update the cache\n",
    "            self.key_cache[layer_idx].cat(key_states)\n",
    "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "            self._debug_key_cache[layer_idx] = torch.cat([self._debug_key_cache[layer_idx], key_states], dim=-2)\n",
    "     \n",
    "class LlamaForCausalLMDB(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "        self.model = LlamaModelDB(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "        self.middleware = {}\n",
    "        self.fwcall = 0\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        self.fwcall += 1\n",
    "        print(f\"fwcall: {self.fwcall}, key cache size(one layer): {past_key_values[0][0].shape if past_key_values is not None else None}\")\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "class LlamaModelDB(LlamaModel):\n",
    "    def __init__(self, config):\n",
    "        super(LlamaModel, self).__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayerDB(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        self.middleware = {}\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        '''\n",
    "        Only difference is that we use DatabaseCache instead of DynamicCache\n",
    "        '''\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if self.gradient_checkpointing and self.training and use_cache:\n",
    "            use_cache = False\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        past_seen_tokens = 0\n",
    "        if use_cache:  # kept for BC (cache positions)\n",
    "            if not isinstance(past_key_values, StaticCache):\n",
    "                past_key_values = DatabaseCache.from_legacy_cache(past_key_values)\n",
    "                past_seen_tokens = past_key_values.get_seq_length()\n",
    "\n",
    "        if cache_position is None:\n",
    "            if isinstance(past_key_values, StaticCache):\n",
    "                raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds, cache_position, past_seen_tokens)\n",
    "\n",
    "        # embed positions\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    causal_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = None\n",
    "        if use_cache:\n",
    "            next_cache = (\n",
    "                next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n",
    "            )\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "class LlamaDecoderLayerDB(LlamaDecoderLayer):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super(LlamaDecoderLayer, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # self.self_attn = LlamaSdpaAttention(config, layer_idx)\n",
    "        self.self_attn = LlamaSdpaAttentionDB(config, layer_idx)\n",
    "\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "class LlamaSdpaAttentionDB(LlamaSdpaAttention):\n",
    "    \"\"\"\n",
    "    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n",
    "    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n",
    "    SDPA API.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.middleware = {}\n",
    "\n",
    "    # Adapted from LlamaAttention.forward\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        if output_attentions:\n",
    "            return super().forward(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "            )\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        causal_mask = attention_mask\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n",
    "            \n",
    "        attn_output = past_key_value.query(\n",
    "            query_states, \n",
    "            self.layer_idx,\n",
    "            attn_mask=causal_mask,\n",
    "            dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "            is_causal=causal_mask is None and q_len > 1,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        self.middleware.update({\n",
    "            \"query_states\" : query_states,\n",
    "            \"key_states\" : key_states,\n",
    "            \"value_states\" : value_states,\n",
    "            \"past_key_value\" : past_key_value\n",
    "        })\n",
    "\n",
    "        return attn_output, None, past_key_value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用IndexFlatIP验证正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_states = m['key_states']\n",
    "value_states = m['value_states']\n",
    "query_states = m['query_states']\n",
    "\n",
    "sdpa = torch.nn.functional.scaled_dot_product_attention(\n",
    "    m['query_states'],\n",
    "    m['key_states'],\n",
    "    m['value_states']\n",
    ")\n",
    "\n",
    "dbcache = DatabaseCache()\n",
    "dbcache.update(key_states, value_states, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 0\n",
    "\n",
    "bsz, num_heads, query_len, head_dim = query_states.shape\n",
    "seq_len = dbcache._seen_tokens\n",
    "\n",
    "attn_score = torch.zeros(bsz, num_heads, query_len, seq_len)\n",
    "for b in range(bsz):\n",
    "    for h in range(num_heads):\n",
    "        D, I = dbcache.key_cache[layer_idx][h].search(query_states[b, h, :, :].cpu().numpy(), seq_len) # TODO: specify k\n",
    "        # convert D to tensor\n",
    "        D = torch.tensor(D, device=query_states.device)\n",
    "        for (idx, cols) in enumerate(I):\n",
    "            for (jdx, col) in enumerate(cols):\n",
    "                attn_score[b, h, idx, col] = D[idx, jdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_states[0, 0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = IndexFlatIP(head_dim)\n",
    "\n",
    "index.add(key_states[0, 0, :, :].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17.8612], device='cuda:0')\n",
      "17.861155\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(query_states[0, 0, :, :].cpu().numpy(), 5)\n",
    "\n",
    "print(query_states[0, 0, :, :] @ key_states[0, 0, I[0][0], :])\n",
    "print(D[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 17.8612]],\n",
       "\n",
       "         [[  2.9589]],\n",
       "\n",
       "         [[  4.6459]],\n",
       "\n",
       "         [[ 34.8256]],\n",
       "\n",
       "         [[ 26.3393]],\n",
       "\n",
       "         [[  4.4898]],\n",
       "\n",
       "         [[ 16.9595]],\n",
       "\n",
       "         [[-29.1442]],\n",
       "\n",
       "         [[ 15.6031]],\n",
       "\n",
       "         [[-16.4950]],\n",
       "\n",
       "         [[ 16.2660]],\n",
       "\n",
       "         [[ 18.6018]],\n",
       "\n",
       "         [[ 40.9244]],\n",
       "\n",
       "         [[-12.6539]],\n",
       "\n",
       "         [[ -3.8843]],\n",
       "\n",
       "         [[ 12.1046]],\n",
       "\n",
       "         [[ 12.3422]],\n",
       "\n",
       "         [[  9.4559]],\n",
       "\n",
       "         [[ 21.0365]],\n",
       "\n",
       "         [[ 36.2610]],\n",
       "\n",
       "         [[ 11.7032]],\n",
       "\n",
       "         [[ -4.3956]],\n",
       "\n",
       "         [[ 24.3023]],\n",
       "\n",
       "         [[ 17.2784]],\n",
       "\n",
       "         [[  3.4854]],\n",
       "\n",
       "         [[ 10.2540]],\n",
       "\n",
       "         [[ 16.6599]],\n",
       "\n",
       "         [[ -8.6022]],\n",
       "\n",
       "         [[ 16.4774]],\n",
       "\n",
       "         [[ 16.5451]],\n",
       "\n",
       "         [[-14.7055]],\n",
       "\n",
       "         [[  1.9440]]]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(query_states, key_states.transpose(-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 17.8612]],\n",
       "\n",
       "         [[  2.9589]],\n",
       "\n",
       "         [[  4.6459]],\n",
       "\n",
       "         [[ 34.8256]],\n",
       "\n",
       "         [[ 26.3393]],\n",
       "\n",
       "         [[  4.4898]],\n",
       "\n",
       "         [[ 16.9595]],\n",
       "\n",
       "         [[-29.1442]],\n",
       "\n",
       "         [[ 15.6031]],\n",
       "\n",
       "         [[-16.4950]],\n",
       "\n",
       "         [[ 16.2660]],\n",
       "\n",
       "         [[ 18.6018]],\n",
       "\n",
       "         [[ 40.9244]],\n",
       "\n",
       "         [[-12.6539]],\n",
       "\n",
       "         [[ -3.8843]],\n",
       "\n",
       "         [[ 12.1046]],\n",
       "\n",
       "         [[ 12.3422]],\n",
       "\n",
       "         [[  9.4559]],\n",
       "\n",
       "         [[ 21.0365]],\n",
       "\n",
       "         [[ 36.2610]],\n",
       "\n",
       "         [[ 11.7032]],\n",
       "\n",
       "         [[ -4.3956]],\n",
       "\n",
       "         [[ 24.3023]],\n",
       "\n",
       "         [[ 17.2784]],\n",
       "\n",
       "         [[  3.4854]],\n",
       "\n",
       "         [[ 10.2540]],\n",
       "\n",
       "         [[ 16.6599]],\n",
       "\n",
       "         [[ -8.6022]],\n",
       "\n",
       "         [[ 16.4774]],\n",
       "\n",
       "         [[ 16.5451]],\n",
       "\n",
       "         [[-14.7055]],\n",
       "\n",
       "         [[  1.9440]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale & softmax\n",
    "scaling_factor = torch.sqrt(torch.tensor(head_dim))\n",
    "attn_score = torch.softmax(attn_score / scaling_factor, dim=-1)\n",
    "\n",
    "# weighted sum\n",
    "attn_output = torch.zeros_like(query_states)\n",
    "for b in range(bsz):\n",
    "    for h in range(num_heads):\n",
    "        for i in range(query_len):\n",
    "            # Each output vector is a sum over all value vectors, weighted by the attention scores\n",
    "            for j in range(seq_len):\n",
    "                attn_output[b, h, i, :] += attn_score[b, h, i, j] * dbcache.value_cache[layer_idx][b, h, j, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output2 = attn_score @ dbcache.value_cache[layer_idx].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 7.5211e-03, -1.0375e-02,  3.5426e-03,  ..., -1.8492e-03,\n",
      "            3.9925e-03, -4.3669e-03]],\n",
      "\n",
      "         [[ 2.1980e-03,  3.3925e-03, -2.4338e-03,  ...,  2.2310e-03,\n",
      "            2.4552e-03,  5.0936e-03]],\n",
      "\n",
      "         [[-4.0987e-03, -3.7784e-03,  3.0119e-03,  ...,  7.2256e-04,\n",
      "           -4.6644e-05,  9.7175e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7973e-02, -6.9194e-03, -1.7842e-02,  ...,  7.5553e-03,\n",
      "            3.7649e-03, -1.2968e-02]],\n",
      "\n",
      "         [[-1.0510e-02,  1.2955e-03,  8.9638e-04,  ...,  7.3972e-04,\n",
      "            3.3017e-04,  4.8898e-03]],\n",
      "\n",
      "         [[-7.9546e-03,  8.9661e-03,  1.2278e-03,  ...,  4.6792e-05,\n",
      "            1.1711e-03, -9.9407e-03]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 7.5211e-03, -1.0375e-02,  3.5426e-03,  ..., -1.8492e-03,\n",
      "            3.9925e-03, -4.3669e-03]],\n",
      "\n",
      "         [[ 2.1980e-03,  3.3925e-03, -2.4338e-03,  ...,  2.2310e-03,\n",
      "            2.4552e-03,  5.0936e-03]],\n",
      "\n",
      "         [[-4.0987e-03, -3.7784e-03,  3.0119e-03,  ...,  7.2256e-04,\n",
      "           -4.6644e-05,  9.7175e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7973e-02, -6.9194e-03, -1.7842e-02,  ...,  7.5553e-03,\n",
      "            3.7649e-03, -1.2968e-02]],\n",
      "\n",
      "         [[-1.0510e-02,  1.2955e-03,  8.9638e-04,  ...,  7.3972e-04,\n",
      "            3.3017e-04,  4.8898e-03]],\n",
      "\n",
      "         [[-7.9546e-03,  8.9661e-03,  1.2278e-03,  ...,  4.6792e-05,\n",
      "            1.1711e-03, -9.9407e-03]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(sdpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 7.5211e-03, -1.0375e-02,  3.5426e-03,  ..., -1.8492e-03,\n",
      "            3.9925e-03, -4.3669e-03]],\n",
      "\n",
      "         [[ 2.1980e-03,  3.3925e-03, -2.4338e-03,  ...,  2.2310e-03,\n",
      "            2.4552e-03,  5.0936e-03]],\n",
      "\n",
      "         [[-4.0987e-03, -3.7784e-03,  3.0119e-03,  ...,  7.2256e-04,\n",
      "           -4.6644e-05,  9.7175e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7973e-02, -6.9194e-03, -1.7842e-02,  ...,  7.5553e-03,\n",
      "            3.7649e-03, -1.2968e-02]],\n",
      "\n",
      "         [[-1.0510e-02,  1.2955e-03,  8.9638e-04,  ...,  7.3972e-04,\n",
      "            3.3017e-04,  4.8898e-03]],\n",
      "\n",
      "         [[-7.9546e-03,  8.9661e-03,  1.2278e-03,  ...,  4.6792e-05,\n",
      "            1.1711e-03, -9.9407e-03]]]])\n"
     ]
    }
   ],
   "source": [
    "print(attn_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用pipeline进行文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "modelDB = LlamaForCausalLMDB.from_pretrained(\"llama-2-7b-hf\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaForCausalLMDB' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwcall: 15, key cache size(one layer): None\n",
      "fwcall: 16, key cache size(one layer): (1, 32, 6, 128)\n",
      "fwcall: 17, key cache size(one layer): (1, 32, 7, 128)\n",
      "fwcall: 18, key cache size(one layer): (1, 32, 8, 128)\n",
      "fwcall: 19, key cache size(one layer): (1, 32, 9, 128)\n",
      "fwcall: 20, key cache size(one layer): (1, 32, 10, 128)\n",
      "fwcall: 21, key cache size(one layer): (1, 32, 11, 128)\n",
      "fwcall: 22, key cache size(one layer): (1, 32, 12, 128)\n",
      "fwcall: 23, key cache size(one layer): (1, 32, 13, 128)\n",
      "fwcall: 24, key cache size(one layer): (1, 32, 14, 128)\n",
      "fwcall: 25, key cache size(one layer): (1, 32, 15, 128)\n",
      "fwcall: 26, key cache size(one layer): (1, 32, 16, 128)\n",
      "fwcall: 27, key cache size(one layer): (1, 32, 17, 128)\n",
      "fwcall: 28, key cache size(one layer): (1, 32, 18, 128)\n",
      "fwcall: 29, key cache size(one layer): (1, 32, 19, 128)\n",
      "fwcall: 30, key cache size(one layer): (1, 32, 20, 128)\n",
      "fwcall: 31, key cache size(one layer): (1, 32, 21, 128)\n",
      "fwcall: 32, key cache size(one layer): (1, 32, 22, 128)\n",
      "fwcall: 33, key cache size(one layer): (1, 32, 23, 128)\n",
      "fwcall: 34, key cache size(one layer): (1, 32, 24, 128)\n",
      "fwcall: 35, key cache size(one layer): (1, 32, 25, 128)\n",
      "fwcall: 36, key cache size(one layer): (1, 32, 26, 128)\n",
      "fwcall: 37, key cache size(one layer): (1, 32, 27, 128)\n",
      "fwcall: 38, key cache size(one layer): (1, 32, 28, 128)\n",
      "fwcall: 39, key cache size(one layer): (1, 32, 29, 128)\n",
      "fwcall: 40, key cache size(one layer): (1, 32, 30, 128)\n",
      "fwcall: 41, key cache size(one layer): (1, 32, 31, 128)\n",
      "fwcall: 42, key cache size(one layer): (1, 32, 32, 128)\n",
      "fwcall: 43, key cache size(one layer): (1, 32, 33, 128)\n",
      "fwcall: 44, key cache size(one layer): (1, 32, 34, 128)\n",
      "fwcall: 45, key cache size(one layer): (1, 32, 35, 128)\n",
      "fwcall: 46, key cache size(one layer): (1, 32, 36, 128)\n",
      "fwcall: 47, key cache size(one layer): (1, 32, 37, 128)\n",
      "fwcall: 48, key cache size(one layer): (1, 32, 38, 128)\n",
      "fwcall: 49, key cache size(one layer): (1, 32, 39, 128)\n",
      "fwcall: 50, key cache size(one layer): (1, 32, 40, 128)\n",
      "fwcall: 51, key cache size(one layer): (1, 32, 41, 128)\n",
      "fwcall: 52, key cache size(one layer): (1, 32, 42, 128)\n",
      "fwcall: 53, key cache size(one layer): (1, 32, 43, 128)\n",
      "fwcall: 54, key cache size(one layer): (1, 32, 44, 128)\n",
      "fwcall: 55, key cache size(one layer): (1, 32, 45, 128)\n",
      "fwcall: 56, key cache size(one layer): (1, 32, 46, 128)\n",
      "fwcall: 57, key cache size(one layer): (1, 32, 47, 128)\n",
      "fwcall: 58, key cache size(one layer): (1, 32, 48, 128)\n",
      "fwcall: 59, key cache size(one layer): (1, 32, 49, 128)\n",
      "fwcall: 60, key cache size(one layer): (1, 32, 50, 128)\n",
      "fwcall: 61, key cache size(one layer): (1, 32, 51, 128)\n",
      "fwcall: 62, key cache size(one layer): (1, 32, 52, 128)\n",
      "fwcall: 63, key cache size(one layer): (1, 32, 53, 128)\n",
      "fwcall: 64, key cache size(one layer): (1, 32, 54, 128)\n",
      "fwcall: 65, key cache size(one layer): (1, 32, 55, 128)\n",
      "fwcall: 66, key cache size(one layer): (1, 32, 56, 128)\n",
      "fwcall: 67, key cache size(one layer): (1, 32, 57, 128)\n",
      "fwcall: 68, key cache size(one layer): (1, 32, 58, 128)\n",
      "fwcall: 69, key cache size(one layer): (1, 32, 59, 128)\n",
      "fwcall: 70, key cache size(one layer): (1, 32, 60, 128)\n",
      "fwcall: 71, key cache size(one layer): (1, 32, 61, 128)\n",
      "fwcall: 72, key cache size(one layer): (1, 32, 62, 128)\n",
      "fwcall: 73, key cache size(one layer): (1, 32, 63, 128)\n",
      "fwcall: 74, key cache size(one layer): (1, 32, 64, 128)\n",
      "fwcall: 75, key cache size(one layer): (1, 32, 65, 128)\n",
      "fwcall: 76, key cache size(one layer): (1, 32, 66, 128)\n",
      "fwcall: 77, key cache size(one layer): (1, 32, 67, 128)\n",
      "fwcall: 78, key cache size(one layer): (1, 32, 68, 128)\n",
      "fwcall: 79, key cache size(one layer): (1, 32, 69, 128)\n",
      "fwcall: 80, key cache size(one layer): (1, 32, 70, 128)\n",
      "fwcall: 81, key cache size(one layer): (1, 32, 71, 128)\n",
      "fwcall: 82, key cache size(one layer): (1, 32, 72, 128)\n",
      "fwcall: 83, key cache size(one layer): (1, 32, 73, 128)\n",
      "fwcall: 84, key cache size(one layer): (1, 32, 74, 128)\n",
      "fwcall: 85, key cache size(one layer): (1, 32, 75, 128)\n",
      "fwcall: 86, key cache size(one layer): (1, 32, 76, 128)\n",
      "fwcall: 87, key cache size(one layer): (1, 32, 77, 128)\n",
      "fwcall: 88, key cache size(one layer): (1, 32, 78, 128)\n",
      "fwcall: 89, key cache size(one layer): (1, 32, 79, 128)\n",
      "fwcall: 90, key cache size(one layer): (1, 32, 80, 128)\n",
      "fwcall: 91, key cache size(one layer): (1, 32, 81, 128)\n",
      "fwcall: 92, key cache size(one layer): (1, 32, 82, 128)\n",
      "fwcall: 93, key cache size(one layer): (1, 32, 83, 128)\n",
      "fwcall: 94, key cache size(one layer): (1, 32, 84, 128)\n",
      "fwcall: 95, key cache size(one layer): (1, 32, 85, 128)\n",
      "fwcall: 96, key cache size(one layer): (1, 32, 86, 128)\n",
      "[{'generated_text': 'In this work, we develop and develop an algebraic approach to automated verification of programs given by means of first order function definitions involving linear and quadratic program verification.Ћ verification conditions, by using first order predicates. We present the generalised algebraic conditions under which a given function of such a linear-arbitrary functions. can be guaranteed to be in the of the verification and that of program correctness.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# device = \"cpu\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=modelDB,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = device,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "sequence = \"In this work, we develop\"\n",
    "\n",
    "output = pipe(sequence, max_length=128, do_sample=True, temperature=0.9)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = modelDB.model.layers[0].self_attn.middleware\n",
    "cache = m['past_key_value']\n",
    "query_states = m['query_states']\n",
    "key_states = m['key_states']\n",
    "value_states = m['value_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 8, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 8, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.key_cache[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query_states, key_states, value_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_compare.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = cache.query(query_states, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 3.6696e-03, -7.9229e-03,  8.4558e-03,  ..., -4.5834e-03,\n",
      "           -2.3658e-03, -1.7671e-04]],\n",
      "\n",
      "         [[ 4.3845e-04, -5.5317e-03,  5.8105e-03,  ...,  8.9691e-03,\n",
      "           -4.0985e-04, -5.5041e-03]],\n",
      "\n",
      "         [[-6.8097e-03,  3.5033e-03,  4.1462e-03,  ...,  5.9926e-03,\n",
      "            2.3143e-04, -2.0504e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8282e-02,  4.3157e-02, -1.2844e-02,  ...,  3.6885e-02,\n",
      "            7.5483e-03, -1.6709e-03]],\n",
      "\n",
      "         [[ 9.6602e-05,  6.6361e-03,  8.9406e-03,  ..., -1.1882e-02,\n",
      "           -5.2853e-03,  1.6323e-02]],\n",
      "\n",
      "         [[ 3.4548e-03,  4.0896e-03, -1.1464e-02,  ..., -3.3808e-04,\n",
      "            4.1257e-03,  6.8261e-03]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.3781e-03, -5.7980e-04,  2.9330e-03,  ...,  7.4228e-04,\n",
      "           -1.4491e-03, -1.3022e-04]],\n",
      "\n",
      "         [[-2.8401e-03, -1.2746e-03, -3.7840e-03,  ...,  2.6666e-03,\n",
      "           -4.3657e-04, -1.0268e-03]],\n",
      "\n",
      "         [[-3.1803e-04,  1.7788e-03,  2.1568e-03,  ...,  1.2615e-03,\n",
      "            2.9185e-03,  8.2977e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4362e-02, -2.4401e-03, -1.8101e-02,  ...,  2.2499e-02,\n",
      "           -1.3045e-02,  7.9051e-03]],\n",
      "\n",
      "         [[ 4.3792e-03,  4.5733e-03, -9.5786e-04,  ...,  1.0060e-03,\n",
      "           -8.3106e-05,  2.5171e-03]],\n",
      "\n",
      "         [[ 1.1982e-04,  3.7192e-03, -1.3928e-03,  ...,  3.4574e-03,\n",
      "            1.4829e-03,  1.6591e-03]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss import IndexPQ\n",
    "index = IndexPQ(128, 8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m['value_states'].shape)\n",
    "\n",
    "indices = [IndexPQ(128, 8, 4) for _ in range(32)]\n",
    "\n",
    "for i, index in enumerate(indices):\n",
    "    tmp = m['value_states'][:, i, :, :].view(-1, 128).cpu().numpy()\n",
    "    index.train(tmp)\n",
    "    index.add(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['value_states'][:, i, :, :].view(-1, 128).cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(m['value_states'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"SEIEE in SJTU is\"\n",
    "output = pipe(sequence, max_length=512, do_sample=True, temperature=0.9)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从这里我们可以看到，一个新的文本生成任务会使用一个新的缓存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m['past_key_value']._seen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['hidden_states_input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__call__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 乘法器优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "    \"\"\"\n",
    "    tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "    new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n",
    "    return tensor.view(new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"k.shape\", m['past_key_value'].key_cache[0].shape)\n",
    "print(\"v.shape\", m['past_key_value'].value_cache[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of q, k\n",
    "q = m['past_key_value'].key_cache[0].clone()\n",
    "k = m['past_key_value'].value_cache[0].clone()\n",
    "\n",
    "# merge heads\n",
    "num_heads = 32\n",
    "attn_head_size = 128\n",
    "q = _merge_heads(q, num_heads, attn_head_size)\n",
    "k = _merge_heads(k, num_heads, attn_head_size)\n",
    "\n",
    "print(\"q.shape\", q.shape)\n",
    "print(\"k.shape\", k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming q and k are defined as torch.Tensor with the given shapes\n",
    "# q.shape == torch.Size([1, 357, 4096])\n",
    "# k.shape == torch.Size([1, 357, 4096])\n",
    "\n",
    "# Expand q and k to prepare for element-wise multiplication\n",
    "# Expand q to [1, 357, 1, 4096] and k to [1, 1, 357, 4096]\n",
    "q_expanded = q.unsqueeze(2)  # Adding a singleton dimension for broadcasting\n",
    "k_expanded = k.unsqueeze(1)  # Adding a different singleton dimension for broadcasting\n",
    "\n",
    "# Element-wise multiplication\n",
    "# Result shape will be [1, 357, 357, 4096], capturing each multiplication\n",
    "multiplication_results = q_expanded * k_expanded\n",
    "\n",
    "# To verify, let's examine the shape\n",
    "print(\"Shape of multiplication_results:\", multiplication_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplication_results.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multiplication_results.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(multiplication_results.reshape(-1).cpu().numpy(), bins=100, log=True)\n",
    "# cdf of abs(multiplication_results)\n",
    "\n",
    "plt.hist(multiplication_results.reshape(-1).abs().cpu().numpy(), bins=100, cumulative=True, density=True, histtype='step')\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.title(f\"Q*K^T, {q.shape[1]} * {q.shape[2]} * {k.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 32\n",
    "attn_head_size = 128\n",
    "\n",
    "# store multiplication results in a list\n",
    "multiplication_results = []\n",
    "\n",
    "\n",
    "# for i, layer in enumerate(model.model.layers):\n",
    "for i in [0,15,31]:\n",
    "    layer = model.model.layers[i]\n",
    "    \n",
    "\n",
    "    m = layer.self_attn.middleware\n",
    "    \n",
    "    q = m['past_key_value'].key_cache[0].clone()\n",
    "    k = m['past_key_value'].value_cache[0].clone()\n",
    "\n",
    "    q = _merge_heads(q, num_heads, attn_head_size)\n",
    "    k = _merge_heads(k, num_heads, attn_head_size)\n",
    "\n",
    "    q_expanded = q.unsqueeze(2)\n",
    "    k_expanded = k.unsqueeze(1)\n",
    "    tmp = q_expanded * k_expanded\n",
    "\n",
    "    # offload to cpu\n",
    "    tmp.cpu()\n",
    "\n",
    "    # free cuda memory\n",
    "    del q, k, q_expanded, k_expanded\n",
    "\n",
    "    multiplication_results.append(tmp)\n",
    "\n",
    "# flatten the list\n",
    "multiplication_results = torch.cat([tmp.reshape(-1) for tmp in multiplication_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(multiplication_results.reshape(-1).cpu().numpy(), bins=100, log=True)\n",
    "# cdf of abs(multiplication_results)\n",
    "\n",
    "plt.hist(multiplication_results.reshape(-1).abs().cpu().numpy(), bins=100, cumulative=True, density=True, histtype='step')\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.title(f\"Q*K^T, {q.shape[1]} * {q.shape[2]} * {k.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perplexity import perplexity\n",
    "\n",
    "device = 'cuda:0'\n",
    "root = '~/'\n",
    "dataset = 'PTB'\n",
    "\n",
    "stride = model.config.max_position_embeddings # 4096\n",
    "ppl_baseline = perplexity(model, tokenizer, dataset, device, verbose=True, stride=stride, root=root)\n",
    "print(ppl_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.30it/s]\n",
      "/home/xupeng/miniconda3/envs/faiss/lib/python3.11/site-packages/deepeval/__init__.py:42: UserWarning: You are using deepeval version 0.21.33, however version 0.21.34 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /home/xupeng/.cache/huggingface/modules/datasets_modules/datasets/lukaemon--mmlu/5407247256b75097c6ed96d65e9673eaf8cb7522ab67e1ea65e7bb85b44be036 (last modified on Thu Apr 25 10:08:46 2024) since it couldn't be found locally at lukaemon/mmlu, or remotely on the Hugging Face Hub.\n",
      "Processing high_school_computer_science:   0%|          | 0/100 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing high_school_computer_science:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Define benchmark with specific tasks and shots\u001b[39;00m\n\u001b[1;32m     14\u001b[0m benchmark \u001b[38;5;241m=\u001b[39m MMLU(\n\u001b[1;32m     15\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m[MMLUTask\u001b[38;5;241m.\u001b[39mHIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask\u001b[38;5;241m.\u001b[39mASTRONOMY],\n\u001b[1;32m     16\u001b[0m     n_shots\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# unset http_proxy and https_proxy\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# os.environ.pop('http_proxy')\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# os.environ.pop('https_proxy')\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Replace 'mistral_7b' with your own custom model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(benchmark\u001b[38;5;241m.\u001b[39moverall_score)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.11/site-packages/deepeval/benchmarks/mmlu/mmlu.py:41\u001b[0m, in \u001b[0;36mMMLU.evaluate\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Calculate task accuracy\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m golden \u001b[38;5;129;01min\u001b[39;00m tqdm(goldens, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     prediction, score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgolden\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score:\n\u001b[1;32m     43\u001b[0m         task_correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.11/site-packages/deepeval/benchmarks/mmlu/mmlu.py:81\u001b[0m, in \u001b[0;36mMMLU.predict\u001b[0;34m(self, model, task, golden)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshots_dataset \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     74\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample dataset is empty. Call load_benchmark.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m prompt: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m MMLUTemplate\u001b[38;5;241m.\u001b[39mgenerate_output(\n\u001b[1;32m     76\u001b[0m     train_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshots_dataset,\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mgolden\u001b[38;5;241m.\u001b[39minput,\n\u001b[1;32m     78\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m     79\u001b[0m     n_shots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_shots,\n\u001b[1;32m     80\u001b[0m )\n\u001b[0;32m---> 81\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Define Metric\u001b[39;00m\n\u001b[1;32m     84\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mexact_match_score(\n\u001b[1;32m     85\u001b[0m     golden\u001b[38;5;241m.\u001b[39mexpected_output, prediction\n\u001b[1;32m     86\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss/lib/python3.11/site-packages/transformers/generation/utils.py:1415\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;66;03m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;66;03m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;66;03m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;66;03m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1413\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   1414\u001b[0m )\n\u001b[0;32m-> 1415\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39moutput_attentions\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['http_proxy'] = '127.0.0.1:7897'\n",
    "# os.environ['https_proxy'] = '127.0.0.1:7897'\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"llama-2-7b-hf\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"llama-2-7b-hf\")\n",
    "\n",
    "\n",
    "from deepeval.benchmarks import MMLU\n",
    "from deepeval.benchmarks.tasks import MMLUTask\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "benchmark = MMLU(\n",
    "    tasks=[MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ASTRONOMY],\n",
    "    n_shots=3\n",
    ")\n",
    "\n",
    "benchmark.evaluate(model=model)\n",
    "\n",
    "# unset http_proxy and https_proxy\n",
    "# os.environ.pop('http_proxy')\n",
    "# os.environ.pop('https_proxy')\n",
    "\n",
    "# Replace 'mistral_7b' with your own custom model\n",
    "print(benchmark.overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
