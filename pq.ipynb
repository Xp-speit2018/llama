{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"llama-2-7b-hf\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The temperature in transformers.pipeline means that it gets hot, not cold. You don’t want a 1st order effect to be interpreted as a 2nd order effect by thermodynamics, like we are doing here. I could probably prove to you that there is no difference between Φ(t) = Φ(0) + ηt and Φ(t) = Φ(0)eθt in the first two terms, just by adding 1+0t to one side, but you would still insist otherwise, at least until it is pointed out to you that the error in your interpretation is in the third term. You don’t really care about what the temperature is doing, you only care about the 2nd order effect.\\nI am no longer posting to your comment section, or even reading your comments on my blog comments. You are not welcome.\\n“The “scientific” community is afraid to say that because this would require them to look at and consider the evidence from all the various sources (in the same way that we judge the evidence from a case in a court of law) and not from one cherry-picked result. ”\\nThe only thing about my post that you have addressed is the idea that the IPCC “cherry-picks”. The other concerns I have are regarding the idea that a scientific consensus is a necessary or sufficient condition for accepting a theory, the claim that IPCC scientists have been intimidated by the anti-science/conspiracy fringe, etc.\\nI don’t know why you insist on pretending that I claim that there is no uncertainty in science. But I do not. There is always a level of uncertainty when looking at complex problems, and I think that it is important that we recognize that. To take any scientific result, and state it unconditionally with absolute certainty, is unscientific and a violation of the scientific method. I fully accept that there are, and always will be, uncertainties in our understanding of the complex systems around us.\\nBut that doesn’t mean that the “scientific” consensus on a topic should be taken as gospel. In fact, in my opinion, the consensus that is formed by a group of scientists should be taken as a good first step. That’s the main purpose of the IPCC report. But there is still uncertainty in'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = device,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "sequence = \"The temperature in transformers.pipeline means\"\n",
    "output = pipeline(sequence, max_length=512, do_sample=True, temperature=0.9)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
