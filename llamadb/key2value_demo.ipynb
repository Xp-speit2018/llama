{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Wq, Wk, Wv是nn.Linear的权重，默认是行优先的，需要转置\n",
    "cos, sin, Wq, Wk, Wv = torch.load('key2value_demo.pt')\n",
    "Wq = Wq.t()\n",
    "Wk = Wk.t()\n",
    "Wv = Wv.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore value from key\n",
    "\n",
    "$$k_{cache} = x W_k R_m, v_{cache} = x W_v$$  \n",
    "$$v_{cache} = k_{cache} R_m^{-1} W_k^{-1} W_v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse RoPE\n",
    "$$\n",
    "\\text{Rot}(m \\theta_0) = \n",
    "\\begin{bmatrix}\n",
    "    \\text{cos}(m \\theta_0) & -\\text{sin}(m \\theta_0) \\\\\n",
    "    \\text{sin}(m \\theta_0) & \\text{cos}(m \\theta_0) \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Rot}^{-1}(m \\theta_0) = \\text{Rot}(-m \\theta_0) =\n",
    "\\begin{bmatrix}\n",
    "    \\text{cos}(m \\theta_0) & \\text{sin}(m \\theta_0) \\\\\n",
    "    -\\text{sin}(m \\theta_0) & \\text{cos}(m \\theta_0) \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 1\n",
    "q_len = 4096\n",
    "num_heads = 32\n",
    "head_dim = 128\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(bsz, q_len, num_heads*head_dim)\n",
    "\n",
    "query_states = (x @ Wq).view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "key_states   = (x @ Wk).view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "value_states = (x @ Wv).view(bsz, q_len, num_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rot, key_rot = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rerot, key_rerot = apply_rotary_pos_emb(query_rot, key_rot, cos, -sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(key_states, key_rerot, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse nn.Linear (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.double\n",
    "w = Wk.to(dtype)\n",
    "torch.allclose(w @ torch.inverse(w), torch.eye(q_len, dtype=dtype), atol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "w = Wk.to(dtype)\n",
    "torch.allclose(w @ torch.inverse(w), torch.eye(q_len, dtype=dtype), atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array type float128 is unsupported in linalg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      4\u001b[0m M \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(size, size)\u001b[38;5;241m.\u001b[39mastype(dtype)\n\u001b[0;32m----> 6\u001b[0m res \u001b[38;5;241m=\u001b[39m M \u001b[38;5;241m@\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mabs(res \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39meye(size))\u001b[38;5;241m.\u001b[39mmax())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/linalg/linalg.py:557\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    555\u001b[0m _assert_stacked_2d(a)\n\u001b[1;32m    556\u001b[0m _assert_stacked_square(a)\n\u001b[0;32m--> 557\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m \u001b[43m_commonType\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/linalg/linalg.py:173\u001b[0m, in \u001b[0;36m_commonType\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    170\u001b[0m         result_type \u001b[38;5;241m=\u001b[39m double\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m rt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;66;03m# unsupported inexact scalar\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is unsupported in linalg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    174\u001b[0m                 (a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname,))\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     result_type \u001b[38;5;241m=\u001b[39m double\n",
      "\u001b[0;31mTypeError\u001b[0m: array type float128 is unsupported in linalg"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dtype = np.float128\n",
    "size = 128\n",
    "M = np.random.randn(size, size).astype(dtype)\n",
    "\n",
    "res = M @ np.linalg.inv(M)\n",
    "print(np.abs(res - np.eye(size)).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好像为不同平台适配128位很困难：https://github.com/pytorch/pytorch/issues/48338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore v from k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "\n",
    "# to dtype\n",
    "key_rot = key_rot.to(dtype)\n",
    "query_rot = query_rot.to(dtype)\n",
    "cos = cos.to(dtype)\n",
    "sin = sin.to(dtype)\n",
    "\n",
    "W_k_double = Wk.to(torch.double)\n",
    "W_k_double_inv = torch.inverse(W_k_double).to(dtype)\n",
    "W_v = Wv.to(dtype)\n",
    "\n",
    "_, key_rerot = apply_rotary_pos_emb(query_rot, key_rot, cos, -sin)\n",
    "key_rerot_full_head = key_rerot.transpose(1, 2).contiguous().view(bsz, q_len, num_heads*head_dim)\n",
    "\n",
    "\n",
    "transition_matrix = (W_k_double_inv @ W_v).to(dtype)\n",
    "\n",
    "value_restored = (key_rerot_full_head @ transition_matrix).view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(value_restored, value_states.to(dtype), atol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(key_rerot, key_states.to(dtype), atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(value_restored, value_states.to(dtype), atol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(value_restored, value_states.to(dtype), atol=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct vectors from IndexPQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用IndexFlatIP确保正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "index = faiss.IndexFlatIP(128)\n",
    "index.add(key_states[0, 0].cpu().detach().numpy())\n",
    "q = query_states[0, 0, :2, :].cpu().detach().numpy() \n",
    "D, I = index.search(q, 4096)\n",
    "# index.make_direct_map()\n",
    "k = index.reconstruct(int(I[0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(torch.Tensor(k), key_states[0, 0, I[0, 0]].cpu(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from faiss import read_index\n",
    "def restore_index(layer_idx: int, head_idx: int, toGPU=False):\n",
    "    \"\"\"\n",
    "    Restore a FAISS IndexPQ from disk.\n",
    "    \n",
    "    Args:\n",
    "    layer_idx (int): The index of the layer in the model.\n",
    "    head_idx (int): The index of the attention head within the layer.\n",
    "\n",
    "    Returns:\n",
    "    faiss.IndexPQ: The restored FAISS index.\n",
    "    \"\"\"\n",
    "    index_filename = f\"../llama_pqindex/PTB/key_{layer_idx}_{head_idx}.ivf\" # TODO: use ivfpq with nlist=1 to move to GPU\n",
    "    # index_filename = f\"../pq_index/pq_{layer_idx}_{head_idx}.index\"\n",
    "    idx = read_index(index_filename)\n",
    "    if toGPU is True:\n",
    "        # move the index to gpu\n",
    "        res = faiss.StandardGpuResources()\n",
    "        res.noTempMemory() #TODO: install nightly build https://github.com/facebookresearch/faiss/issues/3259\n",
    "        idx = faiss.index_cpu_to_gpu(res, 0, idx)\n",
    "        # print(f\"pq_{layer_idx}_{head_idx}.index moved to GPU.\")\n",
    "\n",
    "        # copy to ivf index\n",
    "        \n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = restore_index(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method handle_Index.<locals>.replacement_reconstruct_batch of <faiss.swigfaiss_avx2.IndexIVFPQ; proxy of <Swig Object of type 'faiss::IndexIVFPQ *' at 0x7f91ac14fab0> >>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.reconstruct_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 4096, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(key_states[0, 0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = query_states[0, 0, :2, :].cpu().detach().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(q, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 152.4641  ,  147.69005 ,  145.22713 , ..., -170.49046 ,\n",
       "        -170.6556  , -185.49026 ],\n",
       "       [ 124.03221 ,  119.00385 ,  118.868866, ...,  -99.34771 ,\n",
       "         -99.79555 ,  -99.87963 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 848, 2971, 3658, ..., 3024,  661, 2814],\n",
       "       [1287,  145, 2400, ..., 3311, 1976, 3658]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43631/3618152049.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  I = torch.tensor(I)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "I = torch.tensor(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = I[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_len = 1\n",
    "seq_len = 4096\n",
    "attn_bias = torch.zeros(query_len, seq_len, device=query_states.device, dtype=query_states.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1076 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mattn_bias\u001b[49m\u001b[43m[\u001b[49m\u001b[43mI\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1076 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "attn_bias[I]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.8966e-01, 8.3588e-03, 7.1205e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [9.8152e-01, 6.4285e-03, 5.6167e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor(D), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.make_direct_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = index.reconstruct(int(I[0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1897, -2.4531,  2.2896,  1.5974,  1.0974, -1.7216,  2.5351, -0.2188,\n",
       "        -2.1048,  2.4843, -3.3355,  1.8666, -1.3950, -0.3860,  2.2833,  0.6918,\n",
       "         1.1429,  0.8756, -0.3545,  0.6246, -0.3108, -0.1326, -0.4309,  0.7417,\n",
       "         1.7682,  1.5424,  0.9124, -1.8929,  2.0146,  2.9387,  1.1566, -1.3198,\n",
       "         1.3583, -1.7361,  1.3166, -0.1700,  1.6137,  1.4473,  1.6422, -0.1327,\n",
       "         1.4049, -1.7469, -0.4068, -2.0375, -2.3081,  1.4111,  0.7240, -0.4021,\n",
       "         0.5501, -0.8176,  0.6696,  0.3275,  0.9077, -2.2053,  2.0409,  0.6902,\n",
       "         0.6504, -1.5217, -2.3723,  0.7258, -2.2717, -1.5013, -1.7850, -0.5082,\n",
       "        -2.3895,  1.4159,  2.4559, -0.0972,  2.2484, -2.9504, -0.7898,  0.2306,\n",
       "        -2.1079,  2.4772, -0.4761,  1.9000, -1.1578, -2.1209,  1.7029,  0.0284,\n",
       "        -0.2022, -1.7919,  1.5018, -0.9045, -1.5731,  1.2282,  2.0642, -0.1675,\n",
       "        -0.5248, -0.8281,  1.7748,  1.3238,  0.0307,  2.6912, -0.9722, -1.6677,\n",
       "         2.0722,  2.1624, -1.9113,  0.9757,  1.5337, -0.7305,  0.5083,  1.2157,\n",
       "        -1.7401, -0.2647, -2.0474, -0.1476, -0.6846,  0.8365,  1.3529,  1.1284,\n",
       "         1.1545, -1.5471, -2.2441,  1.9787,  2.1064,  0.8426, -1.8934,  2.1162,\n",
       "        -0.5021, -1.5745,  1.4874,  2.0570,  0.7712, -2.0875,  2.0210, -2.3869],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_states[0, 0, int(I[0, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.81850350e+00, -1.57141471e+00,  1.34008694e+00,  1.25246370e+00,\n",
       "        1.24328887e+00, -9.89186093e-02, -1.01044849e-01,  6.85416758e-02,\n",
       "        3.40758497e-03,  9.34282601e-01, -2.71085715e+00,  5.44420294e-02,\n",
       "       -5.53655505e-01, -6.08758986e-01,  8.17654654e-02, -2.80177921e-01,\n",
       "        4.26508397e-01,  1.44758210e-01, -2.11644962e-01,  5.20126879e-01,\n",
       "       -6.05017960e-01, -4.58945110e-02, -3.70356232e-01,  5.84389210e-01,\n",
       "        1.61656463e+00,  5.91018379e-01,  1.13304138e+00,  1.18021257e-01,\n",
       "        1.13816106e+00,  1.33691120e+00,  1.61989617e+00, -1.60063279e+00,\n",
       "        1.70022321e+00, -1.38676214e+00, -3.43358099e-01,  7.92919993e-02,\n",
       "       -1.57923505e-01,  9.59423482e-01,  4.93834257e-01, -3.71007442e-01,\n",
       "        1.37167108e+00, -6.86888814e-01, -5.55884600e-01, -1.34201860e+00,\n",
       "       -1.16221464e+00,  5.61712543e-03,  5.36624551e-01,  1.16127884e+00,\n",
       "        7.61983693e-01, -5.10609627e-01, -6.17755726e-02,  1.04267396e-01,\n",
       "        9.02703926e-02, -7.61232734e-01,  6.75623596e-01,  4.13577229e-01,\n",
       "        2.26177260e-01, -2.95467496e-01, -4.02590692e-01,  6.38923168e-01,\n",
       "       -3.90336484e-01, -5.87984741e-01, -8.46989632e-01, -5.24410382e-02,\n",
       "       -2.27138877e+00, -7.15054750e-01,  1.32344532e+00, -9.98805285e-01,\n",
       "        1.28842211e+00, -7.21659303e-01, -2.70081282e-01, -1.69654470e-03,\n",
       "        1.67023629e-01,  3.23999047e-01, -3.76065493e-01,  4.01336461e-01,\n",
       "       -3.32546562e-01, -1.11959827e+00,  1.06715488e+00, -3.99633348e-01,\n",
       "       -3.50184739e-01, -7.56956041e-01,  3.29126567e-01, -4.42256480e-01,\n",
       "       -8.80173624e-01,  7.04075813e-01,  8.91694188e-01, -2.00596824e-01,\n",
       "       -2.07249075e-01, -1.06903815e+00,  8.79952312e-01, -4.87222001e-02,\n",
       "        1.05104923e+00,  1.82098520e+00, -8.59728873e-01, -1.38403249e+00,\n",
       "        1.43953776e+00,  1.52320397e+00, -1.87851334e+00,  1.52564776e+00,\n",
       "        5.66241682e-01, -7.10205376e-01, -1.67927253e+00, -2.72754133e-01,\n",
       "       -5.98254681e-01,  6.37887061e-01, -6.15887105e-01, -7.66154528e-01,\n",
       "       -1.43910515e+00,  1.36435300e-01, -2.79403299e-01,  8.31153035e-01,\n",
       "        1.11958063e+00, -1.50666487e+00, -4.51678753e-01,  6.73286796e-01,\n",
       "        4.40817177e-01,  4.40486893e-02, -1.53653908e+00,  4.36397284e-01,\n",
       "       -1.02748084e+00, -1.07017541e+00,  2.31058583e-01,  6.36961937e-01,\n",
       "        1.36393592e-01, -5.87269783e-01,  1.79604962e-01, -5.44130445e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
