{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wzw/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers.models.llama.modeling_llama import LlamaSdpaAttention, apply_rotary_pos_emb\n",
    "import torch\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "def read_fvecs(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        vecs = []\n",
    "        while True:\n",
    "            data = f.read(4)\n",
    "            if len(data) < 4:\n",
    "                break\n",
    "            d = int.from_bytes(data, 'little')\n",
    "            vec = np.frombuffer(f.read(d * 4), dtype=np.float32)\n",
    "            vecs.append(vec)\n",
    "        return np.array(vecs)\n",
    "\n",
    "def write_fvecs(filename, vecs, mode='ab'):\n",
    "    with open(filename, mode) as f:\n",
    "        for vec in vecs:\n",
    "            d = len(vec)\n",
    "            f.write(np.int32(d).tobytes())\n",
    "            f.write(vec.astype(np.float32).tobytes())\n",
    "\n",
    "def save_forward(\n",
    "    self: LlamaSdpaAttention,\n",
    "    hidden_states,\n",
    "    attention_mask = None,\n",
    "    position_ids = None,\n",
    "    past_key_value = None,\n",
    "    output_attentions: bool = False,\n",
    "    use_cache: bool = False,\n",
    "    cache_position = None,\n",
    "):\n",
    "\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = self.q_proj(hidden_states)\n",
    "    key_states = self.k_proj(hidden_states)\n",
    "    value_states = self.v_proj(hidden_states)\n",
    "\n",
    "    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "    # save key to files, per layer per head to .fvecs filess\n",
    "    for b in range(bsz):\n",
    "        for h in range(self.num_heads):\n",
    "            key = key_states[b, h].view(q_len, self.head_dim).cpu().detach().numpy()\n",
    "            write_fvecs(f'../llama_key/key_{self.layer_idx}_{h}.fvecs', key, 'ab')\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "        # cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx)\n",
    "    \n",
    "    causal_mask = None\n",
    "\n",
    "    # In case we are not compiling, we may set `causal_mask` to None, which is required to dispatch to SDPA's Flash Attention 2 backend, rather\n",
    "    # relying on the `is_causal` argument.\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "        is_causal=causal_mask is None and q_len > 1,\n",
    "    )\n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n",
    "\n",
    "    attn_output = self.o_proj(attn_output)\n",
    "\n",
    "    return attn_output, None, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained('../llama-2-7b-hf')\n",
    "tokenizer = LlamaTokenizer.from_pretrained('../llama-2-7b-hf')\n",
    "\n",
    "# replace the forward function with the custom one\n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn.forward = types.MethodType(save_forward, layer.self_attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.012s]     Loading dataset PTB\n",
      "[8.026s]     Encoding dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 27/28 [07:14<00:16, 16.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.380826950073242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# add parent path\n",
    "sys.path.append('../')\n",
    "\n",
    "from perplexity import perplexity\n",
    "\n",
    "device = 'cuda:0'\n",
    "root = '../'\n",
    "dataset = 'PTB'\n",
    "\n",
    "stride = model.config.max_position_embeddings # 4096\n",
    "ppl_baseline = perplexity(model, tokenizer, dataset, device, verbose=True, stride=stride, root=root)\n",
    "print(ppl_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move ../llama_key/*.fvecs to ../llama_key/PTB/*.fvecs\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# 创建目标目录\n",
    "os.makedirs(f'../llama_key/{dataset}', exist_ok=True)\n",
    "\n",
    "# 获取所有符合通配符的文件\n",
    "files = glob.glob('../llama_key/*.fvecs')\n",
    "\n",
    "# 移动每个文件到目标目录\n",
    "for file in files:\n",
    "    shutil.move(file, f'../llama_key/{dataset}/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
