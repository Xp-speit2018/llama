{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wzw/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from modeling_llamadb import LlamaForCausalLMDB, apply_rotary_pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLMDB.from_pretrained(\"../llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRotaryEmbedding()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.rotary_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore value from key\n",
    "\n",
    "$$k_{cache} = x W_k R_m, v_{cache} = x W_v$$  \n",
    "$$v_{cache} = k_{cache} R_m^{-1} W_k^{-1} W_v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse RoPE\n",
    "$$\n",
    "\\text{Rot}(m \\theta_0) = \n",
    "\\begin{bmatrix}\n",
    "    \\text{cos}(m \\theta_0) & -\\text{sin}(m \\theta_0) \\\\\n",
    "    \\text{sin}(m \\theta_0) & \\text{cos}(m \\theta_0) \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Rot}^{-1}(m \\theta_0) = \\text{Rot}(-m \\theta_0) =\n",
    "\\begin{bmatrix}\n",
    "    \\text{cos}(m \\theta_0) & \\text{sin}(m \\theta_0) \\\\\n",
    "    -\\text{sin}(m \\theta_0) & \\text{cos}(m \\theta_0) \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RecursiveNamespace:\n",
    "    def __init__(self):\n",
    "        self.__dict__['_attributes'] = {}\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name not in self._attributes:\n",
    "            self._attributes[name] = RecursiveNamespace()\n",
    "        return self._attributes[name]\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, RecursiveNamespace):\n",
    "            self._attributes[name] = value\n",
    "        else:\n",
    "            current = self\n",
    "            *parts, last = name.split('.')\n",
    "            for part in parts:\n",
    "                current = getattr(current, part)\n",
    "            current._attributes[last] = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._attributes)\n",
    "    \n",
    "A = RecursiveNamespace()\n",
    "A.device.type = 'cpu'\n",
    "A.dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_position_embeddings = 4096\n",
    "position_ids = torch.arange(max_position_embeddings).view(1, max_position_embeddings)\n",
    "cos, sin = attn.rotary_emb(A, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 1\n",
    "q_len = 4096\n",
    "num_heads = 32\n",
    "head_dim = 128\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(bsz, q_len, num_heads*head_dim)\n",
    "\n",
    "key_states   = attn.k_proj(x).view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "query_states = attn.q_proj(x).view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "value_states = attn.v_proj(x).view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rot, key_rot = apply_rotary_pos_emb(query_states, key_states, cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rerot, key_rerot = apply_rotary_pos_emb(query_rot, key_rot, cos, -sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(key_states, key_rerot, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_cpu = cos.cpu()\n",
    "sin_cpu = sin.cpu()\n",
    "Wq_cpu = attn.q_proj.weight.cpu()\n",
    "Wk_cpu = attn.k_proj.weight.cpu()\n",
    "Wv_cpu = attn.v_proj.weight.cpu()\n",
    "\n",
    "torch.save(\n",
    "    (\n",
    "        cos_cpu,\n",
    "        sin_cpu,\n",
    "        Wq_cpu,\n",
    "        Wk_cpu,\n",
    "        Wv_cpu,\n",
    "        # x\n",
    "    ),\n",
    "    \"key2value_demo.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(attn.k_proj.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0162,  0.0079, -0.0013,  ...,  0.0166, -0.0099, -0.0135],\n",
      "        [ 0.0192,  0.0015,  0.0036,  ..., -0.0211,  0.0152,  0.0234],\n",
      "        [-0.0236, -0.0217,  0.0017,  ...,  0.0150, -0.0165, -0.0118],\n",
      "        ...,\n",
      "        [ 0.0128, -0.0007, -0.0008,  ...,  0.0002,  0.0031,  0.0081],\n",
      "        [-0.0056,  0.0173, -0.0032,  ..., -0.0032,  0.0115, -0.0110],\n",
      "        [ 0.0037, -0.0021,  0.0013,  ...,  0.0070, -0.0115,  0.0095]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(attn.k_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -1.9431e-05, -5.2676e-05,  ...,  5.3465e-05,\n",
       "         -1.1873e-04,  4.1932e-05],\n",
       "        [-1.2130e-05,  1.0001e+00,  7.7486e-06,  ..., -2.2125e-04,\n",
       "          3.1185e-04,  2.8515e-04],\n",
       "        [-4.2040e-05,  1.5593e-04,  9.9993e-01,  ..., -2.3746e-04,\n",
       "          5.5456e-04,  6.5470e-04],\n",
       "        ...,\n",
       "        [ 6.8635e-05,  6.8426e-05,  4.5431e-04,  ...,  9.9842e-01,\n",
       "         -3.8624e-05, -4.5717e-04],\n",
       "        [-6.1132e-06,  1.0967e-04,  1.2416e-04,  ..., -4.0436e-04,\n",
       "          1.0003e+00,  9.1076e-05],\n",
       "        [-2.5466e-05, -1.7881e-06,  9.0897e-05,  ..., -6.6257e-04,\n",
       "          7.4387e-05,  9.9996e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.k_proj.weight @ torch.pinverse(attn.k_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  1.1921e-07, -5.7220e-06,  ...,  4.3631e-05,\n",
       "          2.8610e-06,  1.0014e-05],\n",
       "        [-8.3447e-07,  1.0000e+00,  6.1989e-06,  ...,  4.3869e-05,\n",
       "         -1.0014e-05,  1.5259e-05],\n",
       "        [ 1.7881e-07,  4.7684e-07,  1.0000e+00,  ...,  2.0981e-05,\n",
       "          2.4080e-05,  3.0696e-06],\n",
       "        ...,\n",
       "        [ 4.7982e-06,  7.4267e-05, -4.1872e-05,  ...,  9.9957e-01,\n",
       "         -2.8133e-04, -6.8893e-04],\n",
       "        [-7.2937e-05, -1.8716e-05, -9.3579e-04,  ..., -1.7776e-03,\n",
       "          9.9938e-01, -5.7840e-04],\n",
       "        [-6.6906e-06, -5.9485e-05,  3.6937e-04,  ...,  2.2829e-03,\n",
       "         -8.4162e-05,  1.0001e+00]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.k_proj.weight @ torch.inverse(attn.k_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = attn.k_proj.weight.to(torch.double)\n",
    "tmp = w @ torch.inverse(w)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(w @ torch.inverse(w), torch.eye(4096, dtype=torch.double), atol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7997e-05)\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "size = 1024\n",
    "\n",
    "# M = torch.randn(128, 128)\n",
    "M = torch.randn(size, size, dtype=dtype)\n",
    "\n",
    "res = M @ torch.inverse(M)\n",
    "\n",
    "print(torch.abs(res - torch.eye(size, dtype=dtype)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array type float128 is unsupported in linalg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat128\n\u001b[1;32m      4\u001b[0m M \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(size, size)\u001b[38;5;241m.\u001b[39mastype(dtype)\n\u001b[0;32m----> 6\u001b[0m res \u001b[38;5;241m=\u001b[39m M \u001b[38;5;241m@\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mabs(res \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39meye(size))\u001b[38;5;241m.\u001b[39mmax())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/linalg/linalg.py:557\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    555\u001b[0m _assert_stacked_2d(a)\n\u001b[1;32m    556\u001b[0m _assert_stacked_square(a)\n\u001b[0;32m--> 557\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m \u001b[43m_commonType\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/linalg/linalg.py:173\u001b[0m, in \u001b[0;36m_commonType\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    170\u001b[0m         result_type \u001b[38;5;241m=\u001b[39m double\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m rt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;66;03m# unsupported inexact scalar\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is unsupported in linalg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    174\u001b[0m                 (a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname,))\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     result_type \u001b[38;5;241m=\u001b[39m double\n",
      "\u001b[0;31mTypeError\u001b[0m: array type float128 is unsupported in linalg"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dtype = np.float128\n",
    "\n",
    "M = np.random.randn(size, size).astype(dtype)\n",
    "\n",
    "res = M @ np.linalg.inv(M)\n",
    "print(np.abs(res - np.eye(size)).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of `inverse(k_proj)` and `v_proj`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "\n",
    "# to dtype\n",
    "key_rot = key_rot.to(dtype)\n",
    "query_rot = query_rot.to(dtype)\n",
    "cos = cos.to(dtype)\n",
    "sin = sin.to(dtype)\n",
    "W_k_double = attn.k_proj.weight.T.to(torch.double)\n",
    "W_v_double = attn.v_proj.weight.T.to(torch.double)\n",
    "\n",
    "W_k_double_inv = torch.inverse(W_k_double).to(dtype)\n",
    "W_v = W_v_double.to(dtype)\n",
    "\n",
    "_, key_rerot = apply_rotary_pos_emb(query_rot, key_rot, cos, -sin)\n",
    "key_rerot_full_head = key_rerot.transpose(1, 2).contiguous().view(bsz, q_len, num_heads*head_dim)\n",
    "\n",
    "\n",
    "transition_matrix = (W_k_double_inv @ W_v).to(dtype)\n",
    "\n",
    "value_restored = (key_rerot_full_head @ transition_matrix).view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(key_rerot_full_head, x.to(dtype), atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6250,  0.9579,  0.4295,  ..., -2.0997,  1.0471,  0.1994],\n",
       "         [-2.7079,  3.2356, -2.7173,  ...,  0.1224, -0.8601, -0.6023],\n",
       "         [-0.3822, -0.0718,  0.2622,  ...,  0.3680,  0.2994,  1.1320],\n",
       "         ...,\n",
       "         [-1.9004,  2.7978, -2.1939,  ..., -0.8557,  0.2071,  0.1726],\n",
       "         [ 1.0422, -1.9806,  2.1556,  ..., -0.0361,  0.3327, -0.5052],\n",
       "         [-0.6820,  0.9117, -1.3218,  ...,  1.0008, -1.4883,  1.3174]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_rerot_full_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(key_rerot, key_states.to(dtype), atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(value_restored, value_states.to(dtype), atol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0154, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(value_restored - value_states.to(dtype)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0007, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(value_restored - value_states.to(dtype)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2260)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(torch.abs(value_restored - value_states) > 1e-3) / value_states.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0058,  0.2702, -0.3478, -0.6390, -1.2680, -0.7903, -0.4243, -0.1293,\n",
       "         0.1100, -1.5165], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_states[0, 0, 0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2167,  0.5526,  0.0596, -0.3388, -0.1308,  0.3450,  0.0022, -0.4988,\n",
       "        -0.6386,  0.6053], dtype=torch.float64, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_restored[0, 0, 0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.9145,  2.6446, -1.7584,  ..., -1.0911,  0.8427, -0.7621],\n",
       "         [-0.5875,  0.6003, -0.1183,  ..., -0.3745, -0.6578,  0.5973],\n",
       "         [ 0.6806, -0.7473,  0.4930,  ...,  0.0648,  2.8428, -0.5266],\n",
       "         ...,\n",
       "         [-0.6588, -0.4816,  0.4644,  ..., -1.5955, -1.0929,  1.5297],\n",
       "         [ 1.3331, -0.6762,  0.8802,  ...,  1.5387, -1.0052,  0.2580],\n",
       "         [-0.2668,  0.3794, -0.6736,  ..., -0.6867, -2.0124, -0.3171]]],\n",
       "       dtype=torch.float64, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_rerot.transpose(1, 2).contiguous().view(bsz, q_len, num_heads*head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.9145,  2.6446, -1.7584,  ..., -1.0911,  0.8427, -0.7621],\n",
       "         [-0.5875,  0.6003, -0.1183,  ..., -0.3745, -0.6578,  0.5973],\n",
       "         [ 0.6806, -0.7473,  0.4930,  ...,  0.0648,  2.8428, -0.5266],\n",
       "         ...,\n",
       "         [-0.6588, -0.4816,  0.4644,  ..., -1.5955, -1.0929,  1.5297],\n",
       "         [ 1.3331, -0.6762,  0.8802,  ...,  1.5387, -1.0052,  0.2580],\n",
       "         [-0.2668,  0.3794, -0.6736,  ..., -0.6867, -2.0124, -0.3171]]],\n",
       "       dtype=torch.float64, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_rerot_full_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.2282e-01,  1.0803e+00, -4.5230e-01,  ...,  1.6503e-01,\n",
       "          -9.3736e-01, -5.0019e-01],\n",
       "         [-1.6404e+00,  4.6956e-01, -4.5691e-01,  ...,  1.0660e+00,\n",
       "           9.1425e-02, -1.9353e-03],\n",
       "         [ 1.3318e+00, -6.9224e-01, -8.6427e-02,  ...,  1.0561e-01,\n",
       "          -2.0532e+00, -9.0696e-01],\n",
       "         ...,\n",
       "         [-1.0923e+00, -8.0692e-01, -2.3702e-01,  ..., -4.9561e-01,\n",
       "          -3.8218e-01, -2.7865e-01],\n",
       "         [-1.0283e+00,  1.6916e+00, -1.3834e+00,  ...,  1.0330e+00,\n",
       "          -3.3758e-01,  9.6891e-02],\n",
       "         [ 3.3261e-01,  1.6135e+00, -2.3792e-01,  ..., -4.8955e-02,\n",
       "          -7.5147e-01, -1.0646e-01]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.65355977452702"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "52 * np.log(2) / np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.923689900271567"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "23 * np.log(2) / np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max error\n",
      "2.962434713770639716e-14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def gauss_jordan_inverse(matrix):\n",
    "    n = matrix.shape[0]\n",
    "    # Create an augmented matrix with the identity matrix on the right\n",
    "    augmented_matrix = np.hstack((matrix, np.eye(n)))\n",
    "    \n",
    "    # Apply Gauss-Jordan elimination\n",
    "    for i in range(n):\n",
    "        # Make the diagonal contain all ones\n",
    "        diag_element = augmented_matrix[i, i]\n",
    "        augmented_matrix[i] = augmented_matrix[i] / diag_element\n",
    "        \n",
    "        # Make the other elements in the current column zero\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                row_factor = augmented_matrix[j, i]\n",
    "                augmented_matrix[j] = augmented_matrix[j] - row_factor * augmented_matrix[i]\n",
    "    \n",
    "    # The right half of the augmented matrix is now the inverse\n",
    "    inverse_matrix = augmented_matrix[:, n:]\n",
    "    return inverse_matrix\n",
    "\n",
    "def gauss_jordan_inverse_parallel(matrix):\n",
    "    n = matrix.shape[0]\n",
    "    augmented_matrix = np.hstack((matrix, np.eye(n)))\n",
    "\n",
    "    def process_row(i):\n",
    "        diag_element = augmented_matrix[i, i]\n",
    "        augmented_matrix[i] = augmented_matrix[i] / diag_element\n",
    "\n",
    "        def zero_out_column(j):\n",
    "            if i != j:\n",
    "                row_factor = augmented_matrix[j, i]\n",
    "                augmented_matrix[j] = augmented_matrix[j] - row_factor * augmented_matrix[i]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            executor.map(zero_out_column, range(n))\n",
    "\n",
    "    for i in range(n):\n",
    "        process_row(i)\n",
    "\n",
    "    inverse_matrix = augmented_matrix[:, n:]\n",
    "    return inverse_matrix\n",
    "\n",
    "# Example usage\n",
    "size = 1024\n",
    "dtype = np.float128\n",
    "A = np.random.randn(size, size).astype(dtype)\n",
    "A_inv = gauss_jordan_inverse_parallel(A)\n",
    "\n",
    "# print(\"Original matrix:\")\n",
    "# print(A)\n",
    "# print(\"Inverse matrix:\")\n",
    "# print(A_inv)\n",
    "\n",
    "res = np.matmul(A, A_inv)\n",
    "print(\"max error\")\n",
    "print(np.abs(res - np.eye(size)).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
